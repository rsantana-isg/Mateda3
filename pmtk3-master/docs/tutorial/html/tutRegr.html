
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      --><title>Regression using parametric discriminative models in pmtk3</title><meta name="generator" content="MATLAB 7.9"><meta name="date" content="2010-09-08"><meta name="m-file" content="tutRegr"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h1>Regression using parametric discriminative models in pmtk3</h1><!--introduction--><p>Many of the basic concepts of supervised learning, and their pmtk3 implementation, can be explained using linear regression as a model. We can handle non-linear relationships using basis function expansion, as we explain below. We will discuss categorical responses (classification) later.</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Fitting a linear regression model by maximum likelihood</a></li><li><a href="#6">Bayesian parameter inference</a></li><li><a href="#22">Frequentist parameter inference</a></li><li><a href="#25">Prediction with linear regression</a></li><li><a href="#28">Preprocessing and basis function expansion</a></li><li><a href="#34">Overfitting, regularization and MAP estimation</a></li><li><a href="#44">Cross validation approach to 'tuning' ridge regression</a></li><li><a href="#51">Regularization paths</a></li><li><a href="#53">Bayesian approach to 'tuning'  ridge regression</a></li><li><a href="#60">Empirical Bayes for ridge regression</a></li><li><a href="#65">Variational Bayes for ridge regression</a></li><li><a href="#68">Sparse linear regression</a></li><li><a href="#69">L1 regularization</a></li><li><a href="#74">Automatic relevancy determination (ARD)</a></li></ul></div><h2>Fitting a linear regression model by maximum likelihood<a name="1"></a></h2><p>As a basic example, we can fit a linear regression model to a data set using maximum likelihood estimation as follows (extracted from <a href="http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Discriminative_models_for_regression_and_classification/linregBayesCaterpillar.m">linregBayesCaterpillar.m</a> ):</p><pre class="codeinput">clear <span class="string">all</span>
X = loadData(<span class="string">'caterpillar'</span>); <span class="comment">% from http://www.ceremade.dauphine.fr/~xian/BCS/caterpillar</span>
y = log(X(:,11)); <span class="comment">% log number of nests</span>
X = X(:,1:10);
[model] = linregFit(X, y)
</pre><pre class="codeoutput">model = 
        lambda: 0
             w: [11x1 double]
        sigma2: 0.5852
       preproc: [1x1 struct]
     modelType: 'linreg'
    likelihood: 'gaussian'
</pre><p>Let us check that this matches the usual equation for the MLE (adding a column of 1s to deal with the offset term)</p><pre class="codeinput">X1 = [ones(size(X,1),1) X];
wOLS = X1\y;
assert(approxeq(model.w, wOLS))
</pre><h2>Bayesian parameter inference<a name="6"></a></h2><p>If you fit by the model by MLE, you can examine the value of the estimated parameters by typing <tt>model.params</tt>, where params is the name of the parameter (here w or sigma2). But what if we want to know how much confidence we should have in these estimates? For this, we should use Bayesian inference. We can compute the posterior distribution of the parameters given the data and an uninformative prior as follows:</p><pre class="codeinput">[modelB, logev, postSummary] = linregFitBayes(X, y, <span class="string">'prior'</span>, <span class="string">'uninf'</span>);
</pre><p>Here <tt>modelB</tt> is the model which contains the posterior of the parameters:</p><pre class="codeinput">modelB
</pre><pre class="codeoutput">modelB = 
      preproc: [1x1 struct]
           wN: [11x1 double]
           VN: [11x11 double]
           aN: 11
           bN: 7.5649
    modelType: 'linregBayes'
        prior: 'uninf'
</pre><p>There is no longer a field called <tt>w</tt> or <tt>sigma2</tt> since we are not using point estimation. Instead, modelB contains the parameters of the posterior, which has the following form</p><p><img src="tutRegr_eq55655.png" alt="$$p(w,\sigma^2|D)  = N(w|w_N, V_N) IG(\sigma^2|a_N,b_N)$$"></p><p>Since we used an uninformative prior, the posterior mean is the same as the MLE</p><pre class="codeinput">assert(approxeq(wOLS, modelB.wN))
</pre><p>logev is the log evidence, or marginal likelihood, and is a measure of 'goodness of fit' of the overall model:</p><p><img src="tutRegr_eq86927.png" alt="$$p(D) = \int \int p(D|w,\sigma^2) p(w,\sigma^2) d w d \sigma^2 $$"></p><p>This can only be computed if we use a proper prior, not an uninformative prior. Hence in this case <tt>logev=[]</tt>.</p><p><tt>postSummary</tt> is a summary of the posterior. It contains the the posterior mean, standard deviation and 95% credible interval of each regression parameters. It also determines if each  coefficient is significantly different from 0, based on whether its 95% CI excludes 0.</p><pre class="codeinput">postSummary
</pre><pre class="codeoutput">postSummary = 
       what: [11x1 double]
     stderr: [11x1 double]
    credint: [11x2 double]
        sig: [1 1 1 0 1 1 0 0 0 0 0]
</pre><p>We can print the posterior summary as a table using the command below. We put a little * next to the significant coefficients.</p><pre class="codeinput">[modelB, logev, postSummary] = linregFitBayes(X, y, <span class="string">'prior'</span>, <span class="string">'uninf'</span>, <span class="keyword">...</span>
  <span class="string">'displaySummary'</span>, true);
</pre><pre class="codeoutput">coeff mean       stddev     95pc CI              sig   
   w0   10.998   3.06027  [   4.652,   17.345]     * 
   w1   -0.004   0.00156  [  -0.008,   -0.001]     * 
   w2   -0.054   0.02190  [  -0.099,   -0.008]     * 
   w3    0.068   0.09947  [  -0.138,    0.274]       
   w4   -1.294   0.56381  [  -2.463,   -0.124]     * 
   w5    0.232   0.10438  [   0.015,    0.448]     * 
   w6   -0.357   1.56646  [  -3.605,    2.892]       
   w7   -0.237   1.00601  [  -2.324,    1.849]       
   w8    0.181   0.23672  [  -0.310,    0.672]       
   w9   -1.285   0.86485  [  -3.079,    0.508]       
  w10   -0.433   0.73487  [  -1.957,    1.091]       

</pre><p>We see that coefficients  0, 1, 2, 4, 5 are "significant" by this measure. (Other methods of testing significance, based on Bayes factors, can also be used, but are a bit more complicated to implement, and one cannot use uninformative priors when using Bayes factors.)</p><p>Note that pmtk currenlty has rather limited support for Bayesian model fitting, and not all Bayesian model fitting procedures currently implement this posterior summary feature.</p><h2>Frequentist parameter inference<a name="22"></a></h2><p>It turns out that in this particular example, the Bayesian analysis is identical to a classical frequentist analysis (because the posterior for linear regression under an uninformative Jeffreys prior is equivalent to the sampling distribution of the MLE). To see this, let us use the <a href="http://www.mathworks.com/products/statistics/">stats toolbox</a> to fit the model and perform a frequentist analysis:</p><pre class="codeinput">X1 = [ones(size(X,1),1), X];
[b, bint] = regress(y, X1);
<span class="comment">% b(j) is coefficient j, bint(j,:) = lower and upper 95% conf interval</span>
assert(approxeq(b, postSummary.what))
assert(approxeq(bint, postSummary.credint))
<span class="keyword">for</span> i=1:length(b)
  fprintf(<span class="string">'%8.3f, [%8.3f, %8.3f]\n'</span>, b(i), bint(i,1), bint(i,2));
<span class="keyword">end</span>
fprintf(<span class="string">'\n'</span>);
</pre><pre class="codeoutput">  10.998, [   4.652,   17.345]
  -0.004, [  -0.008,   -0.001]
  -0.054, [  -0.099,   -0.008]
   0.068, [  -0.138,    0.274]
  -1.294, [  -2.463,   -0.124]
   0.232, [   0.015,    0.448]
  -0.357, [  -3.605,    2.892]
  -0.237, [  -2.324,    1.849]
   0.181, [  -0.310,    0.672]
  -1.285, [  -3.079,    0.508]
  -0.433, [  -1.957,    1.091]

</pre><p>We see that the MLE is the same as the posterior mean, and the 95% frequentist confidence interval is the same as the 95% Bayesian credible interval.</p><p>In general, a Bayesian and frequentist analysis may not give the same results. In pmtk, all inference is Bayesian. However, pmtk supports some non-Bayesian estimation methods, such as cross validation, as we will see below.</p><h2>Prediction with linear regression<a name="25"></a></h2><p>In machine learning, we usually care more about prediction than in trying to interpret the fitted parameters (especially since many models of interest are hard to interpret or even strictly unidentifiable).</p><p>As an example, consider fitting a linear regression model to some 1d data using MLE and Bayesian methods (using <a href="http://pmtk3.googlecode.com/svn/trunk/demos/otherDemos/supervisedModels/linregPostPredLinearDemo.m">linregPostPredLinearDemo.m</a> ), and then plotting the predictions on a test set (which is just a grid of points in the interval [-7,7])</p><pre class="codeinput">setSeed(1);
[xtrain, ytrain, xtest] =  polyDataMake(<span class="string">'sampling'</span>, <span class="string">'sparse'</span>, <span class="string">'deg'</span>, 2);

fitMethods = {@(x,y) linregFit(x,y), @(x,y) linregFitBayes(x,y)};
predictMethods = {@(x,y) linregPredict(x,y), @(x,y) linregPredictBayes(x,y)};
names = {<span class="string">'MLE'</span>, <span class="string">'Bayes'</span>};

<span class="keyword">for</span> i=1:length(fitMethods)
  model = fitMethods{i}(xtrain, ytrain);
  [mu, v] = predictMethods{i}(model, xtest);
  figure; hold <span class="string">on</span>
  plot(xtest, mu,  <span class="string">'k-'</span>, <span class="string">'linewidth'</span>, 3, <span class="string">'displayname'</span>, <span class="string">'prediction'</span>);
  plot(xtrain,ytrain,<span class="string">'ro'</span>,<span class="string">'markersize'</span>, 14, <span class="string">'linewidth'</span>, 3, <span class="keyword">...</span>
     <span class="string">'displayname'</span>, <span class="string">'training data'</span>);
  NN = length(xtest);
  ndx = 1:5:NN; <span class="comment">% plot subset of errorbars to reduce clutter</span>
  sigma = sqrt(v);
  legend(<span class="string">'location'</span>, <span class="string">'northwest'</span>);
  errorbar(xtest(ndx), mu(ndx), sigma(ndx));
  title(names{i});
<span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="tutRegr_01.png" alt=""> <img vspace="5" hspace="5" src="tutRegr_02.png" alt=""> <p>The predicted means (black lines) are the same, but  in the plugin case, the predicted variance is constant, whereas in the Bayesian case, the predicted variance increases as we move further away from the training data, as it should, since our uncertainty increases as we extrapolate further.</p><h2>Preprocessing and basis function expansion<a name="28"></a></h2><p>We are free to preprocess the data in any way we choose before fitting the model. In pmtk, you can create a preprocessor (pp) 'object', and then pass it to the fitting function; the pp will  be applied to the training data before fitting the model, and will be applied again to the test data. The advantage of this approach is that the pp is stored inside the model, which reduces the chance of applying inconsistent transformations to training and test data.</p><p>One common form of preprocessing is basis function expansion. This replaces the original features with a larger set, thus providing an easy way to fit nonlinear models. A simple example of this is polynomial expansion:</p><p><img src="tutRegr_eq70773.png" alt="$$\phi(x) = (1, x, x^2, \ldots, x^d)$$"></p><p>Below we give a simple example where we fit polynomials of increasing degree to 1d data. (We first scale the data before comouting a polynomial expansion, for reasons of numerical stability. As usual, we add a column of 1s to handle the offset term.) We then compute the mean squared error on the training and test sets (part of <a href="http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Introduction/linregPolyVsDegree.m">linregPolyVsDegree.m</a> )</p><pre class="codeinput">N = 21;
[xtrain, ytrain, xtest, ytestNoisefree, ytest] = <span class="keyword">...</span>
    polyDataMake(<span class="string">'sampling'</span>,<span class="string">'thibaux'</span>, <span class="string">'n'</span>, N);
degs = 1:20;
Nm = length(degs);
<span class="comment">% Plot error vs degree</span>
mseTrain = zeros(1,Nm); mseTest = zeros(1,Nm);
<span class="keyword">for</span> m=1:length(degs)
    deg = degs(m);
    pp = preprocessorCreate(<span class="string">'rescaleX'</span>, true, <span class="string">'poly'</span>, deg, <span class="string">'addOnes'</span>, true);
    model = linregFit(xtrain, ytrain, <span class="string">'preproc'</span>, pp);
    ypredTrain = linregPredict(model, xtrain);
    ypredTest = linregPredict(model, xtest);
    mseTrain(m) = mean((ytrain-ypredTrain).^2);
    mseTest(m) = mean((ytest-ypredTest).^2);
<span class="keyword">end</span>
ndx = (degs&lt;=16);
figure;
hold <span class="string">on</span>
plot(degs(ndx), mseTrain(ndx), <span class="string">'bs:'</span>, <span class="string">'linewidth'</span>, 2, <span class="string">'markersize'</span>, 12);
plot(degs(ndx), mseTest(ndx), <span class="string">'rx-'</span>, <span class="string">'linewidth'</span>, 2, <span class="string">'markersize'</span>, 12);
xlabel(<span class="string">'degree'</span>)
ylabel(<span class="string">'mse'</span>)
legend(<span class="string">'train'</span>, <span class="string">'test'</span>)
</pre><img vspace="5" hspace="5" src="tutRegr_03.png" alt=""> <p>We see the characteristic U-shaped curve on the test set: on the left, the model is too simple (underfits) and on the right, the model is too complex (overfits). Below we plot some of the fitted functions for different degrees:</p><pre class="codeinput"><span class="keyword">for</span> deg = [2, 14, 20]
    pp = preprocessorCreate(<span class="string">'rescaleX'</span>, true, <span class="string">'poly'</span>, deg, <span class="string">'addOnes'</span>, true);
    model = linregFit(xtrain, ytrain, <span class="string">'preproc'</span>, pp);
    ypredTrain = linregPredict(model, xtrain);
    ypredTest = linregPredict(model, xtest);
    mseTrain(m) = mean((ytrain-ypredTrain).^2);
    mseTest(m) = mean((ytest-ypredTest).^2);

    figure;
    scatter(xtrain,ytrain,<span class="string">'b'</span>,<span class="string">'filled'</span>);
    hold <span class="string">on</span>;
    plot(xtest, ypredTest, <span class="string">'k'</span>, <span class="string">'linewidth'</span>, 3);
    hold <span class="string">off</span>
    title(sprintf(<span class="string">'degree %d'</span>, deg))
    set(gca,<span class="string">'ylim'</span>,[-10 15]);
    set(gca,<span class="string">'xlim'</span>,[-1 21]);
<span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="tutRegr_04.png" alt=""> <img vspace="5" hspace="5" src="tutRegr_05.png" alt=""> <img vspace="5" hspace="5" src="tutRegr_06.png" alt=""> <h2>Overfitting, regularization and MAP estimation<a name="34"></a></h2><p>Using maximum likelihood to train a model  often results in overfitting. This means that the model fits the training set well, but is overly complex and consequently performs poorly on test data. We saw an example of that above, where fitting a high degree polynomial (with many parameters) to just N=21 data points results in a very 'wiggly' function that has poor performance on the test set.</p><p>Using Bayesian inference with an uninformative prior does not help. What we need is an informative prior, that encodes our preference for simpler models. A popular away to achieve this is to use a zero-mean spherical Gaussian prior of the form <img src="tutRegr_eq54469.png" alt="$p(w) = N(w|0,\alpha^{-1} I)$">, where <img src="tutRegr_eq87919.png" alt="$\alpha$"> is the precision (strength) of the prior. This says that, a priori, we expect the regression weights to be small, which means we believe the function is simple/ smooth (not "too wiggly"). We can compute the posterior of <img src="tutRegr_eq64535.png" alt="$w$"> with this prior using a variety of different models/ likelihood functions. But a computationally simpler approach is to use MAP estimation (aka regularization), which just computes the posterior mode, which is given by</p><p><img src="tutRegr_eq93496.png" alt="$$\hat{w} = \arg \max_w \log p(w|D) = \arg \max_w \log p(D|w) + \log p(w)$$"></p><p>In the case of a Gaussian likelihood (linear regression) and Gaussian prior, we get</p><p><img src="tutRegr_eq66205.png" alt="$$\log p(D|w) + \log p(w) = -\frac{\beta}{2} ||Xw - y||^2 -&#xA; \frac{\alpha}{2} ||w||^2 + \mbox{const}$$"></p><p>where <img src="tutRegr_eq35555.png" alt="$\beta=1/\sigma^2$"> is the precision of the measurement noise. If we define <img src="tutRegr_eq65989.png" alt="$\lambda = \alpha/ \beta$"> to be the amount of regularization, we can rewrite this as follows:</p><p><img src="tutRegr_eq19845.png" alt="$$\hat{w} = \arg \min_w ||Xw - y||^2 + \lambda ||w||^2$$"></p><p>We see that this is a least squares problem with an L2 penalty on the weight vector --- this is known as ridge regression.</p><p>If <img src="tutRegr_eq23351.png" alt="$\lambda$"> is too small, the model will overfit (since the function is too wiggly), but if it is too big, the model will underfit (since the function is too smooth). This is illustrated below, where we examine the mean squared error on the training and  test sets as a function of <img src="tutRegr_eq23351.png" alt="$\lambda$"> for a fixed degree 14 polynomial. This illustrates the characteristic U-shape on the test set (part of <a href="http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Decision_theory/linregPolyVsRegDemo.m">linregPolyVsRegDemo.m</a> ).</p><pre class="codeinput">clear <span class="string">all</span>
setSeed(0);
n=21;
[xtrain, ytrain, xtest, ytestNoisefree, ytest, sigma2] =<span class="keyword">...</span>
  polyDataMake(<span class="string">'sampling'</span>,<span class="string">'thibaux'</span>,<span class="string">'n'</span>,n);

<span class="comment">% Preprocess all data</span>
deg = 14;
ytrain = centerCols(ytrain);
ytest = centerCols(ytest);
pp = preprocessorCreate(<span class="string">'poly'</span>, deg, <span class="string">'rescaleX'</span>, true, <span class="string">'standardizeX'</span>, false, <span class="string">'addOnes'</span>, false);
[pp, Xtrain] = preprocessorApplyToTrain(pp, xtrain);
[Xtest] = preprocessorApplyToTest(pp, xtest);
<span class="comment">% No need for model to do any pre-processing.</span>
pp = preprocessorCreate( <span class="string">'standardizeX'</span>, false, <span class="string">'addOnes'</span>, false);

lambdas = logspace(-10,1.3,10);
NL = length(lambdas);
printNdx = round(linspace(2, NL-1, 3));
testMse = zeros(1,NL); trainMse = zeros(1,NL);
<span class="keyword">for</span> k=1:NL
  lambda = lambdas(k);
  [model] = linregFit(Xtrain, ytrain, <span class="string">'regtype'</span>, <span class="string">'L2'</span>, <span class="string">'lambda'</span>, lambda, <span class="string">'preproc'</span>, pp);
  [ypredTest, s2] = linregPredict(model, Xtest);
  ypredTrain = linregPredict(model, Xtrain);
  testMse(k) = mean((ypredTest - ytest).^2);
  trainMse(k) = mean((ypredTrain - ytrain).^2);
<span class="keyword">end</span>


hlam=figure; hold <span class="string">on</span>
ndx =  log(lambdas); <span class="comment">% 1:length(lambdas);</span>
plot(ndx, trainMse, <span class="string">'bs:'</span>, <span class="string">'linewidth'</span>, 2, <span class="string">'markersize'</span>, 12);
plot(ndx, testMse, <span class="string">'rx-'</span>, <span class="string">'linewidth'</span>, 2, <span class="string">'markersize'</span>, 12);
legend(<span class="string">'train mse'</span>, <span class="string">'test mse'</span>, <span class="string">'location'</span>, <span class="string">'northwest'</span>)
xlabel(<span class="string">'log lambda'</span>)
title(<span class="string">'mean squared error'</span>)
</pre><img vspace="5" hspace="5" src="tutRegr_07.png" alt=""> <p>Below we print the fitted function for certain chosen lambdas</p><pre class="codeinput"><span class="keyword">for</span> k=printNdx
  lambda = lambdas(k);
  [model] = linregFit(Xtrain, ytrain, <span class="string">'lambda'</span>, lambda, <span class="string">'preproc'</span>, pp);
  [ypredTest, s2] = linregPredict(model, Xtest);
  ypredTrain = linregPredict(model, Xtrain);
  sig = sqrt(s2);
  figure;
  scatter(xtrain, ytrain,<span class="string">'b'</span>,<span class="string">'filled'</span>);
  hold <span class="string">on</span>;
  plot(xtest, ypredTest, <span class="string">'k'</span>, <span class="string">'linewidth'</span>, 3);
  plot(xtest, ypredTest + sig, <span class="string">'b:'</span>);
  plot(xtest, ypredTest - sig, <span class="string">'b:'</span>);
  title(sprintf(<span class="string">'ln lambda %5.3f'</span>, log(lambda)))
<span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="tutRegr_08.png" alt=""> <img vspace="5" hspace="5" src="tutRegr_09.png" alt=""> <img vspace="5" hspace="5" src="tutRegr_10.png" alt=""> <h2>Cross validation approach to 'tuning' ridge regression<a name="44"></a></h2><p>One simple way to choose regularization parameters is cross validation. Below we show how to estimate the expected loss for a ridge regression model as we vary the regularizer. We use the <a href="http://matlabtools.googlecode.com/svn/trunk/stats/cvEstimate.m">cvEstimate.m</a> function, which can be used to estimate the frequentist risk of any estimation procedure (here each procedure corresponds to ridge regression with a different value of lambda) (based on <a href="http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Decision_theory/linregPolyVsRegDemo.m">linregPolyVsRegDemo.m</a> )</p><pre class="codeinput"><span class="keyword">for</span> k=1:NL
  lambda = lambdas(k);
  fitFn = @(Xtr,ytr) linregFit(Xtr, ytr, <span class="string">'lambda'</span>, lambda, <span class="string">'preproc'</span>, pp);
  predFn = @(mod, Xte) linregPredict(mod, Xte);
  lossFn = @(yhat, yte)  mean((yhat - yte).^2);
  N = size(Xtrain, 1);
  <span class="comment">%nfolds = N; % LOOCV</span>
  nfolds = 5;
  <span class="comment">% since the data is sorted left to right, we must randomize the order</span>
  [mu(k), se(k)] = cvEstimate(fitFn, predFn, lossFn, Xtrain, ytrain, nfolds, <span class="keyword">...</span>
    <span class="string">'randomizeOrder'</span>,false);
<span class="keyword">end</span>
</pre><p>We can plot the results as shown below.</p><pre class="codeinput">figure; hold <span class="string">on</span>
ndx =  log(lambdas); <span class="comment">% 1:length(lambdas);</span>
xlabel(<span class="string">'log lambda'</span>)
ylabel(<span class="string">'mse'</span>)
errorbar(ndx, mu, se, <span class="string">'ko-'</span>,<span class="string">'linewidth'</span>, 2, <span class="string">'markersize'</span>, 12 );
title(sprintf(<span class="string">'%d-fold cross validation, ntrain = %d'</span>, nfolds, N))
set(gca,<span class="string">'yscale'</span>,<span class="string">'log'</span>)
<span class="comment">% draw vertical line at best value</span>
dof = 1./(eps+lambdas);
idx_opt  = argmin(mu);
verticalLine(ndx(idx_opt), <span class="string">'color'</span>,<span class="string">'b'</span>, <span class="string">'linewidth'</span>,2);
</pre><img vspace="5" hspace="5" src="tutRegr_11.png" alt=""> <p>We see that it exhibits a U-shape similar to the test error. The vertical line denotes the best value.</p><p>Selecting a regularization parameter using CV is such a common operation that we have created a function called <a href="http://matlabtools.googlecode.com/svn/trunk/stats/fitCv.m">fitCv.m</a> that makes it a bit easier. You just pass in a grid of parameters, and fitting, prediction and loss funcions, and it calls <a href="http://matlabtools.googlecode.com/svn/trunk/stats/cvEstimate.m">cvEstimate.m</a> on each combination and picks the best. We give an example below.</p><pre class="codeinput">fitFn2 = @(Xtr,ytr,lam) linregFit(Xtr, ytr, <span class="string">'lambda'</span>, lam, <span class="string">'preproc'</span>, pp);
[model2, bestParam2, mu2, se2] = <span class="keyword">...</span>
    fitCv(lambdas, fitFn2, predFn, lossFn, Xtrain, ytrain,  nfolds, <span class="keyword">...</span>
    <span class="string">'randomizeOrder'</span>, false);
assert(approxeq(mu, mu2))
</pre><h2>Regularization paths<a name="51"></a></h2><p>For some models and regularizers of strength <img src="tutRegr_eq23351.png" alt="$\lambda$">, it is possible to efficiently compute MAP estimates for a whole set of <img src="tutRegr_eq23351.png" alt="$\lambda$"> values. This is known as the regularization path. In fact, computing the path can be faster than computing a single (lightly regularized) solution, since the estimate <img src="tutRegr_eq14356.png" alt="$\hat{w}(\lambda)$"> can be used to warm-start the optimizer when computing <img src="tutRegr_eq23132.png" alt="$\hat{w}(\lambda')$"> for <img src="tutRegr_eq06975.png" alt="$\lambda' < \lambda$">.</p><p>The package <a href="http://www-stat.stanford.edu/~tibs/glmnet-matlab/">glmnet</a> supports the computation of regularization paths for linear and logistic regression using L2 and L1 regularization, using a coordinate descent method. It automatically computes a suitable range of <img src="tutRegr_eq23351.png" alt="$\lambda$"> values, depending on the data and type of regularizer. glmnet is included in pmtkSupport.</p><p>Of course, the regularization path is not a model, it is a set of models. But can use CV to pick the best. However, using <a href="http://matlabtools.googlecode.com/svn/trunk/stats/fitCv.m">fitCv.m</a> to do this would be inefficient, since it loops over models, and then loops over folds internally (inside <a href="http://matlabtools.googlecode.com/svn/trunk/stats/cvEstimate.m">cvEstimate.m</a> ): it is more efficient to loop over folds on the outside, and compute all the models at once on each fold in the inner loop.</p><p>The function <a href="http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/linearRegression/linregFitPathCv.m">linregFitPathCv.m</a> does this in the case of linear regression with an L2 or L1 regularizer. Below we apply it to our running example and show that the estimated error has a similar profile to before.</p><pre class="codeinput">[bestModel, path] = linregFitPathCv(Xtrain, ytrain, <span class="string">'regtype'</span>, <span class="string">'l2'</span>,  <span class="string">'preproc'</span>, pp,<span class="keyword">...</span>
   <span class="string">'nfolds'</span>, nfolds);
figure;
plot(path.cvErr, <span class="string">'linewidth'</span>, 2)
</pre><pre class="codeoutput">Warning: In the directory "C:\kmurphy\GoogleCode\pmtkSupport\glmnet-matlab", glmnetMex.mexw32 now shadows glmnetMex.dll.
 Please see the MATLAB 7.1 Release Notes. 
</pre><img vspace="5" hspace="5" src="tutRegr_12.png" alt=""> <h2>Bayesian approach to 'tuning'  ridge regression<a name="53"></a></h2><p>A Bayesian alternative to cross validation is to to compute the probability of each value of the regularizer, <img src="tutRegr_eq67685.png" alt="$p(\alpha|D) \propto p(D|\alpha) p(\alpha)$">. If we use a uniform prior for <img src="tutRegr_eq87919.png" alt="$\alpha$">, we can focus on the <img src="tutRegr_eq23729.png" alt="$p(D|\alpha)$"> term, which is called the marginal likelihood  or evidence. This quantity is given by</p><p><img src="tutRegr_eq68090.png" alt="$$p(D|\alpha,\beta) = \int p(y|X,w,\beta) p(w|\alpha) dw $$"></p><p>where, for simplicity, we assumed a known noise precision <img src="tutRegr_eq42727.png" alt="$\beta$">. For linear regression with a Gaussian prior, we can compute this in closed form.</p><pre class="codeinput">beta = 1/sigma2;
alphas = beta * lambdas;

<span class="keyword">for</span> k=1:NL
  lambda = lambdas(k);
  [model, logev(k)] = linregFitBayes(Xtrain, ytrain, <span class="string">'preproc'</span>, pp, <span class="keyword">...</span>
    <span class="string">'prior'</span>, <span class="string">'gauss'</span>, <span class="string">'alpha'</span>, alphas(k), <span class="string">'beta'</span>, beta);
<span class="keyword">end</span>
</pre><p>When we plot the log evidence vs alpha, it exhibits the same (inverted) U shape as the test error, as shown below (extracted from <a href="http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Decision_theory/linregPolyVsRegDemo.m">linregPolyVsRegDemo.m</a> )</p><pre class="codeinput">figLogev = figure;
plot(log(alphas), logev, <span class="string">'ko-'</span>, <span class="string">'linewidth'</span>, 2, <span class="string">'markersize'</span>, 12);
xlabel(<span class="string">'log alpha'</span>)
title(<span class="string">'log evidence'</span>)
</pre><img vspace="5" hspace="5" src="tutRegr_13.png" alt=""> <h2>Empirical Bayes for ridge regression<a name="60"></a></h2><p>One important practical benefit of the Bayesian approach is that we can use numerical optimization to pick the regularizer, using</p><p><img src="tutRegr_eq04832.png" alt="$$\hat{\alpha} = \arg \max p(D|\alpha)$$"></p><p>This is called empirical Bayes, ML-II estimation, or the 'evidence procedure'. By contrast, with CV we are restricted to performing a discrete search over a finite grid of values. This is not tractable when we have multiple regularization parameters to tune.</p><p>In pmtk3, we just specify that the prior is of type 'eb', which stands for empirical Bayes. This is illustrated below. The vertical line corresponds to the EB estimate.</p><pre class="codeinput">[modelEB, logevEB] = linregFitBayes(Xtrain, ytrain, <span class="string">'preproc'</span>, pp, <span class="string">'prior'</span>, <span class="string">'eb'</span>);
alphaEB = modelEB.netlab.alpha;
figure(figLogev);
verticalLine(log(alphaEB), <span class="string">'linewidth'</span>, 3, <span class="string">'color'</span>, <span class="string">'r'</span>);
</pre><img vspace="5" hspace="5" src="tutRegr_14.png" alt=""> <p>The EB code uses the <a href="http://www1.aston.ac.uk/eas/research/groups/ncrg/resources/netlab/">netlab</a> package, written by Ian Nabney. This is included in <a href="http://code.google.com/p/pmtksupport/">pmtkSupport</a>.</p><h2>Variational Bayes for ridge regression<a name="65"></a></h2><p>An alternative to EB is to use variational Bayes to infer the posterior over <img src="tutRegr_eq87919.png" alt="$\alpha$"> and <img src="tutRegr_eq42727.png" alt="$\beta$">. This is illustrated below. The posterior mean value for <img src="tutRegr_eq87919.png" alt="$\alpha$"> is shown by the blue line. We see this is very similar to the EB estimate, since we used a vague prior for <img src="tutRegr_eq87919.png" alt="$\alpha$">.</p><pre class="codeinput">[modelVB, logevVB] = linregFitBayes(Xtrain, ytrain, <span class="string">'preproc'</span>, pp, <span class="string">'prior'</span>, <span class="string">'vb'</span>);
alphaVB = modelVB.expectAlpha;
figure(figLogev);
verticalLine(log(alphaVB), <span class="string">'linewidth'</span>, 3, <span class="string">'color'</span>, <span class="string">'b'</span>);
</pre><img vspace="5" hspace="5" src="tutRegr_15.png" alt=""> <p>The VB code uses functions written by <a href="http://www.bcs.rochester.edu/people/jdrugowitsch/code.html">Jan Drugowitsch</a>. This is included in <a href="http://code.google.com/p/pmtksupport/">pmtkSupport</a>.</p><h2>Sparse linear regression<a name="68"></a></h2><p>There has been a lot of recent interest in models which only use a subset of the features/ variables. These are known as sparse models. pmtk supports several ways of promoting sparsity, and selecting relevant variables, some of which we discuss below.</p><h2>L1 regularization<a name="69"></a></h2><p>The best known way to encourage sparsity is to use L1 regularization. This is equivalent to MAP estimation under a Laplace prior. When combined with linear  regression, this method is known as lasso. The overall objective is convex but nonsmooth, since the log prior is not differentiable at the origin.</p><p>There are many different optimization algorithms one can use; this is currently a hot topic of research. By default, we use Mark Schmidt's <a href="http://www.cs.ubc.ca/~schmidtm/Software/L1General/L1General.html">L1general</a> package, which can be used for a variety of different models. However, we can also use the Stephen Boyd's interior point method, <a href="http://www.stanford.edu/~boyd/l1_ls/">lsls</a>.</p><p>As an example, consider the prostate cancer dataset used in the Hastie book. We can compute the regularization path using warm starting as follows (from <a href="http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Variable_selection_and_sparsity/lassoPathProstate.m">lassoPathProstate.m</a> )</p><pre class="codeinput">clear <span class="string">all</span>
load <span class="string">prostateStnd</span>
lambdaMax =  lambdaMaxLasso(X, y);
NL = 30;
lambdas  =  logspace(log10(lambdaMax), -2, NL);
pp = preprocessorCreate();
D = size(X,2);
winit = zeros(D,1);
<span class="keyword">for</span> i=1:NL
  lambda = lambdas(i);
  model = linregFit(X, y, <span class="string">'lambda'</span>, lambda, <span class="string">'regType'</span>, <span class="string">'L1'</span>, <span class="keyword">...</span>
    <span class="string">'preproc'</span>, pp, <span class="string">'winit'</span>, winit);
  weightsLam(i,:) = rowvec(model.w);
  winit = model.w;
<span class="keyword">end</span>
figure;
plot(log(lambdas), weightsLam, <span class="string">'o-'</span>, <span class="string">'linewidth'</span>, 2);
legend(names{1:end-1}, <span class="string">'Location'</span>, <span class="string">'NorthWest'</span>);
xlabel(<span class="string">'log(lambda)'</span>);
</pre><img vspace="5" hspace="5" src="tutRegr_16.png" alt=""> <p>We can set fitFnName to 'l1ls' if desired; this gives the same results as above. Alternatively, we can use the path fitting algorithm from glmnet, and then use CV to pick the best model, as follows.</p><pre class="codeinput">options = glmnetSet();
options.nlambda = NL;
[bestModel, path] = linregFitPathCv(X, y, <span class="string">'regtype'</span>, <span class="string">'l1'</span>,  <span class="string">'preproc'</span>, pp,<span class="keyword">...</span>
  <span class="string">'options'</span>, options, <span class="string">'nfolds'</span>, 5);
ndxKeep = find(abs(bestModel.w) &gt; 0)
figure;
lambdas2 = rowvec(path.lambdas);
plot(path.w', <span class="string">'-o'</span>, <span class="string">'LineWidth'</span>, 2);
legend(names{1:size(X, 2)}, <span class="string">'Location'</span>, <span class="string">'NorthEast'</span>);
hold <span class="string">on</span>
bestNdx = find(bestModel.lambda==lambdas2);
verticalLine(bestNdx, <span class="string">'linewidth'</span>, 2, <span class="string">'color'</span>, <span class="string">'r'</span>);
</pre><pre class="codeoutput">Warning: In the directory "C:\kmurphy\GoogleCode\pmtkSupport\glmnet-matlab", glmnetMex.mexw32 now shadows glmnetMex.dll.
 Please see the MATLAB 7.1 Release Notes. 
ndxKeep =
     1
     2
     3
     6
</pre><img vspace="5" hspace="5" src="tutRegr_17.png" alt=""> <p>We see that the prediction optimal model contains features 1,2,3,6. Using CV to select lambda for lasso is not model selection consistent. Here we use it as a means of promoting sparsity for computational and statistical efficiency reasons, rather than as a method of detecting relevant variables.</p><h2>Automatic relevancy determination (ARD)<a name="74"></a></h2><p>An alternative way to promote sparsity is to use a diagonal Gaussian prior combined with empirical or variational Bayes. The posterior precisions will be driven to infinity for the irrelevant features. This feature is implemented in Drugowitch's VB code, and can be called from pmtk as follows.</p><pre class="codeinput">[modelVB] = linregFitBayes(X, y, <span class="string">'preproc'</span>, pp, <span class="string">'prior'</span>, <span class="string">'vb'</span>, <span class="string">'useARD'</span>, true);
alphaVB = modelVB.expectAlpha;
ndxKeepVB = find(alphaVB &lt; 0.5*max(alphaVB))
</pre><pre class="codeoutput">ndxKeepVB =
     1
     2
     5
</pre><p>As is commonly the case, ARD picks a sparser model than L1. In this example, it only keeps features 1,2,5, although this obviously depends on the threshold used.</p><p>
<hr>
</p><p>This page was auto-generated by calling <i>pmtkPublish(C:\kmurphy\GoogleCode\pmtk3\docs\tutorial\tutRegr.m)</i>  on 08-Sep-2010 17:21:14</p><p class="footer"><br>
      Published with MATLAB&reg; 7.9<br></p></div><!--
##### SOURCE BEGIN #####
%% Regression using parametric discriminative models in pmtk3
%
% Many of the basic concepts of supervised learning,
% and their pmtk3 implementation,
% can be explained using linear regression as a model.
% We can handle non-linear relationships
% using basis function expansion, as we explain below.
% We will discuss categorical responses (classification) later.

%% Fitting a linear regression model by maximum likelihood
% As a basic example, we can fit a linear regression model to a
% data set using maximum likelihood estimation
% as follows (extracted from <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Discriminative_models_for_regression_and_classification/linregBayesCaterpillar.m linregBayesCaterpillar.m> ):
%%
clear all
X = loadData('caterpillar'); % from http://www.ceremade.dauphine.fr/~xian/BCS/caterpillar
y = log(X(:,11)); % log number of nests
X = X(:,1:10);
[model] = linregFit(X, y) 
%%
% Let us check that this matches the usual equation for the MLE
% (adding a column of 1s to deal with the offset term)
%%
X1 = [ones(size(X,1),1) X];
wOLS = X1\y;
assert(approxeq(model.w, wOLS))
%%


%% Bayesian parameter inference
% If you fit by the model by MLE, you can examine the
% value of the estimated parameters by typing |model.params|,
% where params is the name of the parameter (here w or sigma2).
% But what if we want to know how much confidence
% we should have in these estimates?
% For this, we should use Bayesian inference.
% We can compute the posterior distribution of the
% parameters given the data and an uninformative prior
% as follows:
%%
[modelB, logev, postSummary] = linregFitBayes(X, y, 'prior', 'uninf');
%%
% Here |modelB| is the model which contains
% the posterior of the parameters:
%%
modelB
%%
% There is no longer a field called |w| or |sigma2|
% since we are not using point estimation.
% Instead, modelB contains the parameters
% of the posterior,
% which has the following form
%%
% $$p(w,\sigma^2|D)  = N(w|w_N, V_N) IG(\sigma^2|a_N,b_N)$$
%%
% Since we used an uninformative prior,
% the posterior mean is the same as the MLE
%%
assert(approxeq(wOLS, modelB.wN))
%%
%
% logev is the log evidence, or marginal likelihood,
% and is a measure of 'goodness of fit' of the overall model:
%%
% $$p(D) = \int \int p(D|w,\sigma^2) p(w,\sigma^2) d w d \sigma^2 $$
%%
% This can only be computed if we use a proper prior,
% not an uninformative prior.
% Hence in this case |logev=[]|.
%%
% |postSummary| is a summary of the posterior.
% It contains the 
% the posterior mean, standard deviation and 95% credible interval
% of each regression parameters.
% It also determines if each 
%  coefficient is significantly different from 0,
% based on whether its 95% CI excludes 0.
%%
postSummary
%%
% We can print the posterior summary as a table using the command
% below. We put 
% a little * next to the significant coefficients.
%%
[modelB, logev, postSummary] = linregFitBayes(X, y, 'prior', 'uninf', ...
  'displaySummary', true);
%%
% We see that coefficients  0, 1, 2, 4, 5 are "significant"
% by this measure. (Other methods of testing significance, based on Bayes factors, can also be
% used, but are a bit more complicated to implement, and one cannot use uninformative
% priors when using Bayes factors.)
%
% Note that pmtk currenlty has rather limited support for Bayesian
% model fitting, and not all Bayesian model fitting procedures currently implement
% this posterior summary feature.

%% Frequentist parameter inference
% It turns out that in this particular example, the Bayesian analysis is
% identical to a classical frequentist analysis (because the posterior
% for linear regression under an uninformative Jeffreys prior is equivalent
% to the sampling distribution of the MLE). To see this, let us use the
% <http://www.mathworks.com/products/statistics/ stats toolbox> to fit the model and perform a frequentist analysis:
%%
X1 = [ones(size(X,1),1), X];
[b, bint] = regress(y, X1);
% b(j) is coefficient j, bint(j,:) = lower and upper 95% conf interval
assert(approxeq(b, postSummary.what))
assert(approxeq(bint, postSummary.credint))
for i=1:length(b)
  fprintf('%8.3f, [%8.3f, %8.3f]\n', b(i), bint(i,1), bint(i,2));
end
fprintf('\n');
%%
% We see that the MLE is the same as the posterior mean,
% and the 95% frequentist confidence interval is the same as the 95%
% Bayesian credible interval.
%
%
% In general, a Bayesian and frequentist analysis may not give the same
% results. In pmtk, all inference is Bayesian.
% However, pmtk supports some non-Bayesian estimation methods,
% such as cross validation, as we will see below.




%% Prediction with linear regression
% In machine learning, we usually care more about prediction than in trying
% to interpret the fitted parameters (especially since many models of
% interest are hard to interpret or even strictly unidentifiable).
%
% As an example, consider fitting a linear regression model to some 1d data
% using MLE and Bayesian methods (using <http://pmtk3.googlecode.com/svn/trunk/demos/otherDemos/supervisedModels/linregPostPredLinearDemo.m linregPostPredLinearDemo.m> ),
% and then plotting the predictions on a test set (which is just a grid of
% points in the interval [-7,7])
%%
setSeed(1);
[xtrain, ytrain, xtest] =  polyDataMake('sampling', 'sparse', 'deg', 2);

fitMethods = {@(x,y) linregFit(x,y), @(x,y) linregFitBayes(x,y)};
predictMethods = {@(x,y) linregPredict(x,y), @(x,y) linregPredictBayes(x,y)};
names = {'MLE', 'Bayes'};

for i=1:length(fitMethods)
  model = fitMethods{i}(xtrain, ytrain);
  [mu, v] = predictMethods{i}(model, xtest);
  figure; hold on
  plot(xtest, mu,  'k-', 'linewidth', 3, 'displayname', 'prediction');
  plot(xtrain,ytrain,'ro','markersize', 14, 'linewidth', 3, ...
     'displayname', 'training data');
  NN = length(xtest);
  ndx = 1:5:NN; % plot subset of errorbars to reduce clutter
  sigma = sqrt(v);
  legend('location', 'northwest');
  errorbar(xtest(ndx), mu(ndx), sigma(ndx));
  title(names{i});
end
%%
% The predicted means (black lines) are the same, but  in the plugin case, the predicted
% variance is constant, whereas in the Bayesian case, the predicted
% variance increases as we move further away from the training data, as it
% should, since our uncertainty increases as we extrapolate further.
%

%% Preprocessing and basis function expansion
% We are free to preprocess the data in any way we choose before fitting the model.
% In pmtk, you can create a preprocessor (pp) 'object', and then pass it to the fitting function;
% the pp will  be applied to the training data before fitting the model, and will be applied again to the test data.
% The advantage of this approach is that the pp is stored inside the model,
% which reduces the chance of applying inconsistent transformations to
% training and test data.
%
% One common form of preprocessing is basis function expansion.
% This replaces the original features with a larger set,
% thus providing an easy way to fit nonlinear models.
% A simple example of this is polynomial expansion:
%%
% $$\phi(x) = (1, x, x^2, \ldots, x^d)$$
%%
% Below we give a simple example
% where we fit polynomials of increasing degree
% to 1d data. (We first scale the data before
% comouting a polynomial expansion, for reasons of numerical
% stability. As usual, we add a column of 1s to handle the
% offset term.) We then compute the mean
% squared error on the training and test sets
% (part of <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Introduction/linregPolyVsDegree.m linregPolyVsDegree.m> )
%%
N = 21;
[xtrain, ytrain, xtest, ytestNoisefree, ytest] = ...
    polyDataMake('sampling','thibaux', 'n', N);
degs = 1:20;
Nm = length(degs);
% Plot error vs degree
mseTrain = zeros(1,Nm); mseTest = zeros(1,Nm);
for m=1:length(degs)
    deg = degs(m);
    pp = preprocessorCreate('rescaleX', true, 'poly', deg, 'addOnes', true);
    model = linregFit(xtrain, ytrain, 'preproc', pp);
    ypredTrain = linregPredict(model, xtrain);
    ypredTest = linregPredict(model, xtest);
    mseTrain(m) = mean((ytrain-ypredTrain).^2);
    mseTest(m) = mean((ytest-ypredTest).^2);
end
ndx = (degs<=16);
figure;
hold on
plot(degs(ndx), mseTrain(ndx), 'bs:', 'linewidth', 2, 'markersize', 12);
plot(degs(ndx), mseTest(ndx), 'rx-', 'linewidth', 2, 'markersize', 12);
xlabel('degree')
ylabel('mse')
legend('train', 'test')
%%
% We see the characteristic U-shaped curve on the test
% set: on the left, the model is too simple (underfits)
% and on the right, the model is too complex (overfits).
% Below we plot some of the fitted functions
% for different degrees:
%%
for deg = [2, 14, 20]
    pp = preprocessorCreate('rescaleX', true, 'poly', deg, 'addOnes', true);
    model = linregFit(xtrain, ytrain, 'preproc', pp);
    ypredTrain = linregPredict(model, xtrain);
    ypredTest = linregPredict(model, xtest);
    mseTrain(m) = mean((ytrain-ypredTrain).^2);
    mseTest(m) = mean((ytest-ypredTest).^2);
    
    figure;
    scatter(xtrain,ytrain,'b','filled');
    hold on;
    plot(xtest, ypredTest, 'k', 'linewidth', 3);
    hold off
    title(sprintf('degree %d', deg))
    set(gca,'ylim',[-10 15]);
    set(gca,'xlim',[-1 21]);
end

%% Overfitting, regularization and MAP estimation
% Using maximum likelihood to train a model  often results in overfitting.
% This means that the model fits the training set well, but is overly complex
% and consequently performs poorly on test data.
% We saw an example of that above, where fitting a high degree
% polynomial (with many parameters) to just N=21 data points
% results in a very 'wiggly' function that has poor
% performance on the test set.
%
% Using Bayesian inference with an uninformative prior does not help.
% What we need is an informative prior, that encodes our preference for
% simpler models. A popular away to achieve this is to use a zero-mean spherical
% Gaussian prior of the form $p(w) = N(w|0,\alpha^{-1} I)$,
% where $\alpha$ is the precision (strength) of the prior. This says
% that, a priori, we expect the regression weights to be small, which
% means we believe the function is simple/ smooth (not "too wiggly").
% We can compute the posterior of $w$ with this prior using a variety of
% different models/ likelihood functions. But a computationally simpler
% approach is to use MAP estimation (aka regularization), which just computes the posterior
% mode, which is given by
%%
% $$\hat{w} = \arg \max_w \log p(w|D) = \arg \max_w \log p(D|w) + \log p(w)$$
%%
% In the case of a Gaussian likelihood (linear regression) and Gaussian
% prior, we get
%%
% $$\log p(D|w) + \log p(w) = -\frac{\beta}{2} ||Xw - y||^2 - 
%  \frac{\alpha}{2} ||w||^2 + \mbox{const}$$
%%
% where $\beta=1/\sigma^2$ is the precision of the measurement noise.
% If we define $\lambda = \alpha/ \beta$ to be the amount of regularization,
% we can rewrite this as follows:
%%
% $$\hat{w} = \arg \min_w ||Xw - y||^2 + \lambda ||w||^2$$
%%
% We see that this is a least squares problem with an L2 penalty on the
% weight vector REPLACE_WITH_DASH_DASH- this is known as ridge regression.
%
% If $\lambda$ is too small, the model will overfit (since the function
% is too wiggly), but if it is too big, the model will underfit
% (since the function is too smooth). This is illustrated below, where we
% examine the mean squared error on the training and  test sets as a function
% of $\lambda$ for a fixed degree 14 polynomial.
% This illustrates the characteristic U-shape on the test
% set (part of <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Decision_theory/linregPolyVsRegDemo.m linregPolyVsRegDemo.m> ).
%%
clear all
setSeed(0);
n=21;
[xtrain, ytrain, xtest, ytestNoisefree, ytest, sigma2] =...
  polyDataMake('sampling','thibaux','n',n);

% Preprocess all data
deg = 14;
ytrain = centerCols(ytrain);
ytest = centerCols(ytest);
pp = preprocessorCreate('poly', deg, 'rescaleX', true, 'standardizeX', false, 'addOnes', false);
[pp, Xtrain] = preprocessorApplyToTrain(pp, xtrain);
[Xtest] = preprocessorApplyToTest(pp, xtest);
% No need for model to do any pre-processing.
pp = preprocessorCreate( 'standardizeX', false, 'addOnes', false);

lambdas = logspace(-10,1.3,10);
NL = length(lambdas);
printNdx = round(linspace(2, NL-1, 3));
testMse = zeros(1,NL); trainMse = zeros(1,NL);
for k=1:NL
  lambda = lambdas(k);
  [model] = linregFit(Xtrain, ytrain, 'regtype', 'L2', 'lambda', lambda, 'preproc', pp);
  [ypredTest, s2] = linregPredict(model, Xtest);
  ypredTrain = linregPredict(model, Xtrain);
  testMse(k) = mean((ypredTest - ytest).^2);
  trainMse(k) = mean((ypredTrain - ytrain).^2);
end


hlam=figure; hold on
ndx =  log(lambdas); % 1:length(lambdas);
plot(ndx, trainMse, 'bs:', 'linewidth', 2, 'markersize', 12);
plot(ndx, testMse, 'rx-', 'linewidth', 2, 'markersize', 12);
legend('train mse', 'test mse', 'location', 'northwest')
xlabel('log lambda')
title('mean squared error')

%%
% Below we print the fitted function for certain chosen lambdas
%%
for k=printNdx
  lambda = lambdas(k);
  [model] = linregFit(Xtrain, ytrain, 'lambda', lambda, 'preproc', pp);
  [ypredTest, s2] = linregPredict(model, Xtest);
  ypredTrain = linregPredict(model, Xtrain);
  sig = sqrt(s2);
  figure;
  scatter(xtrain, ytrain,'b','filled');
  hold on;
  plot(xtest, ypredTest, 'k', 'linewidth', 3);
  plot(xtest, ypredTest + sig, 'b:');
  plot(xtest, ypredTest - sig, 'b:');
  title(sprintf('ln lambda %5.3f', log(lambda)))
end


%% Cross validation approach to 'tuning' ridge regression
% One simple way to choose regularization parameters is cross validation.
% Below we show how to estimate the expected loss for a ridge
% regression model as we vary the regularizer.
% We use the <http://matlabtools.googlecode.com/svn/trunk/stats/cvEstimate.m cvEstimate.m> function, which can be used
% to estimate the frequentist risk of any estimation procedure
% (here each procedure corresponds to ridge regression with a
% different value of lambda)
% (based on <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Decision_theory/linregPolyVsRegDemo.m linregPolyVsRegDemo.m> )
%%
for k=1:NL
  lambda = lambdas(k);
  fitFn = @(Xtr,ytr) linregFit(Xtr, ytr, 'lambda', lambda, 'preproc', pp);
  predFn = @(mod, Xte) linregPredict(mod, Xte);
  lossFn = @(yhat, yte)  mean((yhat - yte).^2);
  N = size(Xtrain, 1);
  %nfolds = N; % LOOCV
  nfolds = 5;
  % since the data is sorted left to right, we must randomize the order
  [mu(k), se(k)] = cvEstimate(fitFn, predFn, lossFn, Xtrain, ytrain, nfolds, ...
    'randomizeOrder',false);
end

%%
% We can plot the results as shown below.
%%
figure; hold on
ndx =  log(lambdas); % 1:length(lambdas);
xlabel('log lambda')
ylabel('mse')
errorbar(ndx, mu, se, 'ko-','linewidth', 2, 'markersize', 12 );
title(sprintf('%d-fold cross validation, ntrain = %d', nfolds, N))
set(gca,'yscale','log')
% draw vertical line at best value
dof = 1./(eps+lambdas);
idx_opt  = argmin(mu);
verticalLine(ndx(idx_opt), 'color','b', 'linewidth',2);
%%
% We see that it exhibits a U-shape similar to the test error.
% The vertical line denotes the best value.
%
% Selecting a regularization parameter using CV is such
% a common operation that we have created a function
% called <http://matlabtools.googlecode.com/svn/trunk/stats/fitCv.m fitCv.m> that makes it a bit easier.
% You just pass in a grid of parameters,
% and fitting, prediction and loss funcions,
% and it calls <http://matlabtools.googlecode.com/svn/trunk/stats/cvEstimate.m cvEstimate.m> on each combination
% and picks the best.
% We give an example below.
%%
fitFn2 = @(Xtr,ytr,lam) linregFit(Xtr, ytr, 'lambda', lam, 'preproc', pp);
[model2, bestParam2, mu2, se2] = ...
    fitCv(lambdas, fitFn2, predFn, lossFn, Xtrain, ytrain,  nfolds, ...
    'randomizeOrder', false);
assert(approxeq(mu, mu2))
%%

%% Regularization paths
% For some models and regularizers of strength $\lambda$, it is possible to efficiently
% compute MAP estimates for a whole set of $\lambda$ values.
% This is known as the regularization path.
% In fact, computing the path can be faster than computing
% a single (lightly regularized) solution,
% since the estimate $\hat{w}(\lambda)$
% can be used to warm-start the optimizer when computing
% $\hat{w}(\lambda')$ for $\lambda' < \lambda$.
%
% The package <http://www-stat.stanford.edu/~tibs/glmnet-matlab/ glmnet>
% supports the computation of regularization paths for linear
% and logistic regression using L2 and L1 regularization,
% using a coordinate descent method.
% It automatically computes a suitable range of $\lambda$ values,
% depending on the data and type of regularizer.
% glmnet is included in pmtkSupport.
%
% Of course, the regularization path is not a model, it is a set
% of models. But can use CV to pick the best.
% However, using <http://matlabtools.googlecode.com/svn/trunk/stats/fitCv.m fitCv.m> to do this would be inefficient,
% since it loops over models, and then loops over folds internally
% (inside <http://matlabtools.googlecode.com/svn/trunk/stats/cvEstimate.m cvEstimate.m> ): it is more efficient to loop
% over folds on the outside, and compute all the models at once
% on each fold in the inner loop.
%
% The function <http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/linearRegression/linregFitPathCv.m linregFitPathCv.m> does this in the case of linear
% regression with an L2 or L1 regularizer.
% Below we apply it to our running example and show that the estimated
% error has a similar profile to before. 
%%
[bestModel, path] = linregFitPathCv(Xtrain, ytrain, 'regtype', 'l2',  'preproc', pp,...
   'nfolds', nfolds);
figure;
plot(path.cvErr, 'linewidth', 2)



%% Bayesian approach to 'tuning'  ridge regression
% A Bayesian alternative to cross validation is to
% to compute the probability of each value of the regularizer,
% $p(\alpha|D) \propto p(D|\alpha) p(\alpha)$.
% If we use a uniform prior for $\alpha$, we can focus on the
% $p(D|\alpha)$ term, which is called the marginal likelihood  or evidence.
% This quantity is given by
%%
% $$p(D|\alpha,\beta) = \int p(y|X,w,\beta) p(w|\alpha) dw $$
%%
% where, for simplicity, we assumed a known noise 
% precision $\beta$.
% For linear regression with a Gaussian prior,
% we can compute this in closed form.
%%
beta = 1/sigma2;
alphas = beta * lambdas;

for k=1:NL
  lambda = lambdas(k);
  [model, logev(k)] = linregFitBayes(Xtrain, ytrain, 'preproc', pp, ...
    'prior', 'gauss', 'alpha', alphas(k), 'beta', beta);
end
%%
%
% When we plot the log evidence vs alpha,
% it exhibits the same (inverted) U shape as the test error,
% as shown below
% (extracted from <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Decision_theory/linregPolyVsRegDemo.m linregPolyVsRegDemo.m> )
%%
figLogev = figure;
plot(log(alphas), logev, 'ko-', 'linewidth', 2, 'markersize', 12);
xlabel('log alpha')
title('log evidence')
%%

%% Empirical Bayes for ridge regression
% One important practical benefit of the Bayesian approach is that we can 
% use numerical optimization to pick the regularizer,
% using 
%%
% $$\hat{\alpha} = \arg \max p(D|\alpha)$$
%%
% This is called empirical Bayes, ML-II estimation,
% or the 'evidence procedure'.
% By contrast, with CV we are restricted to performing
% a discrete search over a finite grid
% of values. This is not tractable when we have
% multiple regularization parameters to tune.
%
% In pmtk3, we just specify that the prior is of type
% 'eb', which stands for empirical Bayes. This is illustrated below.
% The vertical line corresponds to the EB estimate.
%%
[modelEB, logevEB] = linregFitBayes(Xtrain, ytrain, 'preproc', pp, 'prior', 'eb');
alphaEB = modelEB.netlab.alpha;
figure(figLogev);
verticalLine(log(alphaEB), 'linewidth', 3, 'color', 'r');
%%
% The EB code uses the 
% <http://www1.aston.ac.uk/eas/research/groups/ncrg/resources/netlab/
% netlab> package, written by Ian Nabney.
% This is included in
% <http://code.google.com/p/pmtksupport/ pmtkSupport>.

%% Variational Bayes for ridge regression
% An alternative to EB is to use variational Bayes to infer
% the posterior over $\alpha$ and $\beta$. This is illustrated
% below. The posterior mean value for $\alpha$ is
% shown by the blue line. We see this is very similar
% to the EB estimate, since we used a vague prior for $\alpha$.
%%
[modelVB, logevVB] = linregFitBayes(Xtrain, ytrain, 'preproc', pp, 'prior', 'vb');
alphaVB = modelVB.expectAlpha;
figure(figLogev);
verticalLine(log(alphaVB), 'linewidth', 3, 'color', 'b');
%%
% The VB code uses functions written by
% <http://www.bcs.rochester.edu/people/jdrugowitsch/code.html Jan
% Drugowitsch>. 
% This is included in
% <http://code.google.com/p/pmtksupport/ pmtkSupport>.

%% Sparse linear regression
% There has been a lot of recent interest in models which only
% use a subset of the features/ variables. These are known as 
% sparse models. pmtk supports several ways of promoting sparsity,
% and selecting relevant variables, some of which we discuss below.
%
%% L1 regularization 
% The best known way to encourage sparsity
% is to use L1 regularization.
% This is equivalent to MAP estimation under a Laplace prior.
% When combined with linear  regression, this method
% is known as lasso.
% The overall objective is convex but nonsmooth,
% since the log prior
% is not differentiable at the origin.
%
% There are many different optimization algorithms one can use; this is currently
% a hot topic of research.
% By default, we use Mark Schmidt's
% <http://www.cs.ubc.ca/~schmidtm/Software/L1General/L1General.html
% L1general> package, which can be used for a variety of different models.
% However, we can also use the
% Stephen Boyd's interior point method,
% <http://www.stanford.edu/~boyd/l1_ls/ lsls>.
%
% As an example, consider the prostate cancer dataset used 
% in the Hastie book.
% We can compute the regularization path using warm starting as follows
% (from <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Variable_selection_and_sparsity/lassoPathProstate.m lassoPathProstate.m> )
%%
clear all
load prostateStnd
lambdaMax =  lambdaMaxLasso(X, y);
NL = 30;
lambdas  =  logspace(log10(lambdaMax), -2, NL); 
pp = preprocessorCreate();
D = size(X,2);
winit = zeros(D,1);
for i=1:NL
  lambda = lambdas(i);
  model = linregFit(X, y, 'lambda', lambda, 'regType', 'L1', ...
    'preproc', pp, 'winit', winit);
  weightsLam(i,:) = rowvec(model.w);
  winit = model.w;
end
figure;
plot(log(lambdas), weightsLam, 'o-', 'linewidth', 2);
legend(names{1:end-1}, 'Location', 'NorthWest');
xlabel('log(lambda)');
%% 
% We can set fitFnName to 'l1ls' if desired; this gives the
% same results as above.
% Alternatively, we can use the path fitting algorithm
% from glmnet, and then use CV to pick the best model,
% as follows.
%%
options = glmnetSet();
options.nlambda = NL;
[bestModel, path] = linregFitPathCv(X, y, 'regtype', 'l1',  'preproc', pp,...
  'options', options, 'nfolds', 5);
ndxKeep = find(abs(bestModel.w) > 0) 
figure;
lambdas2 = rowvec(path.lambdas);
plot(path.w', '-o', 'LineWidth', 2);
legend(names{1:size(X, 2)}, 'Location', 'NorthEast');
hold on
bestNdx = find(bestModel.lambda==lambdas2);
verticalLine(bestNdx, 'linewidth', 2, 'color', 'r');
%%
% We see that the prediction optimal model contains
% features 1,2,3,6.
% Using CV to select lambda for lasso
% is not model selection consistent.
% Here we use it as a means of promoting sparsity
% for computational and statistical efficiency reasons,
% rather than as a method of detecting relevant variables.

%% Automatic relevancy determination (ARD)
% An alternative way to promote sparsity is to use a diagonal Gaussian
% prior combined with empirical or variational Bayes.
% The posterior precisions will be driven to infinity for the
% irrelevant features.
% This feature is implemented in Drugowitch's VB code,
% and can be called from pmtk as follows.
%%
[modelVB] = linregFitBayes(X, y, 'preproc', pp, 'prior', 'vb', 'useARD', true);
alphaVB = modelVB.expectAlpha;
ndxKeepVB = find(alphaVB < 0.5*max(alphaVB))
%%
% As is commonly the case, ARD picks a sparser model than L1.
% In this example, it only keeps features 1,2,5,
% although this obviously depends on the threshold used.

%%
% <html>
% <hr>
% </html>
%%

%%
% This page was auto-generated by calling _pmtkPublish(C:\kmurphy\GoogleCode\pmtk3\docs\tutorial\tutRegr.m)_  on 08-Sep-2010 17:21:14


##### SOURCE END #####
--></body></html>