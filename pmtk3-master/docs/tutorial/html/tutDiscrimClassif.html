
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      --><title>Classification using parametric discriminative models in pmtk3</title><meta name="generator" content="MATLAB 7.9"><meta name="date" content="2010-09-08"><meta name="m-file" content="tutDiscrimClassif"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h1>Classification using parametric discriminative models in pmtk3</h1><!--introduction--><p>We extend the parametric discriminative regression models discussed <a href="http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutRegr.html">here</a> to the classification setting.</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Logistic regression: plugin method</a></li><li><a href="#4">Logistic regression: Bayesian method</a></li><li><a href="#9">Visualizing the decision boundaries</a></li><li><a href="#12">Controlling the optimizer</a></li><li><a href="#15">Sparse logistic regression</a></li><li><a href="#16">Multi-layer perceptrons (feedforward neural networks)</a></li><li><a href="#19">Bayesian methods for neural networks</a></li><li><a href="#20">Convolutional neural nets</a></li></ul></div><h2>Logistic regression: plugin method<a name="1"></a></h2><p>Consider fitting a binary logistic regression model to some SAT scores, where the response is whether the student passed or failed the class. First we compute the MLE and use a plugin approximation for prediction, as is standard practice (from <a href="http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Introduction/logregSATdemo.m">logregSATdemo.m</a> )</p><pre class="codeinput">close <span class="string">all</span>; clear <span class="string">all</span>
stat = loadData(<span class="string">'sat'</span>);  y = stat(:,1); X = stat(:,4);
model = logregFit(X, y);
[yhat, prob] = logregPredict(model, X); <span class="comment">%ok</span>
figure;
plot(X, y, <span class="string">'ko'</span>, <span class="string">'linewidth'</span>, 2, <span class="string">'MarkerSize'</span>, 7, <span class="string">'markerfacecolor'</span>, <span class="string">'k'</span>);
hold <span class="string">on</span>
plot(X, prob, <span class="string">'ro'</span>, <span class="string">'linewidth'</span>, 2,<span class="string">'MarkerSize'</span>, 10)
</pre><img vspace="5" hspace="5" src="tutDiscrimClassif_01.png" alt=""> <p>We see that the probability of passing the class smoothly increases as the SAT score goes up, as is to be expected.</p><h2>Logistic regression: Bayesian method<a name="4"></a></h2><p>Now let us fit the model using Bayesian inference with an noninformative prior, which we approximate by <img src="tutDiscrimClassif_eq04175.png" alt="$N(w|0,\infty I)$">, which corresponds to inference with an L2 regularizer of <img src="tutDiscrimClassif_eq58407.png" alt="$\lambda=0$">. By default, the fitting procedure uses a Laplace approximation to the posterior, from which we can extract credible intervals on the parameters, etc.</p><p>To approximate the predictive density, we can plug in the posterior mean:</p><p><img src="tutDiscrimClassif_eq03370.png" alt="$$p(y=1|x,D) = \int \sigma(w^T * x) N(w|\mu,\Sigma) dw&#xA;\approx \sigma(w^T \mu)$$"></p><p>However, this gives essentially the same result as plugging in the MLE. To get a measure of confidence in this prediction, we can sample values of the parameters from their posterior (which we have approximated by a Gaussian), use each such sample to make a prediction, and then compute empirical quantiles of this distribution to get a 95% credible interval. This is done using <a href="http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/logisticRegression/logregPredictBayes.m">logregPredictBayes.m</a> and gives the results shown below (from <a href="http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Introduction/logregSATdemoBayes.m">logregSATdemoBayes.m</a> )</p><pre class="codeinput">model = logregFitBayes(X, y);
[yhat, prob, pCI] = logregPredictBayes(model, X);
figure;
plot(X, y, <span class="string">'ko'</span>, <span class="string">'linewidth'</span>, 2, <span class="string">'MarkerSize'</span>, 7, <span class="string">'markerfacecolor'</span>, <span class="string">'k'</span>);
hold <span class="string">on</span>
plot(X, prob, <span class="string">'ro'</span>, <span class="string">'linewidth'</span>, 2,<span class="string">'MarkerSize'</span>, 10)
<span class="keyword">for</span> i=1:size(X,1)
  line([X(i,1) X(i,1)], [pCI(i,1) pCI(i,2)]);
<span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="tutDiscrimClassif_02.png" alt=""> <h2>Visualizing the decision boundaries<a name="9"></a></h2><p>When comparing classification methods, it is useful to apply them to 2d datasets and to plot the regions of space that get mapped to each class; these are called decision regions, and the boundaries between them are called decision boundaries. We can do this using the <a href="http://matlabtools.googlecode.com/svn/trunk/graphics/plotDecisionBoundary.m">plotDecisionBoundary.m</a> function, which takes a prediction function as an argument. As an example of this, consider the famous XOR dataset. Let us try fitting a logistic regression model to it in the original feature space (from <a href="http://pmtk3.googlecode.com/svn/trunk/demos/otherDemos/supervisedModels/logregXorLinearDemo.m">logregXorLinearDemo.m</a> )</p><pre class="codeinput">close <span class="string">all</span>; clear <span class="string">all</span>;
[X, y] = createXORdata();
model = logregFit(X, y);
plotDecisionBoundary(X, y, @(X)logregPredict(model, X));
yhat = logregPredict(model, X);
errorRate = mean(yhat ~= y)
</pre><pre class="codeoutput">errorRate =
    0.4875
</pre><img vspace="5" hspace="5" src="tutDiscrimClassif_03.png" alt=""> <p>We see that the method performs at chance level, because the data is not linearly separable. A simple fix to this problem is to use basis function expansion, as we discuss <a href="http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutKernelClassif.html">here</a>.</p><h2>Controlling the optimizer<a name="12"></a></h2><p>Computing the ML or MAP parameter estimate, as well as a Laplace approximation to the posterior, involves solving an optimization problem. When we use a Gaussian prior, the resulting objective is convex and smooth. We use Mark Schmidt's excellent <a href="http://www.cs.ubc.ca/~schmidtm/Software/minFunc.html">minFunc</a> package for such problems. (A version is included in <a href="http://code.google.com/p/pmtksupport/">pmtkSupport</a>.)</p><p><a href="http://pmtksupport.googlecode.com/svn/trunk/markSchmidt-21june2010/minFunc/minFunc.m">minFunc.m</a> implements many different algorithms and supports a variety of optional arguments which control the convergence threshold, max number of iterations, etc. In pmtk, you can pass in optional arguments through to minfunc as follows: <tt>[model] = logregFit(X, y,  'fitOptions', options)</tt></p><p>Below we compare different optimizers for finding the MAP estimate for a binary logistic regression model under a weak Gaussian prior, applied to two of the MNIST digit classes. (The code extends easily to the multi-class case, but this is a bit slower). Specifically, we compare the following</p><div><ul><li>sd: steepest descent</li><li>cg: conjugate gradient</li><li>bb: barzilai borwein</li><li>lbfgs: limited memory BFGS</li></ul></div><p>sd, cg and bb are first order methods; lbfgs is second order.</p><p>To make the optimization problem a bit harder, we don't preprocess the data in any way. (By default, <a href="http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/logisticRegression/logregFit.m">logregFit.m</a> standardizes its inputs, as well as adding a column of 1s; standardization helps convergence a lot, as well as being advisable for statistical reasons.) (Based on <a href="http://pmtk3.googlecode.com/svn/trunk/demos/otherDemos/supervisedModels/logregOptDemo.m">logregOptDemo.m</a> )</p><pre class="codeinput">clear <span class="string">all</span>
setSeed(0);
Ntrain = [];
[Xtrain, ytrain, Xtest, ytest] = mnistLoad([2 3]);
lambda = 1e-3;

<span class="comment">% minfunc options</span>
options = [];
options.derivativeCheck = <span class="string">'off'</span>;
options.display = <span class="string">'none'</span>;
<span class="comment">%options.display = 'iter';</span>
options.maxIter = 50;
options.maxFunEvals = 50;
options.TolFun = 1e-3; <span class="comment">% default 1e-5</span>
options.TolX = 1e-3; <span class="comment">% default 1e-5</span>

<span class="comment">% algorithms</span>
methods = {<span class="string">'sd'</span>, <span class="string">'cg'</span>, <span class="string">'bb'</span>, <span class="string">'lbfgs'</span>};

<span class="keyword">for</span> m=1:length(methods)
  method = methods{m}
  options.Method = method;
  tic
  [model, X, lambdaVec, opt] = logregFit(Xtrain, ytrain, <span class="string">'regtype'</span>, <span class="string">'l2'</span>, <span class="keyword">...</span>
    <span class="string">'lambda'</span>, lambda, <span class="string">'fitOptions'</span>, options, <span class="string">'preproc'</span>, []);
  t = toc;
  fvalTrace = opt.output.trace.fval;
  finalObj = opt.finalObj;
  figure;
  plot(fvalTrace, <span class="string">'o-'</span>, <span class="string">'linewidth'</span>, 2);
  set(gca, <span class="string">'xlim'</span>, [10 50], <span class="string">'ylim'</span>, [600 1500]); <span class="comment">% make scales comparable</span>
  title(sprintf(<span class="string">'%s, %5.3f seconds, final obj = %5.3f'</span>, <span class="keyword">...</span>
    method, t, finalObj));
<span class="keyword">end</span>
</pre><pre class="codeoutput">method =
sd
method =
cg
method =
bb
method =
lbfgs
</pre><img vspace="5" hspace="5" src="tutDiscrimClassif_04.png" alt=""> <img vspace="5" hspace="5" src="tutDiscrimClassif_05.png" alt=""> <img vspace="5" hspace="5" src="tutDiscrimClassif_06.png" alt=""> <img vspace="5" hspace="5" src="tutDiscrimClassif_07.png" alt=""> <p>Since the objective is convex, all methods find the same solution  if given enough time. If you want a very precise estimate, LBFGS is the method of choice, but if a somewhat sloppier estimate is sufficient, fast first-order methods such as BB are the way to go. See also</p><div><ul><li><a href="http://research.microsoft.com/en-us/um/people/minka/papers/logreg/">A comparison of numerical optimizers for logistic regression</a>, Tom Minka, tech report, 2003.</li><li><a href="http://pmtk3.googlecode.com/svn/trunk/docs/demoOutput/otherDemos/algorithms/demoMinfuncHighdim.html">output of demoMinfuncHighdim</a></li></ul></div><h2>Sparse logistic regression<a name="15"></a></h2><p>To create sparse classifiers, we can perform MAP estimation using L1 regularization. Analogously to the linear regression case, we can use the following methods:</p><div><ul><li>L1: <tt>logregFit(X, y, 'regtype, 'L1', ...)</tt> (uses <a href="http://www.cs.ubc.ca/~schmidtm/Software/L1General/L1General.html">L1general</a>)</li><li>L1path:  <tt>logregFitPathCv(X, y, ...)</tt> (uses <a href="http://www-stat.stanford.edu/~tibs/glmnet-matlab/">glmnet</a>)</li><li>ARD: <tt>logregFitBayes(X, y, 'prior', 'vb', 'useARD', true)</tt> (uses <a href="http://www.bcs.rochester.edu/people/jdrugowitsch/code.html">VB code</a>).</li></ul></div><h2>Multi-layer perceptrons (feedforward neural networks)<a name="16"></a></h2><p>MLPs are a form of nonlinear model for regression/ classification. The two key operations are:</p><div><ul><li>forwards propagation: compute <img src="tutDiscrimClassif_eq18672.png" alt="$f(x,\theta)$"> by passing <img src="tutDiscrimClassif_eq43551.png" alt="$x$"> through each layer</li><li>back propagation: compute the derivative of the loss function by passing the error signal back through each layer</li></ul></div><p>pmtk uses two different implementations of these functions. One is based on Mark Schmidt's code, available <a href="http://people.cs.ubc.ca/~schmidtm/Software/minFunc/minFunc.html#9">here</a>. This code currently only supports regression and binary classification, but could easily be extended. The other is based on Ian Nabney's <a href="http://www1.aston.ac.uk/eas/research/groups/ncrg/resources/netlab/">Netlab</a>. This supports regression, binary and multi-class classification, but only supports one hidden layer. (Combining the functionality of these two code bases is left as an exercise to the reader.)</p><p>Given a forwards and backwards propagation method, it is easy to implement the <a href="http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/mlp/mlpFit.m">mlpFit.m</a> and <a href="http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/mlp/mlpPredict.m">mlpPredict.m</a> methods. For fitting, we use <a href="http://pmtksupport.googlecode.com/svn/trunk/markSchmidt-21june2010/minFunc/minFunc.m">minfunc.m</a> . Note that the objective function for MLPs is not convex, so we may get stuck in local optima.</p><p>Below we give a simple example of an MLP for solving a nonlinear binary classification problem. (Based on <a href="http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Neural_networks/mlpClassifDemo.m">mlpClassifDemo.m</a> )</p><pre class="codeinput">H = [3, 6, 9];
<span class="keyword">for</span> hi=1:length(H)
  nhidden = H(hi);
setSeed(0);
nVars = 2;
nInstances = 400;
options.Display = <span class="string">'none'</span>;
options.MaxIter = 100;
[X,y] = makeData(<span class="string">'classificationNonlinear'</span>,nInstances,nVars);
[N,D] = size(X);
X1 = [ones(N,1) X];
lambda = 1e-2;
model = mlpFit(X, y, <span class="string">'nhidden'</span>, nhidden, <span class="string">'lambda'</span>, lambda, <span class="keyword">...</span>
  <span class="string">'fitOptions'</span>, options, <span class="string">'method'</span>, <span class="string">'schmidt'</span>);
[yhat, py] = mlpPredict(model, X);
nerr = sum(yhat ~= y);
str = sprintf(<span class="string">'mlp with %d hidden units, nerr = %d'</span>, model.nHidden, nerr);
plotDecisionBoundary(X, y, @(X)mlpPredict(model, X));
title(str);
<span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="tutDiscrimClassif_08.png" alt=""> <img vspace="5" hspace="5" src="tutDiscrimClassif_09.png" alt=""> <img vspace="5" hspace="5" src="tutDiscrimClassif_10.png" alt=""> <p>We see that the training set error rate decreases monotonically with model complexity (number of hidden units). Eventually the model will overfit. Obviously we can use <a href="http://matlabtools.googlecode.com/svn/trunk/stats/fitCv.m">fitCv.m</a> to choose <img src="tutDiscrimClassif_eq67619.png" alt="$H$"> and <img src="tutDiscrimClassif_eq23351.png" alt="$\lambda$">. But when each layer is allowed its own regularizer, using CV to tune them all becomes too slow. This is one motivation for Bayesian methods.</p><h2>Bayesian methods for neural networks<a name="19"></a></h2><p>Bayesian methods for MLPs are not supported by pmtk. If you are interested in this, check out the following packages</p><div><ul><li>Ian Nabney's <a href="http://www1.aston.ac.uk/eas/research/groups/ncrg/resources/netlab/">Netlab</a> package, which uses Laplace approximation (Matlab code).</li><li>Aki Vehtari's <a href="http://www.lce.hut.fi/research/mm/mcmcstuff">mcmcstuff</a> package, which uses MCMC (C and Matlab).</li><li>Radford Neal's <a href="http://www.cs.toronto.edu/~radford/fbm.software.html">FBM</a> package, which uses MCMC (C code).</li></ul></div><h2>Convolutional neural nets<a name="20"></a></h2><p>Convolutional neural nets are not supported by pmtk. If you are interested in this, check out the following packages</p><div><ul><li>Mihail Sirotenko's <a href="http://www.mathworks.com/matlabcentral/fileexchange/24291-cnn-convolutional-neural-network-class">CNN</a> package. Has GPU support. Matlab code.</li><li>Nikolay Chumerin's <a href="http://sites.google.com/site/chumerin/projects/mycnn">myCNN</a> package. Matlab code.</li><li>Jim Mutch's <a href="http://cbcl.mit.edu/jmutch/cns/">CNS</a> package, for 'cortical network simulator'. Has GPU support. Matlab/C++ code.</li><li><a href="http://torch5.sourceforge.net/index.html">Torch5</a>, a machine learning library with CNN support. Written in a scripting language called lua.</li></ul></div><p>
<hr>
</p><p>This page was auto-generated by calling <i>pmtkPublish(C:\kmurphy\GoogleCode\pmtk3\docs\tutorial\tutDiscrimClassif.m)</i>  on 08-Sep-2010 17:19:29</p><p class="footer"><br>
      Published with MATLAB&reg; 7.9<br></p></div><!--
##### SOURCE BEGIN #####
%% Classification using parametric discriminative models in pmtk3
%
% We extend the parametric discriminative regression models discussed
% <http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutRegr.html
% here> to the classification setting.

%% Logistic regression: plugin method
% Consider fitting a binary logistic regression
% model to some SAT scores, where the response is whether the student
% passed or failed the class. First we compute the MLE and use a plugin
% approximation for prediction, as is standard practice (from <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Introduction/logregSATdemo.m logregSATdemo.m> )
%
%%
close all; clear all
stat = loadData('sat');  y = stat(:,1); X = stat(:,4);
model = logregFit(X, y);
[yhat, prob] = logregPredict(model, X); %ok
figure;
plot(X, y, 'ko', 'linewidth', 2, 'MarkerSize', 7, 'markerfacecolor', 'k');
hold on
plot(X, prob, 'ro', 'linewidth', 2,'MarkerSize', 10)
%%
% We see that the probability of passing the class smoothly increases as
% the SAT score goes up, as is to be expected.

%% Logistic regression: Bayesian method
% Now let us fit the model using Bayesian inference with an noninformative
% prior, which we approximate by $N(w|0,\infty I)$,
% which corresponds to inference with an L2 regularizer of $\lambda=0$.
% By default, the fitting procedure uses a Laplace
% approximation to the posterior, from which
% we can extract credible intervals on the parameters, etc.
% 
% To approximate the predictive density, we
% can plug in the posterior mean:
%%
% $$p(y=1|x,D) = \int \sigma(w^T * x) N(w|\mu,\Sigma) dw 
% \approx \sigma(w^T \mu)$$
%%
% However, this gives essentially the same result as plugging in the MLE.
% To get a measure of confidence in this prediction, we can sample values
% of the parameters from their posterior (which we have approximated by a Gaussian), use each such sample to make a
% prediction, and then compute empirical quantiles of this distribution to
% get a 95% credible interval.
% This is done using <http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/logisticRegression/logregPredictBayes.m logregPredictBayes.m> and gives the results shown below
% (from <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Introduction/logregSATdemoBayes.m logregSATdemoBayes.m> )

%%
model = logregFitBayes(X, y);
[yhat, prob, pCI] = logregPredictBayes(model, X);
figure;
plot(X, y, 'ko', 'linewidth', 2, 'MarkerSize', 7, 'markerfacecolor', 'k');
hold on
plot(X, prob, 'ro', 'linewidth', 2,'MarkerSize', 10)
for i=1:size(X,1)
  line([X(i,1) X(i,1)], [pCI(i,1) pCI(i,2)]);
end
%%


%% Visualizing the decision boundaries
% When comparing classification methods, it is useful to apply them to 2d
% datasets and to plot the regions of space that get mapped to each class;
% these are called decision regions, and the boundaries between them are called decision
% boundaries. We can do this using the <http://matlabtools.googlecode.com/svn/trunk/graphics/plotDecisionBoundary.m plotDecisionBoundary.m>
% function, which takes a prediction function as an argument.
% As an example of this, consider the famous XOR dataset.
% Let us try fitting a logistic regression model to it in the original
% feature space (from <http://pmtk3.googlecode.com/svn/trunk/demos/otherDemos/supervisedModels/logregXorLinearDemo.m logregXorLinearDemo.m> )
%%
close all; clear all;
[X, y] = createXORdata();
model = logregFit(X, y);
plotDecisionBoundary(X, y, @(X)logregPredict(model, X));
yhat = logregPredict(model, X);
errorRate = mean(yhat ~= y)
%%
% We see that the method performs at chance level, because the data is not
% linearly separable. A simple fix to this problem is to use
% basis function expansion, as we discuss
% <http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutKernelClassif.html
% here>.


%% Controlling the optimizer
% Computing the ML or MAP parameter estimate, as well as a Laplace
% approximation to the posterior, involves solving an optimization
% problem. When we use a Gaussian prior, the resulting
% objective is convex and smooth. We use 
% Mark Schmidt's excellent
% <http://www.cs.ubc.ca/~schmidtm/Software/minFunc.html minFunc> 
% package for such problems. (A version is included in
% <http://code.google.com/p/pmtksupport/ pmtkSupport>.)
%
% <http://pmtksupport.googlecode.com/svn/trunk/markSchmidt-21june2010/minFunc/minFunc.m minFunc.m> implements many different algorithms and supports
% a variety of optional arguments which control the convergence
% threshold, max number of iterations, etc.
% In pmtk, you can pass in optional arguments through to 
% minfunc as follows:
% |[model] = logregFit(X, y,  'fitOptions', options)|
%
% Below we compare different optimizers for finding the MAP
% estimate for a binary logistic
% regression model under a weak Gaussian prior, applied to two of the MNIST digit classes.
% (The code extends easily to the multi-class case,
% but this is a bit slower).
% Specifically, we compare the following
%
% * sd: steepest descent
% * cg: conjugate gradient
% * bb: barzilai borwein
% * lbfgs: limited memory BFGS 
%
% sd, cg and bb are first order methods; lbfgs is second order.
%
% To make the optimization problem a bit harder, we don't preprocess
% the data in any way. (By default, <http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/logisticRegression/logregFit.m logregFit.m> standardizes its inputs,
% as well as adding a column of 1s; standardization helps convergence
% a lot, as well as being advisable for statistical reasons.)
% (Based on <http://pmtk3.googlecode.com/svn/trunk/demos/otherDemos/supervisedModels/logregOptDemo.m logregOptDemo.m> )
%% 
clear all
setSeed(0);
Ntrain = [];
[Xtrain, ytrain, Xtest, ytest] = mnistLoad([2 3]);
lambda = 1e-3; 

% minfunc options
options = [];
options.derivativeCheck = 'off';
options.display = 'none';
%options.display = 'iter';
options.maxIter = 50;
options.maxFunEvals = 50;
options.TolFun = 1e-3; % default 1e-5
options.TolX = 1e-3; % default 1e-5

% algorithms
methods = {'sd', 'cg', 'bb', 'lbfgs'};

for m=1:length(methods)
  method = methods{m}
  options.Method = method;
  tic
  [model, X, lambdaVec, opt] = logregFit(Xtrain, ytrain, 'regtype', 'l2', ...
    'lambda', lambda, 'fitOptions', options, 'preproc', []);
  t = toc;
  fvalTrace = opt.output.trace.fval;
  finalObj = opt.finalObj;
  figure;
  plot(fvalTrace, 'o-', 'linewidth', 2);
  set(gca, 'xlim', [10 50], 'ylim', [600 1500]); % make scales comparable
  title(sprintf('%s, %5.3f seconds, final obj = %5.3f', ...
    method, t, finalObj));
end
%%
% Since the objective is convex, all methods find the same
% solution  if given enough time.
% If you want a very precise estimate, 
% LBFGS is the method of choice, but if a somewhat sloppier estimate
% is sufficient, fast first-order methods such as BB
% are the way to go.
% See also
%
% * <http://research.microsoft.com/en-us/um/people/minka/papers/logreg/
% A comparison of numerical optimizers for logistic regression>,
% Tom Minka, tech report, 2003.
% * <http://pmtk3.googlecode.com/svn/trunk/docs/demoOutput/otherDemos/algorithms/demoMinfuncHighdim.html
% output of demoMinfuncHighdim>


%% Sparse logistic regression
% To create sparse classifiers,
% we can perform MAP estimation using L1 regularization.
% Analogously to the linear regression case,
% we can use the following methods:
%
% * L1: |logregFit(X, y, 'regtype, 'L1', ...)|  
% (uses <http://www.cs.ubc.ca/~schmidtm/Software/L1General/L1General.html
% L1general>)
% * L1path:  |logregFitPathCv(X, y, ...)|
% (uses <http://www-stat.stanford.edu/~tibs/glmnet-matlab/ glmnet>)
% * ARD: |logregFitBayes(X, y, 'prior', 'vb', 'useARD', true)|
% (uses <http://www.bcs.rochester.edu/people/jdrugowitsch/code.html VB
% code>).


%% Multi-layer perceptrons (feedforward neural networks)
% MLPs are a form of nonlinear model for regression/ classification.
% The two key operations are:
%
% * forwards propagation: compute $f(x,\theta)$ by passing
% $x$ through each layer 
% * back propagation: compute the derivative of the loss function
% by passing the error signal back through each layer
%
% pmtk uses two different implementations of these functions.
% One is based on Mark Schmidt's code,
% available <http://people.cs.ubc.ca/~schmidtm/Software/minFunc/minFunc.html#9
% here>.
% This code currently only supports regression
% and binary classification, but could easily be extended.
% The other is based on Ian Nabney's
% <http://www1.aston.ac.uk/eas/research/groups/ncrg/resources/netlab/
% Netlab>.
% This supports regression, binary and multi-class classification,
% but only supports one hidden layer.
% (Combining the functionality of these two code bases is
% left as an exercise to the reader.)
%
% Given a forwards and backwards propagation method,
% it is easy to implement the <http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/mlp/mlpFit.m mlpFit.m> and <http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/mlp/mlpPredict.m mlpPredict.m> methods.
% For fitting, we use <http://pmtksupport.googlecode.com/svn/trunk/markSchmidt-21june2010/minFunc/minFunc.m minfunc.m> .
% Note that the objective function for MLPs is not convex,
% so we may get stuck in local optima.
%
% Below we give a simple example of an MLP 
% for solving a nonlinear binary classification problem.
% (Based on <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Neural_networks/mlpClassifDemo.m mlpClassifDemo.m> )
%%
H = [3, 6, 9];
for hi=1:length(H)
  nhidden = H(hi);
setSeed(0);
nVars = 2;
nInstances = 400;
options.Display = 'none';
options.MaxIter = 100;
[X,y] = makeData('classificationNonlinear',nInstances,nVars);
[N,D] = size(X);
X1 = [ones(N,1) X];
lambda = 1e-2;
model = mlpFit(X, y, 'nhidden', nhidden, 'lambda', lambda, ...
  'fitOptions', options, 'method', 'schmidt');
[yhat, py] = mlpPredict(model, X);
nerr = sum(yhat ~= y);
str = sprintf('mlp with %d hidden units, nerr = %d', model.nHidden, nerr);
plotDecisionBoundary(X, y, @(X)mlpPredict(model, X));
title(str);
end
%%
% We see that the training set error rate decreases monotonically
% with model complexity (number of hidden units).
% Eventually the model will overfit.
% Obviously we can use <http://matlabtools.googlecode.com/svn/trunk/stats/fitCv.m fitCv.m> to choose 
% $H$ and $\lambda$.
% But when each layer is allowed its own regularizer,
% using CV to tune them all becomes too slow.
% This is one motivation for Bayesian methods.

%% Bayesian methods for neural networks
% Bayesian methods for MLPs are not supported by pmtk.
% If you are interested in this, check out the following packages
%
% * Ian Nabney's
% <http://www1.aston.ac.uk/eas/research/groups/ncrg/resources/netlab/
% Netlab> package, which uses Laplace approximation (Matlab code).
% * Aki Vehtari's
% <http://www.lce.hut.fi/research/mm/mcmcstuff mcmcstuff>
% package, which uses MCMC (C and Matlab).
% * Radford Neal's
% <http://www.cs.toronto.edu/~radford/fbm.software.html FBM> package,
% which uses MCMC (C code).

%% Convolutional neural nets
% Convolutional neural nets are not supported by pmtk.
% If you are interested in this, check out the following packages
%
% * Mihail Sirotenko's
% <http://www.mathworks.com/matlabcentral/fileexchange/24291-cnn-convolutional-neural-network-class
% CNN> package. 
% Has GPU support. Matlab code.
% * Nikolay Chumerin's
% <http://sites.google.com/site/chumerin/projects/mycnn
% myCNN> package. Matlab code.
% * Jim Mutch's
% <http://cbcl.mit.edu/jmutch/cns/ CNS> package,
% for 'cortical network simulator'. Has GPU support.
% Matlab/C++ code.
% * <http://torch5.sourceforge.net/index.html Torch5>,
% a machine learning library with CNN support.
% Written in a scripting
% language called lua.
%


 
%%
% <html>
% <hr>
% </html>
%%

%%
% This page was auto-generated by calling _pmtkPublish(C:\kmurphy\GoogleCode\pmtk3\docs\tutorial\tutDiscrimClassif.m)_  on 08-Sep-2010 17:19:29


##### SOURCE END #####
--></body></html>