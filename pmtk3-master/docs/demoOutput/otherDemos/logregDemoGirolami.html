
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>2D Logistic Regression Demo</title><meta name="generator" content="MATLAB 7.12"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2012-03-27"><meta name="DC.source" content="logregDemoGirolami.m"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h1>2D Logistic Regression Demo</h1><!--introduction--><p>Based on code by Mark Girolami</p><!--/introduction--><pre class="codeinput"><span class="comment">% This file is from pmtk3.googlecode.com</span>

setSeed(0);
D = loadData(<span class="string">'rip'</span>);
X = D.rip_dat_tr;
Xt = D.rip_dat_te;
Ntrain = size(X,1); Ntest = size(Xt,1);
ytrain = X(:,3);
X(:,3)=[];
ytest = Xt(:,3);
Xt(:,3)=[];
Xtrain = X; Xtest = Xt; clear <span class="string">X</span> <span class="string">Xt</span>


polyOrders = 1:10 ;
<span class="keyword">for</span> trial=1:length(polyOrders)
  Polynomial_Order = polyOrders(trial);
  lambda = 1/100;

  <span class="comment">%Limits and grid size for contour plotting</span>
  R=1.3;
  Step=0.1;
  [xs,ys]=meshgrid(-R:Step:R,-R:Step:R);
  [ngrid, ngrid]=size(xs);
  grid = [reshape(xs,ngrid*ngrid,1) reshape(ys,ngrid*ngrid,1)];

  <span class="comment">% Polynomial Basis expansion</span>
  XtrainPoly = ones(Ntrain,1);
  XtestPoly = ones(Ntest,1);
  gridPoly = ones(ngrid*ngrid,1);
  <span class="keyword">for</span> i = 1:Polynomial_Order
    XtrainPoly = [XtrainPoly Xtrain.^i];
    XtestPoly = [XtestPoly Xtest.^i];
    gridPoly = [gridPoly grid.^i];
  <span class="keyword">end</span>
  [N,D] = size(XtrainPoly);

  pp = preprocessorCreate(<span class="string">'standardizeX'</span>, false, <span class="string">'addOnes'</span>, false);
  model = logregFit(XtrainPoly,ytrain,<span class="string">'lambda'</span>, lambda, <span class="string">'preproc'</span>, pp);
  [yhat, Posterior] = logregPredict(model, gridPoly);

  [trainPredLabels] = logregPredict(model, XtrainPoly);
  [testPredLabels] = logregPredict(model, XtestPoly);
  <span class="comment">%fprintf('\n\n 0-1 error using MAP Value\n');</span>
  Train_Error(trial) = 100 - 100*sum(trainPredLabels == ytrain)/Ntrain;
  Test_Error(trial) = 100 - 100*sum(testPredLabels == ytest)/Ntest;

  [modelVB, logevVB(trial)] = logregFitBayes(XtrainPoly,ytrain,<span class="string">'method'</span>,<span class="string">'vb'</span>, <span class="string">'preproc'</span>, pp);
  <span class="comment">%[yhatVB, PosteriorVB] = logregPredictBayes(modelVB, gridPoly);</span>
  <span class="comment">%Posterior = PosteriorVB; % The VB predictions are much smoother</span>

  <span class="keyword">if</span> 0
  <span class="comment">% plot the data points and show the contour of P(C=1|x)</span>
  figure;
  contour(xs,ys,reshape(Posterior,[ngrid,ngrid]));
  hold <span class="string">on</span>
  plot(Xtrain(find(ytrain==1),1), Xtrain(find(ytrain==1),2),<span class="string">'r.'</span>);
  plot(Xtrain(find(ytrain==0),1), Xtrain(find(ytrain==0),2),<span class="string">'o'</span>);
  title(sprintf(<span class="string">'P(C=1|x) for degree %d'</span>, Polynomial_Order))
  <span class="keyword">end</span>

  <span class="keyword">if</span> 1
    figure;
    <span class="comment">% plot P(C=1|x)=0.5 i.e. the separating line</span>
    [cc,hh]=contour(xs,ys,reshape(Posterior,[ngrid,ngrid]),[0.5 0.5]);
    set(hh,<span class="string">'linewidth'</span>,4,<span class="string">'color'</span>,<span class="string">'k'</span>);
    hold <span class="string">on</span>
    plot(Xtrain(find(ytrain==1),1), Xtrain(find(ytrain==1),2),<span class="string">'r.'</span>);
    plot(Xtrain(find(ytrain==0),1), Xtrain(find(ytrain==0),2),<span class="string">'o'</span>);
    title(sprintf(<span class="string">'Degree %d, error rates: train = %3.2f, test = %3.2f'</span>, <span class="keyword">...</span>
      Polynomial_Order, Train_Error(trial), Test_Error(trial)))
    d = Polynomial_Order;
    printPmtkFigure(sprintf(<span class="string">'logregDemoGirolamiDeg%d'</span>, d));
  <span class="keyword">end</span>

<span class="keyword">end</span> <span class="comment">% trial</span>

figure; hold <span class="string">on</span>
plot(polyOrders, Train_Error, <span class="string">'r-o'</span>, <span class="string">'linewidth'</span>, 3, <span class="string">'markersize'</span>, 10);
plot(polyOrders, Test_Error, <span class="string">'b:s'</span>, <span class="string">'linewidth'</span>, 3, <span class="string">'markersize'</span>, 10);
legend(<span class="string">'train'</span>, <span class="string">'test'</span>);
ylabel(<span class="string">'misclassification rates'</span>);
xlabel(<span class="string">'degree'</span>)
printPmtkFigure(<span class="string">'logregDemoGirolamiErr'</span>);

post = exp(normalizeLogspace(logevVB));
figure; bar(post);
set(gca,<span class="string">'xticklabel'</span>,polyOrders);
title(<span class="string">'p(m|D)'</span>)
printPmtkFigure(<span class="string">'logregDemoGirolamiPost'</span>);
</pre><img vspace="5" hspace="5" src="logregDemoGirolami_01.png" alt=""> <img vspace="5" hspace="5" src="logregDemoGirolami_02.png" alt=""> <img vspace="5" hspace="5" src="logregDemoGirolami_03.png" alt=""> <img vspace="5" hspace="5" src="logregDemoGirolami_04.png" alt=""> <img vspace="5" hspace="5" src="logregDemoGirolami_05.png" alt=""> <img vspace="5" hspace="5" src="logregDemoGirolami_06.png" alt=""> <img vspace="5" hspace="5" src="logregDemoGirolami_07.png" alt=""> <img vspace="5" hspace="5" src="logregDemoGirolami_08.png" alt=""> <img vspace="5" hspace="5" src="logregDemoGirolami_09.png" alt=""> <img vspace="5" hspace="5" src="logregDemoGirolami_10.png" alt=""> <img vspace="5" hspace="5" src="logregDemoGirolami_11.png" alt=""> <img vspace="5" hspace="5" src="logregDemoGirolami_12.png" alt=""> <p class="footer"><br>
      Published with MATLAB&reg; 7.12<br></p></div><!--
##### SOURCE BEGIN #####
%% 2D Logistic Regression Demo
% Based on code by Mark Girolami
%%

% This file is from pmtk3.googlecode.com

setSeed(0);
D = loadData('rip');
X = D.rip_dat_tr;
Xt = D.rip_dat_te;
Ntrain = size(X,1); Ntest = size(Xt,1);
ytrain = X(:,3);
X(:,3)=[];
ytest = Xt(:,3);
Xt(:,3)=[];
Xtrain = X; Xtest = Xt; clear X Xt


polyOrders = 1:10 ; 
for trial=1:length(polyOrders)
  Polynomial_Order = polyOrders(trial);
  lambda = 1/100;
  
  %Limits and grid size for contour plotting
  R=1.3;
  Step=0.1;
  [xs,ys]=meshgrid(-R:Step:R,-R:Step:R);
  [ngrid, ngrid]=size(xs);
  grid = [reshape(xs,ngrid*ngrid,1) reshape(ys,ngrid*ngrid,1)];
  
  % Polynomial Basis expansion
  XtrainPoly = ones(Ntrain,1);
  XtestPoly = ones(Ntest,1);
  gridPoly = ones(ngrid*ngrid,1);
  for i = 1:Polynomial_Order
    XtrainPoly = [XtrainPoly Xtrain.^i];
    XtestPoly = [XtestPoly Xtest.^i];
    gridPoly = [gridPoly grid.^i];
  end
  [N,D] = size(XtrainPoly);
  
  pp = preprocessorCreate('standardizeX', false, 'addOnes', false);
  model = logregFit(XtrainPoly,ytrain,'lambda', lambda, 'preproc', pp);
  [yhat, Posterior] = logregPredict(model, gridPoly);
  
  [trainPredLabels] = logregPredict(model, XtrainPoly);
  [testPredLabels] = logregPredict(model, XtestPoly);
  %fprintf('\n\n 0-1 error using MAP Value\n');
  Train_Error(trial) = 100 - 100*sum(trainPredLabels == ytrain)/Ntrain;
  Test_Error(trial) = 100 - 100*sum(testPredLabels == ytest)/Ntest;
  
  [modelVB, logevVB(trial)] = logregFitBayes(XtrainPoly,ytrain,'method','vb', 'preproc', pp);
  %[yhatVB, PosteriorVB] = logregPredictBayes(modelVB, gridPoly);
  %Posterior = PosteriorVB; % The VB predictions are much smoother
   
  if 0
  % plot the data points and show the contour of P(C=1|x)
  figure;
  contour(xs,ys,reshape(Posterior,[ngrid,ngrid]));
  hold on
  plot(Xtrain(find(ytrain==1),1), Xtrain(find(ytrain==1),2),'r.');
  plot(Xtrain(find(ytrain==0),1), Xtrain(find(ytrain==0),2),'o');
  title(sprintf('P(C=1|x) for degree %d', Polynomial_Order))
  end
  
  if 1
    figure;
    % plot P(C=1|x)=0.5 i.e. the separating line
    [cc,hh]=contour(xs,ys,reshape(Posterior,[ngrid,ngrid]),[0.5 0.5]);
    set(hh,'linewidth',4,'color','k');
    hold on
    plot(Xtrain(find(ytrain==1),1), Xtrain(find(ytrain==1),2),'r.');
    plot(Xtrain(find(ytrain==0),1), Xtrain(find(ytrain==0),2),'o');
    title(sprintf('Degree %d, error rates: train = %3.2f, test = %3.2f', ...
      Polynomial_Order, Train_Error(trial), Test_Error(trial)))
    d = Polynomial_Order;
    printPmtkFigure(sprintf('logregDemoGirolamiDeg%d', d));
  end
  
end % trial

figure; hold on
plot(polyOrders, Train_Error, 'r-o', 'linewidth', 3, 'markersize', 10);
plot(polyOrders, Test_Error, 'b:s', 'linewidth', 3, 'markersize', 10);
legend('train', 'test');
ylabel('misclassification rates');
xlabel('degree')
printPmtkFigure('logregDemoGirolamiErr');

post = exp(normalizeLogspace(logevVB));
figure; bar(post); 
set(gca,'xticklabel',polyOrders);
title('p(m|D)')
printPmtkFigure('logregDemoGirolamiPost');



##### SOURCE END #####
--></body></html>