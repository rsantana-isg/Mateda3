
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Illustrate benefit of using T distribution instead of plugin estimate</title><meta name="generator" content="MATLAB 7.12"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2012-03-27"><meta name="DC.source" content="naiveBayesBayesianDemo.m"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h1>Illustrate benefit of using T distribution instead of plugin estimate</h1><!--introduction--><p>We use a naive Bayes classifier with 2 Gaussian features</p><!--/introduction--><pre class="codeinput"><span class="comment">% This file is from pmtk3.googlecode.com</span>

<span class="keyword">function</span> naiveBayesBayesianDemo()
setSeed(0);
C = 3;
[Xtrain, Ytrain] = makeDataHelper([2 5 8]*1);
[Xtest, Ytest] = makeDataHelper([2 5 8]*20);
<span class="comment">%[Xtrain, Ytrain] = makeDataHelper([2 2 2]*5);</span>
<span class="comment">%[Xtest, Ytest] = makeDataHelper([2 2 2]*20);</span>
[styles, colors, symbols] =  plotColors();

figure;
subplot(2, 2, 1)
<span class="keyword">for</span> c = 1:C
    ndx = Ytrain==c;
    plot(Xtrain(ndx, 1), Xtrain(ndx, 2), [colors(c) symbols(c)]);
    hold <span class="string">on</span>
<span class="keyword">end</span>
title(<span class="string">'training data'</span>)

subplot(2,2,2)
<span class="keyword">for</span> c = 1:C
    ndx = Ytest==c;
    plot(Xtest(ndx,1), Xtest(ndx, 2), [colors(c) symbols(c)]);
    hold <span class="string">on</span>
<span class="keyword">end</span>
title(<span class="string">'test data'</span>)

<span class="comment">% train</span>
<span class="keyword">for</span> c=1:C
    ndx = Ytrain==c;
    X = Xtrain(ndx, :);
    n = length(ndx);
    post.nu(c) = n;
    post.mu(c, :) = mean(X, 1);
    post.kappa(c) = n;
    post.pi(c) = n;
    post.s2(c, :) = var(X, 1, 1); <span class="comment">% divide by n, across rows</span>
    mle.mu(c, :) = mean(X, 1);
    mle.s2(c, :) = var(X, 1, 1);
    mle.pi(c) = n;
<span class="keyword">end</span>

post.pi = normalize(post.pi + 1);
mle.pi = normalize(mle.pi);

<span class="comment">% test</span>
ndims = 2;
logprobBayesC = zeros(size(Xtest, 1) ,C, ndims);
logprobPluginC = zeros(size(Xtest, 1) ,C, ndims);
<span class="keyword">for</span> c=1:C
    <span class="keyword">for</span> j=1:ndims
        sf = (1+post.kappa(c))/post.kappa(c);
        model.mu = post.mu(c, j);
        model.Sigma = sf*post.s2(c, j);
        model.dof = post.nu(c);
        logprobBayesC(:,c,j) = studentLogprob(model, Xtest(:,j));
        model.mu = mle.mu(c, j); model.Sigma = mle.s2(c, j);
        logprobPluginC(:,c,j) = gaussLogprob(model, Xtest(:, j));
    <span class="keyword">end</span>
<span class="keyword">end</span>
Ntest = length(Ytest);
logprobBayes = sum(logprobBayesC, 3); <span class="comment">% sum across features</span>
logprior = repmat(log(post.pi), Ntest, 1);
yhatBayes = maxidx(logprobBayes + logprior, [], 2); <span class="comment">% max across classes</span>

logprobPlugin = sum(logprobPluginC, 3); <span class="comment">% sum across features</span>
logprior = repmat(log(mle.pi), Ntest, 1);
yhatPlugin = maxidx(logprobPlugin + logprior, [], 2); <span class="comment">% max across classes</span>

errorsBayes = find(yhatBayes ~= Ytest);
errorsPlugin = find(yhatPlugin ~= Ytest);
errRateBayes = length(errorsBayes)/length(Ytest);
errRatePlugin = length(errorsPlugin)/length(Ytest);


subplot(2,2,3)
<span class="keyword">for</span> c = 1:C
    ndx = yhatBayes == c;
    plot(Xtest(ndx, 1), Xtest(ndx, 2), [colors(c) symbols(c)]);
    hold <span class="string">on</span>
<span class="keyword">end</span>
title(sprintf(<span class="string">'bayes, err rate %5.3f'</span>, errRateBayes))

subplot(2,2,4)
<span class="keyword">for</span> c = 1:C
    ndx = yhatPlugin == c;
    plot(Xtest(ndx, 1), Xtest(ndx, 2), [colors(c) symbols(c)]);
    hold <span class="string">on</span>
<span class="keyword">end</span>
title(sprintf(<span class="string">'plugin, err rate %5.3f'</span>, errRatePlugin))

<span class="keyword">end</span>


<span class="keyword">function</span> [X, Y] = makeDataHelper(nPts_n)
<span class="comment">% Generate three gaussian clusters in 2d</span>
mu = [0,   0.6;
    -0.3 -0.4;
    1   -0.5]';

var = [0.6, 0.4, 0.6];

N = sum(nPts_n);
dim = size(mu,1);
X = zeros(2, N);
Y = zeros(1, N);

curidx = 1;
<span class="keyword">for</span> j = 1:3
    X(:,curidx:curidx+nPts_n(j)-1) = repmat(mu(:,j), 1, nPts_n(j))+var(j).*randn(dim, nPts_n(j));
    Y(curidx:curidx+nPts_n(j)-1) = j;
    curidx = curidx + nPts_n(j);
<span class="keyword">end</span>
X = X'; <span class="comment">% N by 2</span>
Y = Y';
<span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="naiveBayesBayesianDemo_01.png" alt=""> <p class="footer"><br>
      Published with MATLAB&reg; 7.12<br></p></div><!--
##### SOURCE BEGIN #####
%% Illustrate benefit of using T distribution instead of plugin estimate
% We use a naive Bayes classifier with 2 Gaussian features
%%

% This file is from pmtk3.googlecode.com

function naiveBayesBayesianDemo()
setSeed(0);
C = 3;
[Xtrain, Ytrain] = makeDataHelper([2 5 8]*1);
[Xtest, Ytest] = makeDataHelper([2 5 8]*20);
%[Xtrain, Ytrain] = makeDataHelper([2 2 2]*5);
%[Xtest, Ytest] = makeDataHelper([2 2 2]*20);
[styles, colors, symbols] =  plotColors();

figure;
subplot(2, 2, 1)
for c = 1:C
    ndx = Ytrain==c;
    plot(Xtrain(ndx, 1), Xtrain(ndx, 2), [colors(c) symbols(c)]);
    hold on
end
title('training data')

subplot(2,2,2)
for c = 1:C
    ndx = Ytest==c;
    plot(Xtest(ndx,1), Xtest(ndx, 2), [colors(c) symbols(c)]);
    hold on
end
title('test data')

% train
for c=1:C
    ndx = Ytrain==c;
    X = Xtrain(ndx, :);
    n = length(ndx);
    post.nu(c) = n;
    post.mu(c, :) = mean(X, 1);
    post.kappa(c) = n;
    post.pi(c) = n;
    post.s2(c, :) = var(X, 1, 1); % divide by n, across rows
    mle.mu(c, :) = mean(X, 1);
    mle.s2(c, :) = var(X, 1, 1);
    mle.pi(c) = n;
end

post.pi = normalize(post.pi + 1);
mle.pi = normalize(mle.pi);

% test
ndims = 2;
logprobBayesC = zeros(size(Xtest, 1) ,C, ndims);
logprobPluginC = zeros(size(Xtest, 1) ,C, ndims);
for c=1:C
    for j=1:ndims
        sf = (1+post.kappa(c))/post.kappa(c);
        model.mu = post.mu(c, j);
        model.Sigma = sf*post.s2(c, j); 
        model.dof = post.nu(c);
        logprobBayesC(:,c,j) = studentLogprob(model, Xtest(:,j)); 
        model.mu = mle.mu(c, j); model.Sigma = mle.s2(c, j);
        logprobPluginC(:,c,j) = gaussLogprob(model, Xtest(:, j));
    end
end
Ntest = length(Ytest);
logprobBayes = sum(logprobBayesC, 3); % sum across features
logprior = repmat(log(post.pi), Ntest, 1);
yhatBayes = maxidx(logprobBayes + logprior, [], 2); % max across classes

logprobPlugin = sum(logprobPluginC, 3); % sum across features
logprior = repmat(log(mle.pi), Ntest, 1);
yhatPlugin = maxidx(logprobPlugin + logprior, [], 2); % max across classes

errorsBayes = find(yhatBayes ~= Ytest);
errorsPlugin = find(yhatPlugin ~= Ytest);
errRateBayes = length(errorsBayes)/length(Ytest);
errRatePlugin = length(errorsPlugin)/length(Ytest);


subplot(2,2,3)
for c = 1:C
    ndx = yhatBayes == c;
    plot(Xtest(ndx, 1), Xtest(ndx, 2), [colors(c) symbols(c)]);
    hold on
end
title(sprintf('bayes, err rate %5.3f', errRateBayes))

subplot(2,2,4)
for c = 1:C
    ndx = yhatPlugin == c;
    plot(Xtest(ndx, 1), Xtest(ndx, 2), [colors(c) symbols(c)]);
    hold on
end
title(sprintf('plugin, err rate %5.3f', errRatePlugin))

end


function [X, Y] = makeDataHelper(nPts_n)
% Generate three gaussian clusters in 2d
mu = [0,   0.6;
    -0.3 -0.4;
    1   -0.5]';

var = [0.6, 0.4, 0.6];

N = sum(nPts_n);
dim = size(mu,1);
X = zeros(2, N);
Y = zeros(1, N);

curidx = 1;
for j = 1:3
    X(:,curidx:curidx+nPts_n(j)-1) = repmat(mu(:,j), 1, nPts_n(j))+var(j).*randn(dim, nPts_n(j));
    Y(curidx:curidx+nPts_n(j)-1) = j;
    curidx = curidx + nPts_n(j);
end
X = X'; % N by 2
Y = Y';
end


##### SOURCE END #####
--></body></html>