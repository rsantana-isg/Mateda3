
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Compare different classification algorithms on a number of data sets</title><meta name="generator" content="MATLAB 7.12"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2012-03-27"><meta name="DC.source" content="classificationShootout.m"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h1>Compare different classification algorithms on a number of data sets</h1><!--introduction--><p>Based on table 2 of ""Learning sparse Bayesian classifiers: multi-class formulation, fast algorithms, and generalization bounds", Krishnapuram et al, PAMI 2005</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#6">Evaluate the performance of a method on a given data set.</a></li><li><a href="#7">record sparsity level</a></li><li><a href="#9">Display the results in a table</a></li><li><a href="#11">Load various dataSets, and standardize the format</a></li><li><a href="#12">Crabs</a></li><li><a href="#13">fisherIris</a></li><li><a href="#14">Bankruptcy</a></li><li><a href="#15">Pimatr</a></li><li><a href="#16">Soy</a></li><li><a href="#17">Fglass</a></li><li><a href="#18">Display data set info in a latex table</a></li></ul></div><pre class="codeinput"><span class="comment">% This file is from pmtk3.googlecode.com</span>

<span class="keyword">function</span> results = classificationShootout()
</pre><pre class="codeinput"><span class="comment">%PMTKreallySlow (about 8 hours with current cv grid resolution)</span>
<span class="comment">% See also classificationShootoutCvLambdaOnly</span>
<span class="comment">% which is a faster version of this demo</span>
</pre><pre class="codeinput">setSeed(0);
doLatex = true;
doHtml  = true;
split   = 0.7; <span class="comment">% 70% training data, 30% testing</span>
</pre><pre class="codeinput">dataSets = setupData(split);
nDataSets = numel(dataSets);

methods = {<span class="string">'SVM'</span>, <span class="string">'RVM'</span>, <span class="string">'SMLR'</span>, <span class="string">'RMLR'</span>};
<span class="keyword">if</span> ~svmInstalled
  methods = {<span class="string">'SVM'</span>, <span class="string">'RVM'</span>, <span class="string">'SMLR'</span>, <span class="string">'RMLR'</span>};
<span class="keyword">end</span>

nMethods = numel(methods);
results = cell(nDataSets, nMethods);
<span class="keyword">for</span> i=1:nDataSets
    <span class="keyword">for</span> j=1:nMethods
        fprintf(<span class="string">'%s:%s'</span>, dataSets(i).name, methods{j});
        R = evaluateMethod(methods{j}, dataSets(i), split);
        fprintf(<span class="string">':nerrs=%d/%d:(nsvecs=%d/%d)\n'</span>, R.nerrs, R.nTest, R.nsvecs, R.nTrain*R.nClasses);
        results{i, j} = R;
    <span class="keyword">end</span>
<span class="keyword">end</span>
displayResults(results, methods, {dataSets.name}, doLatex, doHtml);
</pre><pre class="codeinput"><span class="keyword">end</span>

<span class="keyword">function</span> results = evaluateMethod(method, dataSet, split)
</pre><h2>Evaluate the performance of a method on a given data set.<a name="6"></a></h2><pre class="codeinput">tic;
X      = rescaleData(standardizeCols(dataSet.X));
y      = dataSet.y;
N      = size(X, 1);
nTrain = floor(split*N);
nTest  = N - nTrain;
Xtrain = X(1:nTrain, :);
Xtest  = X(nTrain+1:end, :);
yTrain = y(1:nTrain);
yTest  = y(nTrain+1:end);

lambdaRange = 1./(2.^(-5:0.5:15));
gammaRange  = 2.^(-15:0.5:3);
twoDgrid = crossProduct(lambdaRange, gammaRange);

<span class="keyword">switch</span> method
    <span class="keyword">case</span> <span class="string">'SVM'</span>

        fitFn = @(X, y, param)<span class="keyword">...</span>
            svmFit(X, y, <span class="string">'C'</span>, 1./param(1), <span class="string">'kernelParam'</span>, param(2), <span class="string">'kernel'</span>, <span class="string">'rbf'</span>);
        predictFn = @svmPredict;
        paramSpace = twoDgrid;

    <span class="keyword">case</span> <span class="string">'RVM'</span>

        <span class="comment">%fitFn = @rvmFit;</span>
        fitFn = @(X,y,gamma) rvmFit(X,y, <span class="string">'kernelFn'</span>,<span class="keyword">...</span>
            @(X1, X2)kernelRbfGamma(X1, X2, gamma));
        predictFn = @rvmPredict;
        paramSpace = gammaRange;

    <span class="keyword">case</span> <span class="string">'SMLR'</span>

        fitFn = @(X, y, param)logregFit(X, y, <span class="keyword">...</span>
            <span class="string">'lambda'</span> , param(1), <span class="keyword">...</span>
            <span class="string">'regType'</span>, <span class="string">'L1'</span>,<span class="keyword">...</span>
            <span class="string">'preproc'</span>, preprocessorCreate(<span class="string">'kernelFn'</span>,<span class="keyword">...</span>
            @(X1, X2)kernelRbfGamma(X1, X2, param(2))));
        predictFn = @logregPredict;
        paramSpace = twoDgrid;

    <span class="keyword">case</span> <span class="string">'RMLR'</span>

       fitFn = @(X, y, param)logregFit(X, y, <span class="keyword">...</span>
            <span class="string">'lambda'</span> , param(1), <span class="keyword">...</span>
            <span class="string">'regType'</span>, <span class="string">'L2'</span>,<span class="keyword">...</span>
            <span class="string">'preproc'</span>, preprocessorCreate(<span class="string">'kernelFn'</span>,<span class="keyword">...</span>
            @(X1, X2)kernelRbfGamma(X1, X2, param(2))));
        predictFn = @logregPredict;
        paramSpace = twoDgrid;

<span class="keyword">end</span>

lossFn = @(yTest, yHat)mean(yHat ~= yTest);
nfolds = 5;
[model, bestParams]  = fitCv(paramSpace, fitFn, predictFn, lossFn, Xtrain, yTrain, nfolds);
results.trainingTime = toc;
tic
yHat   = predictFn(model, Xtest);
results.testingTime = toc;
nerrs  = sum(yHat ~= yTest);

results.method      = method;
results.dataSetName = dataSet.name;
results.nClasses    = dataSet.nClasses;
results.nFeatures   = dataSet.nFeatures;
results.nTrain      = nTrain;
results.nTest       = nTest;
results.nerrs       = nerrs;
results.bestParams  = bestParams;
</pre><h2>record sparsity level<a name="7"></a></h2><pre class="codeinput"><span class="keyword">switch</span> method
    <span class="keyword">case</span> <span class="string">'SVM'</span>
        results.nsvecs = model.nsvecs; <span class="comment">% total for all classes</span>
    <span class="keyword">case</span> <span class="string">'RVM'</span>

        <span class="keyword">if</span> results.nClasses &lt; 3
            results.nsvecs = numel(model.Relevant);
        <span class="keyword">else</span>
            nsvecs = 0;
            <span class="keyword">for</span> i=1:results.nClasses
                nsvecs = nsvecs + numel(model.modelClass{i}.Relevant);
            <span class="keyword">end</span>
            results.nsvecs = nsvecs;
        <span class="keyword">end</span>

    <span class="keyword">case</span> <span class="string">'SMLR'</span>

        w = model.w;
        nsvecs = 0;
        <span class="keyword">for</span> i=1:size(w, 2)
           nsvecs = nsvecs +  sum(~arrayfun(@(x)approxeq(x, 0), w(:, i)));
        <span class="keyword">end</span>
       results.nsvecs = nsvecs;

    <span class="keyword">case</span> <span class="string">'RMLR'</span>
        results.nsvecs = nTrain*results.nClasses;
<span class="keyword">end</span>
</pre><pre class="codeinput"><span class="keyword">end</span>

<span class="keyword">function</span> displayResults(results, methods, dataSetNames, doLatex, doHtml)
</pre><h2>Display the results in a table<a name="9"></a></h2><pre class="codeinput">[ndata, nmeth] = size(results);
data = cell(nmeth, ndata);
<span class="keyword">for</span> i=1:nmeth
    <span class="keyword">for</span> j=1:ndata
        R = results{j, i};
        <span class="keyword">if</span> ~isempty(R)
           data{i, j} =  sprintf(<span class="string">'%d (%d)'</span>, R.nerrs, R.nsvecs);
        <span class="keyword">end</span>
    <span class="keyword">end</span>
<span class="keyword">end</span>

trainingTime = zeros(nmeth, 1);
testingTime  = zeros(nmeth, 1);
<span class="keyword">for</span> i=1:nmeth
    <span class="keyword">for</span> j=1:ndata
        trainingTime(i) = trainingTime(i) + results{j, i}.trainingTime;
        testingTime(i) = testingTime(i) + results{j, i}.testingTime;
    <span class="keyword">end</span>
<span class="keyword">end</span>
data = [data, mat2cellRows(num2str(trainingTime/60, <span class="string">'%.2g'</span>)), mat2cellRows(num2str(testingTime, <span class="string">'%.2g'</span>))];

data = [data; cell(1, ndata + 2)];
<span class="keyword">for</span> j=1:ndata
    R = results{j, 1};
    <span class="keyword">if</span> ~isempty(R)
        data{nmeth+1, j} = sprintf(<span class="string">'%d (%d)'</span>, R.nTest, R.nTrain*R.nClasses);
    <span class="keyword">end</span>
<span class="keyword">end</span>

rowNames = [methods(:); {<span class="string">'Out of'</span>}];
colNames = [dataSetNames(:)', {<span class="string">'train(minutes)'</span>, <span class="string">'test(seconds)'</span>}];

<span class="keyword">if</span> doHtml
    htmlTable(<span class="string">'data'</span>, data, <span class="string">'colNames'</span>, colNames, <span class="string">'rowNames'</span>, rowNames);
<span class="keyword">end</span>
<span class="keyword">if</span> doLatex
    latextable(data, <span class="string">'Vert'</span>, rowNames, <span class="string">'Horiz'</span>, colNames);
<span class="keyword">end</span>

disp(data) <span class="comment">% make sure publishing the demo displays something to the screen!</span>
</pre><pre class="codeinput"><span class="keyword">end</span>



<span class="keyword">function</span> dataSets = setupData(split)
</pre><h2>Load various dataSets, and standardize the format<a name="11"></a></h2><h2>Crabs<a name="12"></a></h2><pre class="codeinput">loadData(<span class="string">'crabs'</span>);
X = [Xtrain; Xtest];
y = [ytrain; ytest];
[X, y] = shuffleRows(X, y);
dataSets(1).X = X;
dataSets(1).y = y;
dataSets(1).name = <span class="string">'Crabs'</span>;
dataSets(1).nClasses  = 2;
dataSets(1).nFeatures = 5;
</pre><h2>fisherIris<a name="13"></a></h2><pre class="codeinput">loadData(<span class="string">'fisherIrisData'</span>);
X = meas;
y = canonizeLabels(species);
[X, y] = shuffleRows(X, y);
dataSets(2).X = X;
dataSets(2).y = y;
dataSets(2).name = <span class="string">'Iris'</span>;
dataSets(2).nClasses  = 3;
dataSets(2).nFeatures = 4;
</pre><h2>Bankruptcy<a name="14"></a></h2><pre class="codeinput">loadData(<span class="string">'bankruptcy'</span>);
X = data(:, 2:end);
y = data(:, 1);
[X, y] = shuffleRows(X, y);
dataSets(3).X = X;
dataSets(3).y = y;
dataSets(3).name = <span class="string">'Bankruptcy'</span>;
dataSets(3).nClasses  = 2;
dataSets(3).nFeatures = 2;
</pre><h2>Pimatr<a name="15"></a></h2><pre class="codeinput">loadData(<span class="string">'pimatr'</span>)
X = data(:, 2:end-1);
y = data(:, end);
[X, y] = shuffleRows(X, y);
dataSets(4).X = X;
dataSets(4).y = y;
dataSets(4).name = <span class="string">'Pima'</span>;
dataSets(4).nClasses  = 2;
dataSets(4).nFeatures = 7;
</pre><h2>Soy<a name="16"></a></h2><pre class="codeinput">loadData(<span class="string">'soy'</span>)
[X, y] = shuffleRows(X, Y);
dataSets(5).X = X;
dataSets(5).y = y;
dataSets(5).name = <span class="string">'Soy'</span>;
dataSets(5).nClasses = 3;
dataSets(5).nFeatures = 35;
</pre><h2>Fglass<a name="17"></a></h2><pre class="codeinput">loadData(<span class="string">'fglass'</span>);
X = [Xtrain; Xtest];
y = [ytrain; ytest];
[X, y] = shuffleRows(X, y);
dataSets(6).X = X;
dataSets(6).y = y;
dataSets(6).name = <span class="string">'Fglass'</span>;
dataSets(6).nClasses  = 6;
dataSets(6).nFeatures = 9;
</pre><h2>Display data set info in a latex table<a name="18"></a></h2><pre class="codeinput">fprintf(<span class="string">'Data Sets\n\n'</span>);
nDataSets = numel(dataSets);
table = zeros(4, nDataSets);
dnames = cell(1, nDataSets);
<span class="keyword">for</span> i=1:nDataSets
    table(1, i) = dataSets(i).nClasses;
    table(2, i) = dataSets(i).nFeatures;
    N = size(dataSets(i).X, 1);
    table(3, i) = floor(split*N);
    table(4, i) =  N - table(3, i);
    dnames{i} = dataSets(i).name;
<span class="keyword">end</span>
Vert = {<span class="string">'Num. Classes'</span>, <span class="string">'Num. Features'</span>, <span class="string">'Num. train'</span>, <span class="string">'Num. test'</span>};
latextable(table, <span class="string">'Horiz'</span>, dnames, <span class="string">'Vert'</span>, Vert, <span class="string">'format'</span>, <span class="string">'%d'</span>);
fprintf(<span class="string">'\n\n'</span>);
</pre><pre class="codeinput"><span class="keyword">end</span>
</pre><p class="footer"><br>
      Published with MATLAB&reg; 7.12<br></p></div><!--
##### SOURCE BEGIN #####
%% Compare different classification algorithms on a number of data sets
% Based on table 2 of 
% ""Learning sparse Bayesian classifiers: multi-class formulation, fast
% algorithms, and generalization bounds", Krishnapuram et al, PAMI 2005
%%

% This file is from pmtk3.googlecode.com

function results = classificationShootout()
%PMTKreallySlow (about 8 hours with current cv grid resolution)
% See also classificationShootoutCvLambdaOnly
% which is a faster version of this demo

%%
setSeed(0);
doLatex = true; 
doHtml  = true;
split   = 0.7; % 70% training data, 30% testing

%%
dataSets = setupData(split);
nDataSets = numel(dataSets);

methods = {'SVM', 'RVM', 'SMLR', 'RMLR'};
if ~svmInstalled
  methods = {'SVM', 'RVM', 'SMLR', 'RMLR'};
end

nMethods = numel(methods);
results = cell(nDataSets, nMethods);
for i=1:nDataSets
    for j=1:nMethods
        fprintf('%s:%s', dataSets(i).name, methods{j}); 
        R = evaluateMethod(methods{j}, dataSets(i), split);
        fprintf(':nerrs=%d/%d:(nsvecs=%d/%d)\n', R.nerrs, R.nTest, R.nsvecs, R.nTrain*R.nClasses);
        results{i, j} = R; 
    end
end
displayResults(results, methods, {dataSets.name}, doLatex, doHtml);
end

function results = evaluateMethod(method, dataSet, split)
%% Evaluate the performance of a method on a given data set.
tic;
X      = rescaleData(standardizeCols(dataSet.X)); 
y      = dataSet.y;
N      = size(X, 1);
nTrain = floor(split*N);
nTest  = N - nTrain;
Xtrain = X(1:nTrain, :);
Xtest  = X(nTrain+1:end, :);
yTrain = y(1:nTrain);
yTest  = y(nTrain+1:end);

lambdaRange = 1./(2.^(-5:0.5:15));
gammaRange  = 2.^(-15:0.5:3); 
twoDgrid = crossProduct(lambdaRange, gammaRange); 

switch method
    case 'SVM'
        
        fitFn = @(X, y, param)...
            svmFit(X, y, 'C', 1./param(1), 'kernelParam', param(2), 'kernel', 'rbf');
        predictFn = @svmPredict;
        paramSpace = twoDgrid;
        
    case 'RVM'
        
        %fitFn = @rvmFit; 
        fitFn = @(X,y,gamma) rvmFit(X,y, 'kernelFn',...
            @(X1, X2)kernelRbfGamma(X1, X2, gamma));
        predictFn = @rvmPredict;
        paramSpace = gammaRange; 
        
    case 'SMLR'
        
        fitFn = @(X, y, param)logregFit(X, y, ...
            'lambda' , param(1), ...
            'regType', 'L1',...
            'preproc', preprocessorCreate('kernelFn',...
            @(X1, X2)kernelRbfGamma(X1, X2, param(2))));
        predictFn = @logregPredict;
        paramSpace = twoDgrid; 
        
    case 'RMLR'
        
       fitFn = @(X, y, param)logregFit(X, y, ...
            'lambda' , param(1), ...
            'regType', 'L2',...
            'preproc', preprocessorCreate('kernelFn',...
            @(X1, X2)kernelRbfGamma(X1, X2, param(2))));
        predictFn = @logregPredict;
        paramSpace = twoDgrid; 
        
end

lossFn = @(yTest, yHat)mean(yHat ~= yTest);
nfolds = 5;
[model, bestParams]  = fitCv(paramSpace, fitFn, predictFn, lossFn, Xtrain, yTrain, nfolds);
results.trainingTime = toc;
tic
yHat   = predictFn(model, Xtest);
results.testingTime = toc; 
nerrs  = sum(yHat ~= yTest);

results.method      = method;
results.dataSetName = dataSet.name;
results.nClasses    = dataSet.nClasses;
results.nFeatures   = dataSet.nFeatures;
results.nTrain      = nTrain;
results.nTest       = nTest;
results.nerrs       = nerrs;
results.bestParams  = bestParams; 
%% record sparsity level
switch method
    case 'SVM'
        results.nsvecs = model.nsvecs; % total for all classes
    case 'RVM'
        
        if results.nClasses < 3
            results.nsvecs = numel(model.Relevant); 
        else
            nsvecs = 0;
            for i=1:results.nClasses
                nsvecs = nsvecs + numel(model.modelClass{i}.Relevant);
            end
            results.nsvecs = nsvecs; 
        end
        
    case 'SMLR'
        
        w = model.w;
        nsvecs = 0;
        for i=1:size(w, 2)
           nsvecs = nsvecs +  sum(~arrayfun(@(x)approxeq(x, 0), w(:, i)));
        end
       results.nsvecs = nsvecs; 
        
    case 'RMLR'
        results.nsvecs = nTrain*results.nClasses; 
end
end

function displayResults(results, methods, dataSetNames, doLatex, doHtml)
%% Display the results in a table

[ndata, nmeth] = size(results); 
data = cell(nmeth, ndata); 
for i=1:nmeth
    for j=1:ndata
        R = results{j, i}; 
        if ~isempty(R)
           data{i, j} =  sprintf('%d (%d)', R.nerrs, R.nsvecs); 
        end
    end
end

trainingTime = zeros(nmeth, 1); 
testingTime  = zeros(nmeth, 1); 
for i=1:nmeth
    for j=1:ndata
        trainingTime(i) = trainingTime(i) + results{j, i}.trainingTime; 
        testingTime(i) = testingTime(i) + results{j, i}.testingTime; 
    end
end
data = [data, mat2cellRows(num2str(trainingTime/60, '%.2g')), mat2cellRows(num2str(testingTime, '%.2g'))];

data = [data; cell(1, ndata + 2)];
for j=1:ndata
    R = results{j, 1};
    if ~isempty(R)
        data{nmeth+1, j} = sprintf('%d (%d)', R.nTest, R.nTrain*R.nClasses); 
    end
end

rowNames = [methods(:); {'Out of'}]; 
colNames = [dataSetNames(:)', {'train(minutes)', 'test(seconds)'}];

if doHtml
    htmlTable('data', data, 'colNames', colNames, 'rowNames', rowNames); 
end
if doLatex
    latextable(data, 'Vert', rowNames, 'Horiz', colNames); 
end

disp(data) % make sure publishing the demo displays something to the screen!

end



function dataSets = setupData(split)
%% Load various dataSets, and standardize the format

%% Crabs
loadData('crabs');
X = [Xtrain; Xtest];
y = [ytrain; ytest];
[X, y] = shuffleRows(X, y);
dataSets(1).X = X;
dataSets(1).y = y;
dataSets(1).name = 'Crabs';
dataSets(1).nClasses  = 2;
dataSets(1).nFeatures = 5;
%% fisherIris
loadData('fisherIrisData');
X = meas;
y = canonizeLabels(species);
[X, y] = shuffleRows(X, y);
dataSets(2).X = X;
dataSets(2).y = y;
dataSets(2).name = 'Iris';
dataSets(2).nClasses  = 3;
dataSets(2).nFeatures = 4;
%% Bankruptcy
loadData('bankruptcy'); 
X = data(:, 2:end); 
y = data(:, 1); 
[X, y] = shuffleRows(X, y); 
dataSets(3).X = X;
dataSets(3).y = y;
dataSets(3).name = 'Bankruptcy';
dataSets(3).nClasses  = 2;
dataSets(3).nFeatures = 2;
%% Pimatr
loadData('pimatr')
X = data(:, 2:end-1); 
y = data(:, end); 
[X, y] = shuffleRows(X, y); 
dataSets(4).X = X;
dataSets(4).y = y;
dataSets(4).name = 'Pima';
dataSets(4).nClasses  = 2;
dataSets(4).nFeatures = 7;
%% Soy
loadData('soy')
[X, y] = shuffleRows(X, Y); 
dataSets(5).X = X;
dataSets(5).y = y;
dataSets(5).name = 'Soy';
dataSets(5).nClasses = 3;
dataSets(5).nFeatures = 35; 
%% Fglass
loadData('fglass');
X = [Xtrain; Xtest];
y = [ytrain; ytest];
[X, y] = shuffleRows(X, y);
dataSets(6).X = X;
dataSets(6).y = y;
dataSets(6).name = 'Fglass';
dataSets(6).nClasses  = 6;
dataSets(6).nFeatures = 9;
%% Display data set info in a latex table

fprintf('Data Sets\n\n'); 
nDataSets = numel(dataSets);
table = zeros(4, nDataSets); 
dnames = cell(1, nDataSets); 
for i=1:nDataSets
    table(1, i) = dataSets(i).nClasses;
    table(2, i) = dataSets(i).nFeatures;
    N = size(dataSets(i).X, 1);  
    table(3, i) = floor(split*N);
    table(4, i) =  N - table(3, i);
    dnames{i} = dataSets(i).name; 
end
Vert = {'Num. Classes', 'Num. Features', 'Num. train', 'Num. test'};
latextable(table, 'Horiz', dnames, 'Vert', Vert, 'format', '%d');
fprintf('\n\n'); 


end

##### SOURCE END #####
--></body></html>