
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Minfunc Kernelized Logreg Demo</title><meta name="generator" content="MATLAB 7.12"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2012-03-27"><meta name="DC.source" content="logregMultinomKernelMinfuncDemo.m"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h1>Minfunc Kernelized Logreg Demo</h1><!--introduction--><p>PMTKauthor Mark Schmidt PMTKmodified Kevin Murphy PMTKurl <a href="http://people.cs.ubc.ca/~schmidtm/Software/minFunc/minFunc.html#7">http://people.cs.ubc.ca/~schmidtm/Software/minFunc/minFunc.html#7</a> It is modified by replacing penalizedKernelL2_matrix, which uses sum_c w(:,c)' K w(:,c) as the regularizer, with the simpler penalizedL2, which uses w' w as the regularizer. The key difference is that we only use kernels to do basis funcion expansion on X; we do not change the regularizer. This makes hardly any difference to the training error.</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#2">Linear</a></li><li><a href="#3">Polynomial</a></li><li><a href="#4">RBF</a></li><li><a href="#5">Compute training errors</a></li></ul></div><pre class="codeinput"><span class="comment">% This file is from pmtk3.googlecode.com</span>

options.Display = <span class="string">'none'</span>;
setSeed(0);
nClasses = 5;
<span class="comment">%nInstances = 1000;</span>
nInstances = 100;
nVars = 2;

[X,y] = makeData(<span class="string">'multinomialNonlinear'</span>,nInstances,nVars,nClasses);

figure;
[n,p] = size(X);
colors = getColorsRGB;
hold <span class="string">on</span>
<span class="keyword">for</span> c = 1:nClasses
   <span class="keyword">if</span> p == 3
      plot(X(y==c,2),X(y==c,3),<span class="string">'.'</span>,<span class="string">'color'</span>,colors(c,:));
   <span class="keyword">else</span>
      plot(X(y==c,1),X(y==c,2),<span class="string">'.'</span>,<span class="string">'color'</span>,colors(c,:));
   <span class="keyword">end</span>
<span class="keyword">end</span>
lambda = 1e-2;
</pre><img vspace="5" hspace="5" src="logregMultinomKernelMinfuncDemo_01.png" alt=""> <h2>Linear<a name="2"></a></h2><pre class="codeinput">funObj = @(w)SoftmaxLoss2(w,X,y,nClasses);
fprintf(<span class="string">'Training linear multinomial logistic regression model...\n'</span>);
wLinear = minFunc(@penalizedL2,zeros(nVars*(nClasses-1),1),options,funObj,lambda);
wLinear = reshape(wLinear,[nVars nClasses-1]);
wLinear = [wLinear zeros(nVars,1)];
</pre><pre class="codeoutput">Training linear multinomial logistic regression model...
</pre><h2>Polynomial<a name="3"></a></h2><pre class="codeinput">polyOrder = 2;
Kpoly = kernelPoly(X,X,polyOrder);
funObj = @(u)SoftmaxLoss2(u,Kpoly,y,nClasses);
fprintf(<span class="string">'Training kernel(poly) multinomial logistic regression model...\n'</span>);
uPoly = minFunc(@penalizedL2,randn(nInstances*(nClasses-1),1),options,funObj,lambda);
<span class="comment">%uPoly = minFunc(@penalizedKernelL2_matrix,randn(nInstances*(nClasses-1),1),options,Kpoly,nClasses-1,funObj,lambda);</span>
uPoly = reshape(uPoly,[nInstances nClasses-1]);
uPoly = [uPoly zeros(nInstances,1)];
</pre><pre class="codeoutput">Training kernel(poly) multinomial logistic regression model...
</pre><h2>RBF<a name="4"></a></h2><pre class="codeinput">rbfScale = 1;
Krbf = kernelRbfSigma(X,X,rbfScale);
funObj = @(u)SoftmaxLoss2(u,Krbf,y,nClasses);
fprintf(<span class="string">'Training kernel(rbf) multinomial logistic regression model...\n'</span>);
uRBF = minFunc(@penalizedL2,randn(nInstances*(nClasses-1),1),options,funObj,lambda);
<span class="comment">%uRBF = minFunc(@penalizedKernelL2_matrix,randn(nInstances*(nClasses-1),1),options,Krbf,nClasses-1,funObj,lambda);</span>
uRBF = reshape(uRBF,[nInstances nClasses-1]);
uRBF = [uRBF zeros(nInstances,1)];
</pre><pre class="codeoutput">Training kernel(rbf) multinomial logistic regression model...
</pre><h2>Compute training errors<a name="5"></a></h2><pre class="codeinput">[junk yhat] = max(X*wLinear,[],2);
trainErr_linear = sum(y~=yhat)/length(y)
[junk yhat] = max(Kpoly*uPoly,[],2);
trainErr_poly = sum(y~=yhat)/length(y)
[junk yhat] = max(Krbf*uRBF,[],2);
trainErr_rbf = sum(y~=yhat)/length(y)



figure;
plotClassifier(X,y,wLinear,<span class="string">'Linear Multinomial Logistic Regression'</span>);

figure;
plotClassifier(X,y,uPoly,<span class="string">'Kernel-Poly Multinomial Logistic Regression'</span>,@kernelPoly,polyOrder);

figure;
plotClassifier(X,y,uRBF,<span class="string">'Kernel-RBF Multinomial Logistic Regression'</span>,@kernelRbfSigma,rbfScale);
</pre><pre class="codeoutput">trainErr_linear =
   0.240000000000000
trainErr_poly =
   0.140000000000000
trainErr_rbf =
   0.190000000000000
</pre><img vspace="5" hspace="5" src="logregMultinomKernelMinfuncDemo_02.png" alt=""> <img vspace="5" hspace="5" src="logregMultinomKernelMinfuncDemo_03.png" alt=""> <img vspace="5" hspace="5" src="logregMultinomKernelMinfuncDemo_04.png" alt=""> <p class="footer"><br>
      Published with MATLAB&reg; 7.12<br></p></div><!--
##### SOURCE BEGIN #####
%% Minfunc Kernelized Logreg Demo
% PMTKauthor Mark Schmidt
% PMTKmodified Kevin Murphy
% PMTKurl http://people.cs.ubc.ca/~schmidtm/Software/minFunc/minFunc.html#7
% It is modified by replacing penalizedKernelL2_matrix,
% which uses sum_c w(:,c)' K w(:,c) as the regularizer,
% with the simpler penalizedL2, which uses w' w as the regularizer.
% The key difference is that we only use kernels to do basis funcion
% expansion on X; we do not change the regularizer.
% This makes hardly any difference to the training error.
%%

% This file is from pmtk3.googlecode.com

options.Display = 'none';
setSeed(0); 
nClasses = 5;
%nInstances = 1000;
nInstances = 100;
nVars = 2;

[X,y] = makeData('multinomialNonlinear',nInstances,nVars,nClasses);

figure;
[n,p] = size(X);
colors = getColorsRGB;
hold on
for c = 1:nClasses
   if p == 3
      plot(X(y==c,2),X(y==c,3),'.','color',colors(c,:));
   else
      plot(X(y==c,1),X(y==c,2),'.','color',colors(c,:));
   end
end
lambda = 1e-2;
%% Linear
funObj = @(w)SoftmaxLoss2(w,X,y,nClasses);
fprintf('Training linear multinomial logistic regression model...\n');
wLinear = minFunc(@penalizedL2,zeros(nVars*(nClasses-1),1),options,funObj,lambda);
wLinear = reshape(wLinear,[nVars nClasses-1]);
wLinear = [wLinear zeros(nVars,1)];
%% Polynomial
polyOrder = 2;
Kpoly = kernelPoly(X,X,polyOrder);
funObj = @(u)SoftmaxLoss2(u,Kpoly,y,nClasses);
fprintf('Training kernel(poly) multinomial logistic regression model...\n');
uPoly = minFunc(@penalizedL2,randn(nInstances*(nClasses-1),1),options,funObj,lambda);
%uPoly = minFunc(@penalizedKernelL2_matrix,randn(nInstances*(nClasses-1),1),options,Kpoly,nClasses-1,funObj,lambda);
uPoly = reshape(uPoly,[nInstances nClasses-1]);
uPoly = [uPoly zeros(nInstances,1)];
%% RBF
rbfScale = 1;
Krbf = kernelRbfSigma(X,X,rbfScale);
funObj = @(u)SoftmaxLoss2(u,Krbf,y,nClasses);
fprintf('Training kernel(rbf) multinomial logistic regression model...\n');
uRBF = minFunc(@penalizedL2,randn(nInstances*(nClasses-1),1),options,funObj,lambda);
%uRBF = minFunc(@penalizedKernelL2_matrix,randn(nInstances*(nClasses-1),1),options,Krbf,nClasses-1,funObj,lambda);
uRBF = reshape(uRBF,[nInstances nClasses-1]);
uRBF = [uRBF zeros(nInstances,1)];
%% Compute training errors
[junk yhat] = max(X*wLinear,[],2);
trainErr_linear = sum(y~=yhat)/length(y)
[junk yhat] = max(Kpoly*uPoly,[],2);
trainErr_poly = sum(y~=yhat)/length(y)
[junk yhat] = max(Krbf*uRBF,[],2);
trainErr_rbf = sum(y~=yhat)/length(y)



figure;
plotClassifier(X,y,wLinear,'Linear Multinomial Logistic Regression');

figure;
plotClassifier(X,y,uPoly,'Kernel-Poly Multinomial Logistic Regression',@kernelPoly,polyOrder);

figure;
plotClassifier(X,y,uRBF,'Kernel-RBF Multinomial Logistic Regression',@kernelRbfSigma,rbfScale);


##### SOURCE END #####
--></body></html>