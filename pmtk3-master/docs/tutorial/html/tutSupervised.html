
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      --><title>Supervised learning in pmtk3</title><meta name="generator" content="MATLAB 7.9"><meta name="date" content="2010-09-08"><meta name="m-file" content="tutSupervised"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h1>Supervised learning in pmtk3</h1><!--introduction--><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Models</a></li><li><a href="#4">Creating a model</a></li><li><a href="#7">Using a model for prediction</a></li><li><a href="#14">More information</a></li></ul></div><h2>Models<a name="1"></a></h2><p>The following is a list  (in alphabetical order) of pmtk models that are designed for supervised learning. We have classified the models based on whether they can be used for classification <img src="tutSupervised_eq35391.png" alt="$y \in \{1,\ldots,C\}$">, regression <img src="tutSupervised_eq14315.png" alt="$y \in R$">, or both; whether they are generative models of the form <img src="tutSupervised_eq35640.png" alt="$p(y,x|\theta)$"> or discriminative models of the form <img src="tutSupervised_eq42833.png" alt="$p(y|x,\theta)$">; and whether they are parametric (so <img src="tutSupervised_eq49435.png" alt="$\theta$"> has fixed size) or non-parametric (so <img src="tutSupervised_eq49435.png" alt="$\theta$"> grows as the training set gets larger). We assume y is a low-dimensional scalar. Models for multivariate conditional density estimation (structured output classification/ regression) will be added later.</p><p>
<table border=1>
<TR ALIGN=left>
<td> Model
<td> Description
<td> Classif/regr
<td> Gen/Discr
<td> Param/non
<tr>
<td> discrimAnalysis
<td> Discriminant analysis (linear, quadratic, regularized, shrunken)
<td> Classif
<td> Gen
<td> Param
<tr>
<td> generativeClassifier
<td> Any class conditional density
<td> Classif
<td> Gen
<td> Param
<tr>
<td> knn
<td> k nearest neighbors
<td> Classif
<td> Gen
<td> Nonparam
<tr>
<td> linreg
<td> Linear regression
<td> Regr
<td> Discrim
<td> Param
<tr>
<td> logreg
<td> Logistic regression
<td> Classif
<td> Discrim
<td> Param
<tr>
<td> mlp
<td> multi-layer perceptron (aka feedforward neural network)
<td> Both
<td> Discrim
<td> Param
<tr>
<td> naiveBayes
<td> Naive Bayes classifier
<td> Classif
<td> Gen
<td> Param
<tr>
<td> rvm
<td> Relevance vector machine
<td> Both
<td> Discrim
<td> Nonparam
<tr>
<td> smlr
<td> Sparse multinomial logistic regression
<td> Both
<td> Discrim
<td> Nonparam
<tr>
<td> svm
<td> Support vector machine
<td> Both
<td> Discrim
<td> Nonparam
</table>
</p><p>More models may be added in the future.</p><h2>Creating a model<a name="4"></a></h2><p>To create a model of type 'foo', use one of the following</p><div><ul><li><tt>model = fooCreate(...)</tt> % manually specify parameters</li><li><tt>model = fooFit(X, y, ...)</tt> % Compute ML or MAP estimate of params</li><li><tt>model = fooFitBayes(X, y, ...)</tt> % Compute posterior of params</li></ul></div><p>where</p><div><ul><li> '...' refers to optional arguments (see below)</li><li>X  is an N*D design matrix containing the training data,  where N is the number of training cases and D is the number of features.</li><li>y is an N*1 response vector, which can be real-valued (regression),     0/1 or -1/+1 (binary classification), or 1:C (multi-class).</li></ul></div><p>If X contains missing values, represented as NaNs, it is best to use a generative model (although not all models currently support this functionality). NaNs in y correspond to semi-supervised learning, which is not yet supported.</p><p>The output of create/ fit is a Matlab structure representing the model. However, we will sometimes call it an 'object', since it behaves like one in many respects.</p><p>In the case of <tt>fooCreate</tt> and <tt>fooFit</tt>, the parameters are point estimates. In the case of <tt>fooFitBayes</tt>, we store a distribution over the parameters, which may be represented parameterically or as a bag of samples. The details will be explained later.</p><h2>Using a model for prediction<a name="7"></a></h2><p>Once the model has been created, you can use it to make predictions  as follows</p><pre>[yhat, py] = fooPredict(model, Xtest) % plugin approximation
[yhat, py] = fooPredictBayes(model, Xtest) % posterior predictive</pre><p>Here Xtest is an Ntest*D matrix of test inputs, and yhat is an Ntest*1 vector of predicted responses of the same type as ytrain (e.g., if ytrain was {-1,+1}, then ytest will also be {-1,+1}; if ytrain was {1,2}, then ytest will be converted to {1,2} as well.) For regression yhat is the predicted mean, for classification yhat is the predicted mode (most probable class label). The meaning of py depends on the model, as follows:</p><div><ul><li>For regression, py is an Ntest*1 vector of predicted variances.</li><li>For binary classification, py is an Ntest*1 vector of the probability of being in class 1.</li><li>For multi-class, py is an Ntest*C matrix, where py(i,c) = p(y=c|Xtest(i,:),params)</li></ul></div><p>The difference between <tt>predict</tt> and <tt>predictBayes</tt> is as follows. <tt>predict</tt> computes</p><p><img src="tutSupervised_eq17118.png" alt="$$p(y|x,\hat{\theta}(D))$$"></p><p>which "plugs in" a point estimate of the parameters, while <tt>predictBayes</tt> computes</p><p><img src="tutSupervised_eq20716.png" alt="$$p(y|x,D) = \int p(y|x,\theta) p(\theta|D) d\theta$"></p><p>which integrates out the unknown parameter. This is called the (posterior) predictive density. In practice, the Bayesian approach results in similar (often identical) values for yhat, but quite different values for py. In particular, the uncertainty is reflected more accurately in the Bayesian approach, as we illustrate later.</p><h2>More information<a name="14"></a></h2><p>Because supervised learning is such a large topic, we have created various sub-pages describing different approaches in more detail, as follows:</p><div><ul><li><a href="http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutRegr.html">Parametric discriminative models for regression</a></li><li><a href="http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutDiscrimClassif.html">Parametric discriminative models for classification</a></li><li><a href="http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutKernelClassif.html">Kernel-based discriminative models for classification and regression</a></li><li><a href="http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutGenClassif.html">Generative models for classification and regression</a></li><li><a href="http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutMLcomp.html">PMTK interface to mlcomp</a>, a website for comparing machine learning algorithms</li></ul></div><p>
<hr>
</p><p>This page was auto-generated by calling <i>pmtkPublish(C:\kmurphy\GoogleCode\pmtk3\docs\tutorial\tutSupervised.m)</i>  on 08-Sep-2010 17:21:31</p><p class="footer"><br>
      Published with MATLAB&reg; 7.9<br></p></div><!--
##### SOURCE BEGIN #####
%% Supervised learning in pmtk3
%
%% Models
% The following is a list  (in alphabetical order)
% of pmtk models that are designed for 
% supervised learning.
% We have classified the models based on whether they can be used for
% classification $y \in \{1,\ldots,C\}$,
% regression $y \in R$, or both; whether they are generative models of
% the form $p(y,x|\theta)$ or discriminative models of the form $p(y|x,\theta)$;
% and whether they are parametric (so $\theta$ has fixed size) or
% non-parametric (so $\theta$ grows as the training set gets larger).
% We assume y is a low-dimensional scalar.
% Models for multivariate conditional density estimation (structured output
% classification/ regression) will be added later.
%
%%
% <html>
% <table border=1>
% <TR ALIGN=left>
% <td> Model 
% <td> Description
% <td> Classif/regr
% <td> Gen/Discr
% <td> Param/non
% <tr>
% <td> discrimAnalysis
% <td> Discriminant analysis (linear, quadratic, regularized, shrunken) 
% <td> Classif
% <td> Gen
% <td> Param
% <tr>
% <td> generativeClassifier
% <td> Any class conditional density 
% <td> Classif
% <td> Gen
% <td> Param
% <tr>
% <td> knn
% <td> k nearest neighbors
% <td> Classif
% <td> Gen
% <td> Nonparam
% <tr>
% <td> linreg
% <td> Linear regression
% <td> Regr
% <td> Discrim
% <td> Param
% <tr>
% <td> logreg
% <td> Logistic regression 
% <td> Classif
% <td> Discrim
% <td> Param
% <tr>
% <td> mlp
% <td> multi-layer perceptron (aka feedforward neural network)
% <td> Both
% <td> Discrim
% <td> Param
% <tr>
% <td> naiveBayes
% <td> Naive Bayes classifier
% <td> Classif
% <td> Gen
% <td> Param
% <tr> 
% <td> rvm
% <td> Relevance vector machine
% <td> Both
% <td> Discrim
% <td> Nonparam
% <tr>
% <td> smlr
% <td> Sparse multinomial logistic regression
% <td> Both
% <td> Discrim
% <td> Nonparam
% <tr>
% <td> svm
% <td> Support vector machine
% <td> Both
% <td> Discrim
% <td> Nonparam
% </table>
% </html>
%%
% More models may be added in the future.
%
%
%% Creating a model
% To create a model of type 'foo', use one of the following
%%
% * |model = fooCreate(...)| % manually specify parameters
% * |model = fooFit(X, y, ...)| % Compute ML or MAP estimate of params
% * |model = fooFitBayes(X, y, ...)| % Compute posterior of params
%%
% where
%
% *  '...' refers to optional arguments (see below)
% * X  is an N*D design matrix containing the training data,
%  where N is the number of training cases and D is the number of features.
% * y is an N*1 response vector, which can be real-valued (regression),
%     0/1 or -1/+1 (binary classification), or 1:C (multi-class).
%
% If X contains missing values, represented as NaNs,
% it is best to use a generative model (although not all
% models currently support this functionality).
% NaNs in y correspond to semi-supervised learning, which
% is not yet supported.
%
% The output of create/ fit is a Matlab structure
% representing the model.
% However, we will sometimes call it an 'object',
% since it behaves like one in many respects.
%
% In the case of |fooCreate| and |fooFit|, the parameters are point estimates.
% In the case of |fooFitBayes|, we store a distribution over
% the parameters, 
% which may be represented parameterically
% or as a bag of samples. The details will be explained later.


%% Using a model for prediction
% Once the model has been created, you can use it to make predictions
%  as follows
%
%%
%  [yhat, py] = fooPredict(model, Xtest) % plugin approximation
%  [yhat, py] = fooPredictBayes(model, Xtest) % posterior predictive
%%
% Here Xtest is an Ntest*D matrix of test inputs,
% and yhat is an Ntest*1 vector of predicted responses of the same type
% as ytrain (e.g., if ytrain was {-1,+1}, then ytest will also be {-1,+1};
% if ytrain was {1,2}, then ytest will be converted to {1,2} as well.)
% For regression yhat is the predicted mean, for classification yhat is the
% predicted mode (most probable class label).
% The meaning of py depends on the model, as follows:
%   
% * For regression, py is an Ntest*1 vector of predicted variances.
% * For binary classification, py is an Ntest*1 vector of the probability of being in class 1.
% * For multi-class, py is an Ntest*C matrix, where py(i,c) = p(y=c|Xtest(i,:),params)
%
% The difference between |predict| and |predictBayes| is as follows.
% |predict| computes
%%
% $$p(y|x,\hat{\theta}(D))$$
%%
% which "plugs in" a point estimate
% of the parameters, while |predictBayes| computes
%%
% $$p(y|x,D) = \int p(y|x,\theta) p(\theta|D) d\theta$
%%
% which integrates out the unknown parameter.
% This is called the (posterior) predictive density.
% In practice, the Bayesian approach results in similar (often identical)
% values for yhat, but quite different values for py. In particular, the
% uncertainty is reflected more accurately in the Bayesian approach, as we
% illustrate later.

%% More information
% Because supervised learning is such a large topic,
% we have created various sub-pages describing different approaches
% in more detail,
% as follows:
%
% * <http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutRegr.html
% Parametric discriminative models for regression>
% * <http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutDiscrimClassif.html
% Parametric discriminative models for classification>
% * <http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutKernelClassif.html
% Kernel-based discriminative models for classification and regression>
% * <http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutGenClassif.html
% Generative models for classification and regression>
% * <http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutMLcomp.html
% PMTK interface to mlcomp>, a website for comparing machine learning
% algorithms
%%
% <html>
% <hr>
% </html>
%%

%%
% This page was auto-generated by calling _pmtkPublish(C:\kmurphy\GoogleCode\pmtk3\docs\tutorial\tutSupervised.m)_  on 08-Sep-2010 17:21:31


##### SOURCE END #####
--></body></html>