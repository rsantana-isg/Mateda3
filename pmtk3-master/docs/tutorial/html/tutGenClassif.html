
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      --><title>Supervised learning using generative models in pmtk3</title><meta name="generator" content="MATLAB 7.9"><meta name="date" content="2010-09-08"><meta name="m-file" content="tutGenClassif"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h1>Supervised learning using generative models in pmtk3</h1><!--introduction--><p>Generative models for classification/ regression are joint models of the outputs and inputs of the form <img src="tutGenClassif_eq35640.png" alt="$p(y,x|\theta)$">. We consider various examples below.</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Naive Bayes classifier</a></li><li><a href="#6">Discriminant analysis</a></li><li><a href="#11">QDA/ LDA/ NBC</a></li><li><a href="#14">Regularized discriminant analysis</a></li><li><a href="#15">Nearest shrunken centroid</a></li><li><a href="#19">Robust discriminant analysis</a></li><li><a href="#22">Using HMMs as class conditional densities</a></li><li><a href="#25">K-nearest neighbor classifier</a></li></ul></div><h2>Naive Bayes classifier<a name="1"></a></h2><p>We now a simple kind of generative classifier called naive Bayes, which is a model of the form</p><p><img src="tutGenClassif_eq36665.png" alt="$$p(y,x|\theta) = p(y|\pi) \prod_{j=1}^D p(x_j|y,\theta)$$"></p><p>We can fit and predict with this model using <a href="http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/naiveBayes/naiveBayesFit.m">naiveBayesFit.m</a> and  <a href="http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/naiveBayes/naiveBayesPredict.m">naiveBayesPredict.m</a> For simplicity, the current implementation assumes  all the features are binary, so <img src="tutGenClassif_eq85950.png" alt="$p(x_j|y=c,\theta) = Ber(x_j|\theta_{jc})$">. It fits by MAP estimation with a vague Dirichlet prior (add-one-smoothing). Typically results are not too sensitive to the setting of this prior (unlike discriminative models).</p><p>Below is an example (from <a href="http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Generative_models_for_classification_and_regression/naiveBayesBowDemo.m">naiveBayesBowDemo.m</a> ) which fits a model to some bag of words data, and then classifies a test set.</p><pre class="codeinput">loadData(<span class="string">'XwindowsDocData'</span>); <span class="comment">% 900x600, 2 classes Xwindows vs MSwindows</span>
Xtrain = xtrain; Xtest = xtest;
model = naiveBayesFit(Xtrain, ytrain);
ypred_train = naiveBayesPredict(model, Xtrain);
err_train = mean(zeroOneLossFn(ytrain, ypred_train));
ypred_test = naiveBayesPredict(model, Xtest);
err_test = mean(zeroOneLossFn(ytest, ypred_test));
fprintf(<span class="string">'misclassification rates  on train = %5.2f pc, on test = %5.2f pc\n'</span>, <span class="keyword">...</span>
    err_train*100, err_test*100);
</pre><pre class="codeoutput">misclassification rates  on train =  8.33 pc, on test = 18.67 pc
</pre><p>See also <a href="http://pmtk3.googlecode.com/svn/trunk/demos/otherDemos/supervisedModels/naiveBayesMnistDemo.m">naiveBayesMnistDemo.m</a> for application of NBC to classify binarized MNIST digits.</p><p>It is simple to modify NBC to handle missing data in X at training and test time; this is left as an exercise to the reader.</p><h2>Discriminant analysis<a name="6"></a></h2><p>Discriminant analysis is a generative classifier where the class conditional density  is a multivariate Gaussian:</p><p><img src="tutGenClassif_eq68632.png" alt="$$p(y=c,x|\theta) = \mbox{discrete}(y|\pi) N(x|\mu_c,\Sigma_c)$$"></p><p>PMTK supports the following variants of this model:</p><p>
<table border=1>
<TR ALIGN=left>
<td> Type
<td> Description
<tr>
<td> QDA
<td> <img src="x0x5CSigma_c.png"> is different for each class.
Induces quadratic decision boundaries.
<tr>
<td> LDA
<td>  <img src="x0x5CSigma_c0x3D0x5CSigma.png"> is the same (tied) for each class.
Induces linear decision boundaries.
<tr>
<td> DDA
<td> <img src="x0x5CSigma_c.png"> is diagonal, so the features are conditionally
independent; this is an example of a naive Bayes classifier.
Induces linear decision boundaries.
<tr>
<td> RDA
<td> Regularized LDA; uses MAP estimation for <img src="x0x5CSigma.png">.
<tr>
<td> shrunkenCentroids
<td> Diagonal LDA with L1 shrinkage on offsets (see below)
</table>
</p><p>We give more details below.</p><h2>QDA/ LDA/ NBC<a name="11"></a></h2><p>Below we give an example (from <a href="http://pmtk3.googlecode.com/svn/trunk/demos/otherDemos/supervisedModels/discrimAnalysisFisherIrisDemo.m">discrimAnalysisFisherIrisDemo.m</a> ) of how to fit a QDA/LDA/ diagDA model. We apply it to a subset of the Fisher Iris dataset.</p><pre class="codeinput">loadData(<span class="string">'fisherIrisData'</span>)
X = meas(51:end, 1:2);  <span class="comment">% for illustrations use 2 species, 2 features</span>
labels = species(51:end);
[y, support] = canonizeLabels(labels);
types = {<span class="string">'quadratic'</span>, <span class="string">'linear'</span>, <span class="string">'diag'</span>};
<span class="keyword">for</span> tt=1:length(types)
  model = discrimAnalysisFit(X, y, types{tt});
  h = plotDecisionBoundary(X, y, @(Xtest)discrimAnalysisPredict(model, Xtest));
  title(sprintf(<span class="string">'Discrim. analysis of type %s'</span>, types{tt}));
  <span class="keyword">if</span> ~isOctave
    legend(h, support, <span class="string">'Location'</span>, <span class="string">'NorthWest'</span>);
    set(gca, <span class="string">'Xtick'</span>, 5:8, <span class="string">'Ytick'</span>, 2:0.5:4);
  <span class="keyword">end</span>
  xlabel(<span class="string">'X_1'</span>); ylabel(<span class="string">'X_2'</span>);
<span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="tutGenClassif_01.png" alt=""> <img vspace="5" hspace="5" src="tutGenClassif_02.png" alt=""> <img vspace="5" hspace="5" src="tutGenClassif_03.png" alt=""> <h2>Regularized discriminant analysis<a name="14"></a></h2><p>When fitting a discriminant analysis model we will encounter numerical problems when estimating <img src="tutGenClassif_eq53484.png" alt="$\Sigma$"> when N &lt; D, even if we use a tied  covariance matrix (i.e., one shared across classes, a method known as linear discriminant analysis). A simple solution is to use a Wishart prior to compute a MAP estimate of <img src="tutGenClassif_eq53484.png" alt="$\Sigma$">. This is called regularized discriminant analysis, and can be fit using <tt>discrimAnalysisFit(X, y, 'rda', lambda)</tt>, where <tt>lambda</tt> controls the amount of regularization. See <a href="http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Generative_models_for_classification_and_regression/cancerHighDimClassifDemo.m">cancerHighDimClassifDemo.m</a> for an example, which reproduces table 18.1 from <a href="http://www-stat.stanford.edu/~tibs/ElemStatLearn/">Elements of statistical learning</a> 2nd edn p656. (We don't run this demo here since it requires computing the SVD of Xtrain (which has size 144* 16063, with 14 classes) which takes more seconds than we are willing to wait (about 40 sec)).</p><h2>Nearest shrunken centroid<a name="15"></a></h2><p>Consider a naive Bayes model in which the diagonal covariance is tied. This has O(D) parameters for the covariance, but O(C D) for the mean. To prevent overfitting, we can shrink the class-conditional means towards the overall mean; this technique is called nearest shrunken centroids. We can fit this model using <tt>discrimAnalysisFit(X, y, 'shrunkenCentroids', lambda)</tt>. We given an example of this below (from  <a href="http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Generative_models_for_classification_and_regression/shrunkenCentroidsSRBCTdemo.m">shrunkenCentroidsSRBCTdemo.m</a> ), where we apply the method to the SRBCT gene microarray dataset, whose training set has size 63*2308 with  C=4 classes. This roughly reproduces figure 18.4 from <a href="http://www-stat.stanford.edu/~tibs/ElemStatLearn/">Elements of statistical learning</a> 2nd edn p656.</p><pre class="codeinput">close <span class="string">all</span>; clear <span class="string">all</span>;
loadData(<span class="string">'srbct'</span>);

Xtest = Xtest(~isnan(ytest), :);
ytest = ytest(~isnan(ytest));

fitFn = @(X,y,lam)  discrimAnalysisFit(X, y, <span class="string">'shrunkenCentroids'</span>, <span class="string">'lambda'</span>,lam);
predictFn = @(model, X)  discrimAnalysisPredict(model, X);

lambdas = linspace(0, 8, 20);
nTrain = length(ytrain);
nTest = length(ytest);
<span class="keyword">for</span> i=1:length(lambdas)
    model = fitFn(Xtrain, ytrain, lambdas(i));
    yhatTrain = predictFn(model, Xtrain);
    yhatTest = predictFn(model, Xtest);
    errTrain(i) = sum(zeroOneLossFn(yhatTrain, ytrain))/nTrain;
    errTest(i) = sum(zeroOneLossFn(yhatTest, ytest))/nTest;
    numgenes(i) = sum(model.shrunkenCentroids(:) ~= 0);
<span class="keyword">end</span>

figure;
plot(lambdas, errTrain, <span class="string">'gx-'</span>, lambdas, errTest, <span class="string">'bo--'</span>,<span class="keyword">...</span>
  <span class="string">'MarkerSize'</span>, 10, <span class="string">'linewidth'</span>, 2)
legend(<span class="string">'Training'</span>, <span class="string">'Test'</span>, <span class="string">'Location'</span>, <span class="string">'northwest'</span>);
xlabel(<span class="string">'Amount of shrinkage'</span>)
ylabel(<span class="string">'misclassification rate'</span>)
title(<span class="string">'SRBCT data'</span>)
</pre><img vspace="5" hspace="5" src="tutGenClassif_04.png" alt=""> <p>We can also visualize the MAP (blue) and ML (gray) estimate of the means for each class.</p><pre class="codeinput">bestModel = fitFn(Xtrain, ytrain, 4);
centShrunk = bestModel.shrunkenCentroids;
model = fitFn(Xtrain, ytrain, 0);
centUnshrunk = model.shrunkenCentroids;

[numGroups D] = size(centShrunk);
<span class="keyword">for</span> g=1:3 <span class="comment">% numGroups</span>
    <span class="comment">%subplot(4,1,g);</span>
    figure; hold <span class="string">on</span>;
    plot(1:D, centUnshrunk(g,:), <span class="string">'Color'</span>, [.8 .8 .8]);
    plot(1:D, centShrunk(g,:), <span class="string">'b'</span>, <span class="string">'LineWidth'</span>, 2);
    title(sprintf(<span class="string">'Class %d'</span>, g));
    hold <span class="string">off</span>;
    printPmtkFigure(sprintf(<span class="string">'shrunkenCentroidsClass%d'</span>, g))
<span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="tutGenClassif_05.png" alt=""> <img vspace="5" hspace="5" src="tutGenClassif_06.png" alt=""> <img vspace="5" hspace="5" src="tutGenClassif_07.png" alt=""> <h2>Robust discriminant analysis<a name="19"></a></h2><p>We can use any joint probability model for the class conditional density in a generative classifier. To train we just call <tt>generativeClassifierFit(fitFn, X, y)</tt> where <tt>fitFn</tt> fits <img src="tutGenClassif_eq65669.png" alt="$p(x|y=c,\theta)$">. To predict we just call <tt>[yhat, post] = generativeClassifierPredict(logprobFn, model, Xtest)</tt>, where <tt>logprobFn</tt> computes <img src="tutGenClassif_eq65669.png" alt="$p(x|y=c,\theta)$">.</p><p>As a simple example, we can make each class conditional density be a Student distribution, to implement robust discriminant analysis. Here is part of <a href="http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Generative_models_for_classification_and_regression/robustDiscrimAnalysisBankruptcyDemo.m">robustDiscrimAnalysisBankruptcyDemo.m</a> which illustrates the syntax:</p><p><tt>modelS = generativeClassifierFit(@studentFit, Xtrain, ytrain)</tt> <tt>[yhat] = generativeClassifierPredict(@studentLogprob, modelS, Xtest)</tt></p><p>Now we run the entire demo, which uses Student and Gaussian class conditional densities, as well as the QDA code. We see that the Student distribution is more robust than the Gaussian, and that the QDA code gives the same results as using a Gaussian model inside the generative classifier code, as it should</p><pre class="codeinput">robustDiscrimAnalysisBankruptcyDemo
</pre><pre class="codeoutput">Num Errors using Student: 2
Num Errors using Gaussian: 3
Num Errors using QDA: 3
</pre><img vspace="5" hspace="5" src="tutGenClassif_08.png" alt=""> <img vspace="5" hspace="5" src="tutGenClassif_09.png" alt=""> <img vspace="5" hspace="5" src="tutGenClassif_10.png" alt=""> <h2>Using HMMs as class conditional densities<a name="22"></a></h2><p>As a more interesting example, consider the problem of classifying time series, such as spoken digits. Since each data vector has variable length, it is natural to use a Markov or hidden Markov model for the class conditional densities. (HMMs are discussed in more detail <a href="http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutLVM.html">tutLVM.html</a> .) For real-valued data, a linear-Gaussian Markov model is not expressive enough, but an HMM with Gaussian emissions is quite flexible.</p><p>Suppose we have two sequences, corresponding to the spoken words "four" and "five". We can train and test the model using the code below (from <a href="http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Generative_models_for_classification_and_regression/isolatedWordClassificationWithHmmsDemo.m">isolatedWordClassificationWithHmmsDemo.m</a> ). It fits two HMMs, one per class.</p><pre class="codeinput">loadData(<span class="string">'data45'</span>);
<span class="comment">% Xtrain{i} is a 13 x T(i) sequence of MFCC data, where T(i) is the length</span>
nstates = 5;
setSeed(0);
Xtrain = [train4'; train5'];
ytrain = [repmat(4, numel(train4), 1) ; repmat(5, numel(train5), 1)];
[Xtrain, ytrain] = shuffleRows(Xtrain, ytrain);
Xtest = test45';
ytest = labels';
[Xtest, ytest] = shuffleRows(Xtest, ytest);
<span class="comment">% Initial Guess for params</span>
pi0 = [1, 0, 0, 0, 0];
transmat0 = normalize(diag(ones(nstates, 1)) + <span class="keyword">...</span>
            diag(ones(nstates-1, 1), 1), 2);
<span class="comment">% Fit</span>
fitArgs = {<span class="string">'pi0'</span>, pi0, <span class="string">'trans0'</span>, transmat0, <span class="string">'maxIter'</span>, 10, <span class="string">'verbose'</span>, true};
fitFn   = @(X)hmmFit(X, nstates, <span class="string">'gauss'</span>, fitArgs{:});
model = generativeClassifierFit(fitFn, Xtrain, ytrain);
<span class="comment">% Predict</span>
logprobFn = @hmmLogprob;
[yhat, post] = generativeClassifierPredict(logprobFn, model, Xtest);
nerrors = sum(yhat ~= ytest)
</pre><pre class="codeoutput">initializing model for EM
1	 loglik: -136774
2	 loglik: -80469.3
3	 loglik: -75503.9
4	 loglik: -73980.4
5	 loglik: -73064.5
6	 loglik: -72316.7
7	 loglik: -71873.3
8	 loglik: -71498.2
9	 loglik: -71272.6
10	 loglik: -71173.4
11	 loglik: -71122.6
initializing model for EM
1	 loglik: -150882
2	 loglik: -99493.4
3	 loglik: -89818
4	 loglik: -88251.6
5	 loglik: -88045.9
6	 loglik: -87758.7
7	 loglik: -87518.5
8	 loglik: -87350
9	 loglik: -87241.1
10	 loglik: -87097.6
11	 loglik: -86920.6
nerrors =
     0
</pre><h2>K-nearest neighbor classifier<a name="25"></a></h2><p>One can view a KNN classifier as a generative classifier where the class conditional density is a non-parametric kernel density estimator. Below we give an example where we apply a 1-NN classifier to a subset of the MNIST digit set (from <a href="http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Introduction/mnistKNNdemo.m">mnistKNNdemo.m</a> : see <a href="http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Introduction/mnist1NNdemo.m">mnist1NNdemo.m</a> for special-purpose code that can handle the full dataset).</p><pre class="codeinput">loadData(<span class="string">'mnistAll'</span>);
trainndx = 1:10000;
testndx =  1:1000;
ntrain = length(trainndx);
ntest = length(testndx);
Xtrain = double(reshape(mnist.train_images(:,:,trainndx),28*28,ntrain)');
Xtest  = double(reshape(mnist.test_images(:,:,testndx),28*28,ntest)');
ytrain = (mnist.train_labels(trainndx));
ytest  = (mnist.test_labels(testndx));
clear <span class="string">mnist</span> <span class="string">trainndx</span> <span class="string">testndx</span>; <span class="comment">% save space</span>

m = knnFit(Xtrain, ytrain, 1);
ypred = knnPredict(m, Xtest);
errorRate = mean(ypred ~= ytest);
fprintf(<span class="string">'Error Rate: %.2f%%\n'</span>,100*errorRate);
</pre><pre class="codeoutput">Error Rate: 8.00%
</pre><p>Below are the test error rates and running times (train + test) for 1NN on different sizes of training and test data (generated using <a href="http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Introduction/mnist1NNdemo.m">mnist1NNdemo.m</a>  ). Note that the standard training set is 60k and the standard test set is 10k. Reassuringly, our error rate of 3.09% for 1NN on this standard train/ test split is the same as that reported  by Kenneth Wilder at <a href="http://yann.lecun.com/exdb/mnist/">this league table</a>.</p><p>
<table border=1>
<TR ALIGN=left>
<td> Ntrain
<td> Ntest
<td> Error rate
<td> Time
<tr>
<td> 60k
<td> 10k
<td> 3.09%
<td> 255s
<tr>
<td> 60k
<td> 1k
<td> 3.80%
<td> 8s
<tr>
<td>  10k
<td> 1k
<td> 8.00%
<td> 1.39s
</table>
</p><p>From this, we see that increasing the size of the training set dramatically reduces the error rate (perhaps a symptom of overfitting?). Also, increasing the size of the test set dramatically increases the cost of testing (due to the need to loop over mini-batches of examples, to save memory).</p><p>
<hr>
</p><p>This page was auto-generated by calling <i>pmtkPublish(tutGenClassif)</i>  on 08-Sep-2010 17:28:54</p><p class="footer"><br>
      Published with MATLAB&reg; 7.9<br></p></div><!--
##### SOURCE BEGIN #####
%% Supervised learning using generative models in pmtk3
%
% Generative models for classification/ regression are joint models of the
% outputs and inputs
% of the form $p(y,x|\theta)$.
% We consider various examples below.
%
%% Naive Bayes classifier
% We now a simple kind of generative classifier
% called naive Bayes, which is a model of the form
%%
% $$p(y,x|\theta) = p(y|\pi) \prod_{j=1}^D p(x_j|y,\theta)$$
%%
% We can fit and predict with this model using
% <http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/naiveBayes/naiveBayesFit.m naiveBayesFit.m> and  <http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/naiveBayes/naiveBayesPredict.m naiveBayesPredict.m>
% For simplicity, the current implementation
% assumes  all the features are binary,
% so $p(x_j|y=c,\theta) = Ber(x_j|\theta_{jc})$.
% It fits by MAP estimation
% with a vague Dirichlet prior (add-one-smoothing).
% Typically results are not too sensitive to the
% setting of this prior (unlike discriminative models).
%
% Below is an example (from <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Generative_models_for_classification_and_regression/naiveBayesBowDemo.m naiveBayesBowDemo.m> )
% which fits a model to some bag of words data,
% and then classifies a test set.
%%
loadData('XwindowsDocData'); % 900x600, 2 classes Xwindows vs MSwindows
Xtrain = xtrain; Xtest = xtest;
model = naiveBayesFit(Xtrain, ytrain);
ypred_train = naiveBayesPredict(model, Xtrain);
err_train = mean(zeroOneLossFn(ytrain, ypred_train));
ypred_test = naiveBayesPredict(model, Xtest);
err_test = mean(zeroOneLossFn(ytest, ypred_test));
fprintf('misclassification rates  on train = %5.2f pc, on test = %5.2f pc\n', ...
    err_train*100, err_test*100);
%%
% See also <http://pmtk3.googlecode.com/svn/trunk/demos/otherDemos/supervisedModels/naiveBayesMnistDemo.m naiveBayesMnistDemo.m> for application of NBC
% to classify binarized MNIST digits.
%
% It is simple to modify NBC to handle missing data in X
% at training and test time; this is left as an exercise
% to the reader.

%% Discriminant analysis
% Discriminant analysis is a generative classifier where
% the class conditional density  is a multivariate Gaussian:
%%
% $$p(y=c,x|\theta) = \mbox{discrete}(y|\pi) N(x|\mu_c,\Sigma_c)$$
%%
% PMTK supports the following variants of this model:
%%
% <html>
% <table border=1>
% <TR ALIGN=left>
% <td> Type
% <td> Description
% <tr>
% <td> QDA
% <td> <img src="x0x5CSigma_c.png"> is different for each class.
% Induces quadratic decision boundaries.
% <tr>
% <td> LDA 
% <td>  <img src="x0x5CSigma_c0x3D0x5CSigma.png"> is the same (tied) for each class.
% Induces linear decision boundaries.
% <tr>
% <td> DDA
% <td> <img src="x0x5CSigma_c.png"> is diagonal, so the features are conditionally
% independent; this is an example of a naive Bayes classifier.
% Induces linear decision boundaries.
% <tr>
% <td> RDA
% <td> Regularized LDA; uses MAP estimation for <img src="x0x5CSigma.png">.
% <tr>
% <td> shrunkenCentroids
% <td> Diagonal LDA with L1 shrinkage on offsets (see below)
% </table>
% </html>
%%
% We give more details below.

%% QDA/ LDA/ NBC
% Below we give an example (from <http://pmtk3.googlecode.com/svn/trunk/demos/otherDemos/supervisedModels/discrimAnalysisFisherIrisDemo.m discrimAnalysisFisherIrisDemo.m> )
% of how to fit a QDA/LDA/ diagDA model.
% We apply it to a subset of the Fisher Iris dataset.
%%
loadData('fisherIrisData')
X = meas(51:end, 1:2);  % for illustrations use 2 species, 2 features
labels = species(51:end);
[y, support] = canonizeLabels(labels);
types = {'quadratic', 'linear', 'diag'};
for tt=1:length(types)
  model = discrimAnalysisFit(X, y, types{tt});
  h = plotDecisionBoundary(X, y, @(Xtest)discrimAnalysisPredict(model, Xtest));
  title(sprintf('Discrim. analysis of type %s', types{tt}));
  if ~isOctave
    legend(h, support, 'Location', 'NorthWest');
    set(gca, 'Xtick', 5:8, 'Ytick', 2:0.5:4);
  end
  xlabel('X_1'); ylabel('X_2');
end
%%

%% Regularized discriminant analysis
% When fitting a discriminant analysis model 
% we will encounter numerical problems
% when estimating $\Sigma$ when N < D, even if we use
% a tied  covariance matrix (i.e., one shared across classes, a method
% known as linear discriminant analysis).
% A simple solution is to use a Wishart prior to compute a MAP
% estimate of $\Sigma$. This is called regularized discriminant analysis,
% and can be fit using |discrimAnalysisFit(X, y, 'rda', lambda)|,
% where |lambda| controls the amount of regularization.
% See <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Generative_models_for_classification_and_regression/cancerHighDimClassifDemo.m cancerHighDimClassifDemo.m>
% for an example, which reproduces
% table 18.1 from
% <http://www-stat.stanford.edu/~tibs/ElemStatLearn/ Elements of statistical learning>
% 2nd edn p656.
% (We don't run this demo here since it requires computing
% the SVD of Xtrain (which has size 144* 16063, with 14 classes)
% which takes
% more seconds than we are willing to wait (about 40 sec)).
%

%% Nearest shrunken centroid
% Consider a naive Bayes model in which the diagonal covariance
% is tied. This has O(D) parameters for the covariance, but O(C D) for the mean.
% To prevent overfitting, we can shrink the class-conditional means towards
% the overall mean; this technique is called nearest shrunken centroids. We
% can fit this model using |discrimAnalysisFit(X, y, 'shrunkenCentroids',
% lambda)|. We given an example of this below (from
%  <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Generative_models_for_classification_and_regression/shrunkenCentroidsSRBCTdemo.m shrunkenCentroidsSRBCTdemo.m> ),
% where we apply the method to
% the SRBCT gene microarray dataset, whose training set
% has size 63*2308 with  C=4 classes.
% This roughly reproduces figure 18.4 from
% <http://www-stat.stanford.edu/~tibs/ElemStatLearn/ Elements of statistical learning>
% 2nd edn p656.
%%
close all; clear all;
loadData('srbct');

Xtest = Xtest(~isnan(ytest), :);
ytest = ytest(~isnan(ytest));

fitFn = @(X,y,lam)  discrimAnalysisFit(X, y, 'shrunkenCentroids', 'lambda',lam);
predictFn = @(model, X)  discrimAnalysisPredict(model, X);

lambdas = linspace(0, 8, 20);
nTrain = length(ytrain);
nTest = length(ytest);
for i=1:length(lambdas)
    model = fitFn(Xtrain, ytrain, lambdas(i));
    yhatTrain = predictFn(model, Xtrain);
    yhatTest = predictFn(model, Xtest);
    errTrain(i) = sum(zeroOneLossFn(yhatTrain, ytrain))/nTrain;
    errTest(i) = sum(zeroOneLossFn(yhatTest, ytest))/nTest;
    numgenes(i) = sum(model.shrunkenCentroids(:) ~= 0);
end

figure;
plot(lambdas, errTrain, 'gx-', lambdas, errTest, 'boREPLACE_WITH_DASH_DASH',...
  'MarkerSize', 10, 'linewidth', 2)
legend('Training', 'Test', 'Location', 'northwest');
xlabel('Amount of shrinkage')
ylabel('misclassification rate')
title('SRBCT data')
%%
% We can also visualize the MAP (blue) and ML (gray) estimate of the means
% for each class.
%% 
bestModel = fitFn(Xtrain, ytrain, 4);
centShrunk = bestModel.shrunkenCentroids;
model = fitFn(Xtrain, ytrain, 0);
centUnshrunk = model.shrunkenCentroids;

[numGroups D] = size(centShrunk);
for g=1:3 % numGroups
    %subplot(4,1,g);
    figure; hold on;
    plot(1:D, centUnshrunk(g,:), 'Color', [.8 .8 .8]);
    plot(1:D, centShrunk(g,:), 'b', 'LineWidth', 2);
    title(sprintf('Class %d', g));
    hold off;
    printPmtkFigure(sprintf('shrunkenCentroidsClass%d', g))
end


%% Robust discriminant analysis
% We can use any joint probability model for the class conditional
% density in a generative classifier.
% To train we just call |generativeClassifierFit(fitFn, X, y)|
% where |fitFn| fits $p(x|y=c,\theta)$.
% To predict we just call 
% |[yhat, post] = generativeClassifierPredict(logprobFn, model, Xtest)|,
% where |logprobFn| computes $p(x|y=c,\theta)$.
%
% As a simple example, we can make each class conditional density
% be a Student distribution, to implement robust discriminant
% analysis. Here is part of <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Generative_models_for_classification_and_regression/robustDiscrimAnalysisBankruptcyDemo.m robustDiscrimAnalysisBankruptcyDemo.m> 
% which illustrates the syntax:
%
% |modelS = generativeClassifierFit(@studentFit, Xtrain, ytrain)|
% |[yhat] = generativeClassifierPredict(@studentLogprob, modelS, Xtest)|
%
% Now we run the entire demo, which uses Student and Gaussian
% class conditional densities, as well as the QDA code.
% We see that the Student distribution is more robust
% than the Gaussian, and that the QDA code gives the same
% results as using a Gaussian model inside the generative
% classifier code, as it should
%%
robustDiscrimAnalysisBankruptcyDemo
%%

%% Using HMMs as class conditional densities
% As a more interesting example, consider the problem of classifying time series,
% such as spoken digits.
% Since each data vector has variable length, it is natural
% to use a Markov or hidden Markov model for the class conditional
% densities.
% (HMMs are discussed in more detail 
% <http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutLVM.html tutLVM.html> .)
% For real-valued data, a linear-Gaussian Markov model is not
% expressive enough, but an HMM with Gaussian emissions
% is quite flexible.
%
% Suppose we have two sequences, corresponding to the spoken words
% "four" and "five". We can train and test the model
% using the code below
% (from <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Generative_models_for_classification_and_regression/isolatedWordClassificationWithHmmsDemo.m isolatedWordClassificationWithHmmsDemo.m> ).
% It fits two HMMs, one per class.
%%
loadData('data45'); 
% Xtrain{i} is a 13 x T(i) sequence of MFCC data, where T(i) is the length
nstates = 5;
setSeed(0); 
Xtrain = [train4'; train5'];
ytrain = [repmat(4, numel(train4), 1) ; repmat(5, numel(train5), 1)];
[Xtrain, ytrain] = shuffleRows(Xtrain, ytrain);
Xtest = test45'; 
ytest = labels'; 
[Xtest, ytest] = shuffleRows(Xtest, ytest); 
% Initial Guess for params
pi0 = [1, 0, 0, 0, 0];
transmat0 = normalize(diag(ones(nstates, 1)) + ...
            diag(ones(nstates-1, 1), 1), 2);
% Fit
fitArgs = {'pi0', pi0, 'trans0', transmat0, 'maxIter', 10, 'verbose', true};
fitFn   = @(X)hmmFit(X, nstates, 'gauss', fitArgs{:}); 
model = generativeClassifierFit(fitFn, Xtrain, ytrain); 
% Predict
logprobFn = @hmmLogprob;
[yhat, post] = generativeClassifierPredict(logprobFn, model, Xtest);
nerrors = sum(yhat ~= ytest)
%%

%% K-nearest neighbor classifier
% One can view a KNN classifier as a generative
% classifier where the class conditional density is a non-parametric
% kernel density estimator.
% Below we give an example where we apply
% a 1-NN classifier to a subset of the MNIST digit set
% (from <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Introduction/mnistKNNdemo.m mnistKNNdemo.m> : see <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Introduction/mnist1NNdemo.m mnist1NNdemo.m> for special-purpose
% code that can handle the full dataset).
%%
loadData('mnistAll');
trainndx = 1:10000;
testndx =  1:1000;
ntrain = length(trainndx);
ntest = length(testndx);
Xtrain = double(reshape(mnist.train_images(:,:,trainndx),28*28,ntrain)');
Xtest  = double(reshape(mnist.test_images(:,:,testndx),28*28,ntest)');
ytrain = (mnist.train_labels(trainndx));
ytest  = (mnist.test_labels(testndx));
clear mnist trainndx testndx; % save space

m = knnFit(Xtrain, ytrain, 1);
ypred = knnPredict(m, Xtest);
errorRate = mean(ypred ~= ytest);
fprintf('Error Rate: %.2f%%\n',100*errorRate);
%%
% Below are the test error rates and running times (train + test)
% for 1NN on different sizes of training and test data
% (generated using <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Introduction/mnist1NNdemo.m mnist1NNdemo.m>  ).
% Note that the standard training set is 60k 
% and the standard test set is 10k.
% Reassuringly, our error rate of 3.09% for 1NN on this standard
% train/ test split is the same as that
% reported  by Kenneth Wilder at
% <http://yann.lecun.com/exdb/mnist/ this league table>.
%%
% <html>
% <table border=1>
% <TR ALIGN=left>
% <td> Ntrain
% <td> Ntest
% <td> Error rate
% <td> Time
% <tr>
% <td> 60k
% <td> 10k
% <td> 3.09%
% <td> 255s
% <tr>
% <td> 60k
% <td> 1k
% <td> 3.80%
% <td> 8s
% <tr>
% <td>  10k
% <td> 1k
% <td> 8.00%
% <td> 1.39s
% </table>
% </html>
%%
% From this, we see that increasing the size of the training
% set dramatically reduces the error rate (perhaps a symptom of
% overfitting?). Also, increasing the size of the test set
% dramatically increases the cost of testing (due to the need
% to loop over mini-batches of examples, to save memory).

%%
% <html>
% <hr>
% </html>
%%

%%
% This page was auto-generated by calling _pmtkPublish(tutGenClassif)_  on 08-Sep-2010 17:28:54


##### SOURCE END #####
--></body></html>