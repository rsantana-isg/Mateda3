
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Reproduce figures 12.2, 12.3 and 12.5 in Hastie ESL 2nd ed.</title><meta name="generator" content="MATLAB 7.12"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2012-03-27"><meta name="DC.source" content="hastieSvmLrDemo.m"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h1>Reproduce figures 12.2, 12.3 and 12.5 in Hastie ESL 2nd ed.</h1><!--introduction--><p>PMTKneedsOptimToolbox</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Generating Models</a></li><li><a href="#2">Create data</a></li><li><a href="#3">Create the generative model to plot the Bayesian decision boundary</a></li><li><a href="#5">Plot the Bayesian decision boundary</a></li><li><a href="#7">Fit the SVM</a></li><li><a href="#8">Plot the SVM decision boundary and the +-1 margins</a></li><li><a href="#9">Find support vectors on margin</a></li><li><a href="#11">Fit LR</a></li><li><a href="#12">Plot the LR decision boundary and the 25%/75% margins</a></li><li><a href="#14">Annotate</a></li></ul></div><h2>Generating Models<a name="1"></a></h2><p>Generate data as per Hastie 2ed, pg 16: two classes, blue and orange, each with a mixture of 10, 2D Gaussians.</p><pre class="codeinput"><span class="comment">%PMTKslow</span>

<span class="comment">% This file is from pmtk3.googlecode.com</span>

requireOptimToolbox
setSeed(42);
ncenters = 10;
d = 2;
<span class="comment">% if false, we use Hastie's exact means/data from his website</span>
regenerateMeans = false;
regenerateData  = false;

blueMix.Sigma     = repmat(eye(d)./5, [1, 1, ncenters]);
blueMix.mixweight = normalize(ones(1, ncenters));
blueMix.K         = ncenters;
orangeMix         = blueMix;  <span class="comment">% same model except for mixture means</span>

data = loadHastieMixtureData();
<span class="keyword">if</span> regenerateMeans
    blueMeanSource.mu      = [1 0];
    blueMeanSource.Sigma   = eye(d);
    orangeMeanSource.mu    = [0 1];
    orangeMeanSource.Sigma = eye(d);
    means = [gaussSample(blueMeanSource , ncenters);
            gaussSample(orangeMeanSource, ncenters)];
<span class="keyword">else</span>
    means = reshape(data.means, [20, 2]);
<span class="keyword">end</span>
blueMix.mu   = means(1:10,   :)';
orangeMix.mu = means(11:end, :)';
</pre><h2>Create data<a name="2"></a></h2><pre class="codeinput"><span class="keyword">if</span> regenerateData
    ntrain = 100; <span class="comment">% per class</span>
    Xtrain = [mixGaussSample(blueMix.mu, blueMix.Sigma, blueMix.mixweight, ntrain);
              mixGaussSample(orangeMix.mu, orangeMix.Sigma, orangeMix.mixweight, ntrain)];
    ytrain = [-1*ones(ntrain, 1); ones(ntrain, 1)];
    [Xtrain, ytrain] = shuffleRows(Xtrain, ytrain);
<span class="keyword">else</span>
    Xtrain = data.X;
    ytrain = convertLabelsToPM1(data.y);
    [Xtrain, ytrain] = shuffleRows(Xtrain, ytrain);
<span class="keyword">end</span>
ntest = 5000; <span class="comment">% per class</span>
Xtest = [mixGaussSample(blueMix.mu, blueMix.Sigma, blueMix.mixweight  , ntest);
         mixGaussSample(orangeMix.mu, orangeMix.Sigma, orangeMix.mixweight, ntest)];
ytest = [-1*ones(ntest, 1); ones(ntest, 1)];
[Xtest, ytest] = shuffleRows(Xtest, ytest);
</pre><h2>Create the generative model to plot the Bayesian decision boundary<a name="3"></a></h2><pre class="codeinput">blueFullModel = mixModelCreate(condGaussCpdCreate(blueMix.mu, blueMix.Sigma),<span class="keyword">...</span>
    <span class="string">'gauss'</span>, numel(blueMix.mixweight), blueMix.mixweight);
orangeFullModel = mixModelCreate(condGaussCpdCreate(orangeMix.mu, orangeMix.Sigma),<span class="keyword">...</span>
    <span class="string">'gauss'</span>, numel(orangeMix.mixweight), orangeMix.mixweight);
genmodel.nclasses = 2;
genmodel.classConditionals = {blueFullModel, orangeFullModel};
genmodel.support = [-1 1];
prior.T = [0.5; 0.5];
prior.K = 2;
prior.d = 1;
genmodel.prior = prior;
bayesPredictFn = @(X)generativeClassifierPredict<span class="keyword">...</span>
                  (@mixModelLogprob, genmodel, X);
bayesError = mean(bayesPredictFn(Xtest) ~= ytest);
</pre><pre class="codeoutput">Undefined function or method 'mixModelCreate' for input arguments of type 'struct'.

Error in ==&gt; hastieSvmLrDemo at 54
blueFullModel = mixModelCreate(condGaussCpdCreate(blueMix.mu, blueMix.Sigma),...
</pre><pre class="codeinput">Cvalues = [10000, 0.1];
nc      = numel(Cvalues);
kernel = {@kernelLinear, @kernelPoly, @kernelRbfGamma};
kernelArgs = {{},{<span class="string">'kernelParam'</span>, 4},{<span class="string">'kernelParam'</span>, 1}};

purple       = [147, 23, 147]./255;
contourProps = {<span class="string">'--'</span>, <span class="string">'linewidth'</span>, 1.5, <span class="string">'linecolor'</span>, purple};

titleStr     = {  {<span class="string">'SVM Linear Kernel'</span>
                   <span class="string">'SVM Polynomial Kernel (degree 4)'</span>
                   sprintf(<span class="string">'SVM RBF Kernel (%s = 1)'</span>, <span class="string">'\gamma'</span>)
                  }
                  {<span class="string">'LR Linear Kernel'</span>
                   <span class="string">'LR Polynomial Kernel (degree 4)'</span>
                    sprintf(<span class="string">' LR RBF Kernel (%s = 1)'</span>, <span class="string">'\gamma'</span>)
                  }
               };
penalties = {<span class="string">'svm'</span>, <span class="string">'logreg'</span>};
<span class="keyword">for</span> k=1:numel(penalties)
    penalty = penalties{k};

<span class="keyword">for</span> j=1:numel(kernel)
    <span class="keyword">if</span> strcmp(penalty, <span class="string">'logreg'</span>) &amp;&amp; j == 1, <span class="keyword">continue</span>; <span class="keyword">end</span>
    <span class="keyword">if</span> j &gt; 1,
        Cvalues = 1; <span class="comment">% Use C = 1 for both poly and rbf kernels</span>
        nc = 1;
    <span class="keyword">end</span>
    <span class="keyword">for</span> i=1:nc
</pre><h2>Plot the Bayesian decision boundary<a name="5"></a></h2><pre class="codeinput">        plotDecisionBoundary(Xtrain, ytrain, bayesPredictFn,<span class="keyword">...</span>
            <span class="string">'contourProps'</span>   , contourProps                ,<span class="keyword">...</span>
            <span class="string">'markerLineWidth'</span>, 1.5);
        hold <span class="string">on</span>;
</pre><pre class="codeinput">        <span class="keyword">switch</span> penalty
            <span class="keyword">case</span> <span class="string">'svm'</span>
</pre><h2>Fit the SVM<a name="7"></a></h2><pre class="codeinput">                svmModel = svmFit(Xtrain, ytrain, <span class="string">'C'</span>, Cvalues(i),<span class="keyword">...</span>
                 <span class="string">'kernel'</span>, kernel{j}, kernelArgs{j}{:}, <span class="keyword">...</span>
                 <span class="string">'fitFn'</span> , @svmQPclassifFit);

                [yhatTrain, ftrain] = svmPredict(svmModel, Xtrain);
                trainError = mean(ytrain ~= yhatTrain);
                [yhatTest, ftest] = svmPredict(svmModel, Xtest);
                testError = mean(ytest ~= yhatTest);
</pre><h2>Plot the SVM decision boundary and the +-1 margins<a name="8"></a></h2><p>These are contours of the function values f:</p><pre class="codeinput">                plotContour(@(x)argout(2, @svmPredict, svmModel, x), <span class="keyword">...</span>
                    axis(), [0 0], <span class="string">'-k'</span>, <span class="string">'linewidth'</span>, 1.5);

                [h,p,c] = plotContour(@(x)argout(2, @svmPredict, svmModel, x), <span class="keyword">...</span>
                    axis(), [-1 1], <span class="string">'--k'</span>, <span class="string">'linewidth'</span>, 1);
                t = sprintf(<span class="string">'C = %g'</span>,  Cvalues(i));
</pre><h2>Find support vectors on margin<a name="9"></a></h2><pre class="codeinput">                SV = Xtrain(svmModel.svi, :);
                D = sqDistance(SV, c');
                S = SV(any(D &lt; 0.005, 2), :);
                plot(S(:, 1), S(:, 2), <span class="string">'.k'</span>, <span class="string">'markersize'</span>, 20);
</pre><pre class="codeinput">            <span class="keyword">case</span> <span class="string">'logreg'</span>
</pre><h2>Fit LR<a name="11"></a></h2><pre class="codeinput">                preproc.kernelFn = @(X1, X2)kernel{j}(X1, X2, kernelArgs{j}{2});
                lrModel = logregFit(Xtrain, ytrain, <span class="string">'regType'</span>, <span class="string">'l2'</span>,<span class="keyword">...</span>
                    <span class="string">'lambda'</span>, 1/Cvalues(i), <span class="string">'preproc'</span>, preproc);
                trainError = mean(ytrain~= logregPredict(lrModel, Xtrain));
                testError  = mean(ytest ~= logregPredict(lrModel, Xtest ));
</pre><h2>Plot the LR decision boundary and the 25%/75% margins<a name="12"></a></h2><pre class="codeinput">                plotContour(@(x)argout(2, @logregPredict, lrModel, x), <span class="keyword">...</span>
                    axis(), [0.5 0.5], <span class="string">'-k'</span>, <span class="string">'linewidth'</span>, 1.5);

                plotContour(@(x)argout(2, @logregPredict, lrModel, x), <span class="keyword">...</span>
                    axis(), [0.25 0.75], <span class="string">'--k'</span>, <span class="string">'linewidth'</span>, 1);
                t = sprintf(<span class="string">'%s = %g'</span>, <span class="string">'\lambda'</span>, 1/Cvalues(i));
</pre><pre class="codeinput">        <span class="keyword">end</span>
</pre><h2>Annotate<a name="14"></a></h2><pre class="codeinput">        title({titleStr{k}{j}; t});
        text = {sprintf(<span class="string">'Training Error: %.2f'</span>, trainError);
            sprintf(<span class="string">'Test Error:       %.2f'</span>, testError);
            sprintf(<span class="string">'Bayes Error:    %.2f'</span>, bayesError)};
        annotation(gcf,<span class="string">'textbox'</span> , [0.15 0.12 0.24 0.18], <span class="keyword">...</span>
            <span class="string">'String'</span>         , text                 , <span class="keyword">...</span>
            <span class="string">'BackgroundColor'</span>, [1 1 1]              , <span class="keyword">...</span>
            <span class="string">'FontWeight'</span>     , <span class="string">'demi'</span>               , <span class="keyword">...</span>
            <span class="string">'FitBoxToText'</span>   , <span class="string">'on'</span>                 , <span class="keyword">...</span>
            <span class="string">'LineStyle'</span>      , <span class="string">'none'</span>);
</pre><pre class="codeinput">    <span class="keyword">end</span>
<span class="keyword">end</span>
<span class="keyword">end</span>
</pre><p class="footer"><br>
      Published with MATLAB&reg; 7.12<br></p></div><!--
##### SOURCE BEGIN #####
%% Reproduce figures 12.2, 12.3 and 12.5 in Hastie ESL 2nd ed.
% PMTKneedsOptimToolbox
%% Generating Models
% Generate data as per Hastie 2ed, pg 16: two classes, blue and orange,
% each with a mixture of 10, 2D Gaussians.
%PMTKslow

% This file is from pmtk3.googlecode.com

requireOptimToolbox
setSeed(42);
ncenters = 10;
d = 2;
% if false, we use Hastie's exact means/data from his website
regenerateMeans = false;
regenerateData  = false;

blueMix.Sigma     = repmat(eye(d)./5, [1, 1, ncenters]);
blueMix.mixweight = normalize(ones(1, ncenters));
blueMix.K         = ncenters;
orangeMix         = blueMix;  % same model except for mixture means

data = loadHastieMixtureData();
if regenerateMeans
    blueMeanSource.mu      = [1 0];
    blueMeanSource.Sigma   = eye(d);
    orangeMeanSource.mu    = [0 1];
    orangeMeanSource.Sigma = eye(d);
    means = [gaussSample(blueMeanSource , ncenters);
            gaussSample(orangeMeanSource, ncenters)];
else
    means = reshape(data.means, [20, 2]);
end
blueMix.mu   = means(1:10,   :)';
orangeMix.mu = means(11:end, :)';
%% Create data
if regenerateData
    ntrain = 100; % per class
    Xtrain = [mixGaussSample(blueMix.mu, blueMix.Sigma, blueMix.mixweight, ntrain);
              mixGaussSample(orangeMix.mu, orangeMix.Sigma, orangeMix.mixweight, ntrain)];
    ytrain = [-1*ones(ntrain, 1); ones(ntrain, 1)];
    [Xtrain, ytrain] = shuffleRows(Xtrain, ytrain);
else
    Xtrain = data.X;
    ytrain = convertLabelsToPM1(data.y);
    [Xtrain, ytrain] = shuffleRows(Xtrain, ytrain);
end
ntest = 5000; % per class
Xtest = [mixGaussSample(blueMix.mu, blueMix.Sigma, blueMix.mixweight  , ntest);
         mixGaussSample(orangeMix.mu, orangeMix.Sigma, orangeMix.mixweight, ntest)];
ytest = [-1*ones(ntest, 1); ones(ntest, 1)];
[Xtest, ytest] = shuffleRows(Xtest, ytest);
%% Create the generative model to plot the Bayesian decision boundary
blueFullModel = mixModelCreate(condGaussCpdCreate(blueMix.mu, blueMix.Sigma),...
    'gauss', numel(blueMix.mixweight), blueMix.mixweight);
orangeFullModel = mixModelCreate(condGaussCpdCreate(orangeMix.mu, orangeMix.Sigma),...
    'gauss', numel(orangeMix.mixweight), orangeMix.mixweight);
genmodel.nclasses = 2;
genmodel.classConditionals = {blueFullModel, orangeFullModel};
genmodel.support = [-1 1];
prior.T = [0.5; 0.5];
prior.K = 2;
prior.d = 1;
genmodel.prior = prior;
bayesPredictFn = @(X)generativeClassifierPredict...
                  (@mixModelLogprob, genmodel, X);
bayesError = mean(bayesPredictFn(Xtest) ~= ytest);
%%
Cvalues = [10000, 0.1];
nc      = numel(Cvalues);
kernel = {@kernelLinear, @kernelPoly, @kernelRbfGamma};
kernelArgs = {{},{'kernelParam', 4},{'kernelParam', 1}};

purple       = [147, 23, 147]./255;
contourProps = {'REPLACE_WITH_DASH_DASH', 'linewidth', 1.5, 'linecolor', purple};

titleStr     = {  {'SVM Linear Kernel'
                   'SVM Polynomial Kernel (degree 4)'
                   sprintf('SVM RBF Kernel (%s = 1)', '\gamma')
                  }
                  {'LR Linear Kernel'
                   'LR Polynomial Kernel (degree 4)'
                    sprintf(' LR RBF Kernel (%s = 1)', '\gamma')
                  }
               };
penalties = {'svm', 'logreg'};
for k=1:numel(penalties) 
    penalty = penalties{k};
    
for j=1:numel(kernel)
    if strcmp(penalty, 'logreg') && j == 1, continue; end
    if j > 1, 
        Cvalues = 1; % Use C = 1 for both poly and rbf kernels
        nc = 1; 
    end
    for i=1:nc
        %% Plot the Bayesian decision boundary
        plotDecisionBoundary(Xtrain, ytrain, bayesPredictFn,...
            'contourProps'   , contourProps                ,...
            'markerLineWidth', 1.5);
        hold on;
        %%
        switch penalty
            case 'svm'
                %% Fit the SVM
                svmModel = svmFit(Xtrain, ytrain, 'C', Cvalues(i),...
                 'kernel', kernel{j}, kernelArgs{j}{:}, ...
                 'fitFn' , @svmQPclassifFit);

                [yhatTrain, ftrain] = svmPredict(svmModel, Xtrain);
                trainError = mean(ytrain ~= yhatTrain);
                [yhatTest, ftest] = svmPredict(svmModel, Xtest);
                testError = mean(ytest ~= yhatTest);
                %% Plot the SVM decision boundary and the +-1 margins
                % These are contours of the function values f:
                plotContour(@(x)argout(2, @svmPredict, svmModel, x), ...
                    axis(), [0 0], '-k', 'linewidth', 1.5);

                [h,p,c] = plotContour(@(x)argout(2, @svmPredict, svmModel, x), ...
                    axis(), [-1 1], 'REPLACE_WITH_DASH_DASHk', 'linewidth', 1);
                t = sprintf('C = %g',  Cvalues(i)); 
                %% Find support vectors on margin
                SV = Xtrain(svmModel.svi, :); 
                D = sqDistance(SV, c'); 
                S = SV(any(D < 0.005, 2), :);
                plot(S(:, 1), S(:, 2), '.k', 'markersize', 20); 
                
            case 'logreg'
                %% Fit LR
                preproc.kernelFn = @(X1, X2)kernel{j}(X1, X2, kernelArgs{j}{2});
                lrModel = logregFit(Xtrain, ytrain, 'regType', 'l2',...
                    'lambda', 1/Cvalues(i), 'preproc', preproc); 
                trainError = mean(ytrain~= logregPredict(lrModel, Xtrain));
                testError  = mean(ytest ~= logregPredict(lrModel, Xtest ));
                
                %% Plot the LR decision boundary and the 25%/75% margins
                plotContour(@(x)argout(2, @logregPredict, lrModel, x), ...
                    axis(), [0.5 0.5], '-k', 'linewidth', 1.5);

                plotContour(@(x)argout(2, @logregPredict, lrModel, x), ...
                    axis(), [0.25 0.75], 'REPLACE_WITH_DASH_DASHk', 'linewidth', 1);
                t = sprintf('%s = %g', '\lambda', 1/Cvalues(i)); 
        end
       %% Annotate
       
        title({titleStr{k}{j}; t});
        text = {sprintf('Training Error: %.2f', trainError);
            sprintf('Test Error:       %.2f', testError);
            sprintf('Bayes Error:    %.2f', bayesError)};
        annotation(gcf,'textbox' , [0.15 0.12 0.24 0.18], ...
            'String'         , text                 , ...
            'BackgroundColor', [1 1 1]              , ...
            'FontWeight'     , 'demi'               , ...
            'FitBoxToText'   , 'on'                 , ...
            'LineStyle'      , 'none');
    end
end
end
%%

##### SOURCE END #####
--></body></html>